{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd.functional as F\n",
    "import torch.distributions as dist\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Retina display\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using backwards in torch tensor(12.)\n",
      "Using autograd.grad in torch tensor(12.)\n",
      "Using grad in jax 12.0\n"
     ]
    }
   ],
   "source": [
    "def f1(x):\n",
    "    return 3*x**2\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "\n",
    "# Torch version 1 (using .backward)\n",
    "z = f1(x)\n",
    "z.backward()\n",
    "print(\"Using backwards in torch\", x.grad)\n",
    "\n",
    "# Torch version 2 (using autograd.grad)\n",
    "print(\"Using autograd.grad in torch\", torch.autograd.grad(f1(x), x)[0])\n",
    "\n",
    "# Jax version\n",
    "print(\"Using grad in jax\", jax.grad(f1)(jnp.array(2.0)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using Method 1 Torch\n",
      "Partial wrt x:  tensor(8.)\n",
      "Partial wrt y:  tensor(3.)\n",
      "\n",
      "Using Method 2 Torch\n",
      "Partial wrt x:  tensor(8.)\n",
      "Partial wrt y:  tensor(3.)\n",
      "\n",
      "Using Jax\n",
      "Partial wrt x:  8.0\n",
      "Partial wrt y:  3.0\n"
     ]
    }
   ],
   "source": [
    "def f2(x, y):\n",
    "    return 2*x**2 + 3*y\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(1.5, requires_grad=True)\n",
    "\n",
    "# Torch version 1 (using .backward)\n",
    "z = f2(x, y)\n",
    "z.backward()\n",
    "print(\"\\nUsing Method 1 Torch\")\n",
    "print(\"Partial wrt x: \", x.grad)\n",
    "print(\"Partial wrt y: \", y.grad)\n",
    "\n",
    "\n",
    "# Torch version 2 (using autograd.grad)\n",
    "print(\"\\nUsing Method 2 Torch\")\n",
    "print(\"Partial wrt x: \", torch.autograd.grad(f2(x, y), x)[0])\n",
    "print(\"Partial wrt y: \", torch.autograd.grad(f2(x, y), y)[0])\n",
    "\n",
    "# Jax version\n",
    "print(\"\\nUsing Jax\")\n",
    "print(\"Partial wrt x: \", jax.grad(f2, argnums=0)(jnp.array(2.0), jnp.array(1.5)))\n",
    "print(\"Partial wrt y: \", jax.grad(f2, argnums=1)(jnp.array(2.0), jnp.array(1.5)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using Method 1 Torch\n",
      "tensor([8., 3.])\n",
      "\n",
      "Using Method 2 Torch\n",
      "tensor([8., 3.])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using Jax\n",
      "(Array(8., dtype=float32, weak_type=True), Array(3., dtype=float32, weak_type=True))\n"
     ]
    }
   ],
   "source": [
    "# Torch version 1 (using .backward)\n",
    "grad_f2_v1 = torch.tensor([x.grad, y.grad])\n",
    "print(\"\\nUsing Method 1 Torch\")\n",
    "print(grad_f2_v1)\n",
    "# Torch version 2 (using autograd.grad)\n",
    "grad_f2_v2 = torch.tensor(torch.autograd.grad(f2(x, y), (x, y)))\n",
    "print(\"\\nUsing Method 2 Torch\")\n",
    "print(grad_f2_v2)\n",
    "\n",
    "# Jax version\n",
    "grad_f2_jax = jax.grad(f2, argnums=[0, 1])(jnp.array(2.0), jnp.array(1.5))\n",
    "print(\"\\nUsing Jax\")\n",
    "print(grad_f2_jax)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient (vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using Method 1 Torch\n",
      "Gradient:  tensor([8., 3.])\n",
      "\n",
      "Using Method 2 Torch\n",
      "Gradient:  (tensor([8., 3.]),)\n",
      "\n",
      "Using Jax\n",
      "Gradient:  [8. 3.]\n"
     ]
    }
   ],
   "source": [
    "def f2_vectorized(input):\n",
    "    x, y = input\n",
    "    return 2*x**2 + 3*y\n",
    "\n",
    "input = torch.tensor([2.0, 1.5], requires_grad=True)\n",
    "\n",
    "# Torch version 1 (using .backward)\n",
    "z = f2_vectorized(input)\n",
    "z.backward()\n",
    "print(\"\\nUsing Method 1 Torch\")\n",
    "print(\"Gradient: \", input.grad)\n",
    "\n",
    "# Torch version 2 (using autograd.grad)\n",
    "print(\"\\nUsing Method 2 Torch\")\n",
    "print(\"Gradient: \", torch.autograd.grad(f2_vectorized(input), input))\n",
    "\n",
    "# Jax version\n",
    "print(\"\\nUsing Jax\")\n",
    "print(\"Gradient: \", jax.grad(f2_vectorized)(jnp.array([2.0, 1.5])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5., -2.], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We take the Jacobian of the function f(x, y, z) = [x**2 + y**2, y - z]\n",
    "# The Jacobian analytically is [[2x, 2y, 0], [0, 1, -1]]\n",
    "def f1(x, y, z):\n",
    "        return x**2 + y**2\n",
    "def f2(x, y, z):\n",
    "        return y - z\n",
    "\n",
    "def f(x, y, z):\n",
    "    return torch.stack([f1(x, y, z), f2(x, y, z)])\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(1.0, requires_grad=True)\n",
    "z = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "output = f(x, y, z)\n",
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are to directly call: `z.backward()` we get the following error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad can be implicitly created only for scalar outputs\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    output.backward()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, `output.backward()` has a parameter called `gradient`\n",
    "\n",
    ">The graph is differentiated using the chain rule. If the tensor is\n",
    "non-scalar (i.e. its data has more than one element) and requires\n",
    "gradient, the function additionally requires specifying ``gradient``.\n",
    "It should be a tensor of matching type and location, that contains\n",
    "the gradient of the differentiated function w.r.t. ``self``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian with Functional method:\n",
      "tensor([[ 4.,  2., -0.],\n",
      "        [ 0.,  1., -1.]])\n",
      "Jacobian with grad method:\n",
      "tensor([[ 4.,  2., -0.],\n",
      "        [ 0.,  1., -1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch.autograd.functional as F\n",
    "\n",
    "jacobian = torch.vstack(F.jacobian(f, (x, y, z))).T\n",
    "print(\"Jacobian with Functional method:\")\n",
    "print(jacobian)\n",
    "\n",
    "# use torch.autograd.grad\n",
    "jacobian = torch.zeros(2, 3)\n",
    "for output_index in range(2):\n",
    "    jacobian[output_index] = torch.vstack(torch.autograd.grad(f(x, y, z)[output_index], (x, y, z))).ravel()\n",
    "print(\"Jacobian with grad method:\")\n",
    "print(jacobian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jax Jacobian using jax.jacobian:\n",
      "[[ 4.  2. -0.]\n",
      " [ 0.  1. -1.]]\n",
      "Jax Jacobian using jax.grad done manually:\n",
      "[[ 4.  2.  0.]\n",
      " [ 0.  1. -1.]]\n"
     ]
    }
   ],
   "source": [
    "# Jax version using jax.jacobian\n",
    "def f_jax(x, y, z):\n",
    "    return jnp.stack([f1(x, y, z), f2(x, y, z)])\n",
    "\n",
    "print(\"Jax Jacobian using jax.jacobian:\")\n",
    "print(jnp.array(jax.jacobian(f_jax, argnums=[0, 1, 2])(jnp.array(2.0), jnp.array(1.0), jnp.array(3.0))).T)\n",
    "\n",
    "g1 = jnp.array(jax.grad(f1, argnums=[0, 1, 2])(jnp.array(2.0), jnp.array(1.0), jnp.array(3.0)))\n",
    "g2 = jnp.array(jax.grad(f2, argnums=[0, 1, 2])(jnp.array(2.0), jnp.array(1.0), jnp.array(3.0)))\n",
    "print(\"Jax Jacobian using jax.grad done manually:\")\n",
    "print(jnp.vstack([g1.T, g2.T]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacobian (vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Functional method\n",
      "tensor([[ 4.,  2., -0.],\n",
      "        [ 0.,  1., -1.]])\n",
      "Jax Jacobian using jax.jacobian:\n",
      "[[ 4.  2.  0.]\n",
      " [ 0.  1. -1.]]\n"
     ]
    }
   ],
   "source": [
    "def f_vectorized(input):\n",
    "    x, y, z = input\n",
    "    return torch.stack([f1(x, y, z), f2(x, y, z)])\n",
    "\n",
    "print(\"Torch Functional method\")\n",
    "print(F.jacobian(f_vectorized, torch.tensor([2.0, 1.0, 3.0])))\n",
    "\n",
    "def f_vectorized_jax(input):\n",
    "    x, y, z = input\n",
    "    return jnp.stack([f1(x, y, z), f2(x, y, z)])\n",
    "\n",
    "print(\"Jax Jacobian using jax.jacobian:\")\n",
    "print(jax.jacobian(f_vectorized_jax)(jnp.array([2.0, 1.0, 3.0])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hessian using autograd.functional.hessian:\n",
      "tensor([[2., 3., 1.],\n",
      "        [3., 2., 2.],\n",
      "        [1., 2., 0.]])\n",
      "Jax Hessian using jax.hessian:\n",
      "[[2. 3. 1.]\n",
      " [3. 2. 2.]\n",
      " [1. 2. 0.]]\n",
      "Jax Hessian using jax.jacobian:\n",
      "[[2. 3. 1.]\n",
      " [3. 2. 2.]\n",
      " [1. 2. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def f(x, y, z):\n",
    "    return x**2 + y**2 + x * y * z\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(1.0, requires_grad=True)\n",
    "z = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# Torch version using autograd.functional.hessian\n",
    "print(\"Hessian using autograd.functional.hessian:\")\n",
    "torch_v1_hessian = torch.tensor(F.hessian(f, (x, y, z)))\n",
    "print(torch_v1_hessian)\n",
    "\n",
    "# Jax version using jax.hessian\n",
    "jax_hessian = jnp.array(jax.hessian(f, argnums=[0, 1, 2])(jnp.array(2.0), jnp.array(1.0), jnp.array(3.0)))\n",
    "print(\"Jax Hessian using jax.hessian:\")\n",
    "print(jax_hessian)\n",
    "\n",
    "# Jax version using jax.jacobian\n",
    "jacobian_fn = jax.jacobian(f, argnums=[0, 1, 2])\n",
    "hessian_fn = jax.jacobian(jacobian_fn, argnums=[0, 1, 2])\n",
    "jax_hessian = jnp.array(hessian_fn(jnp.array(2.0), jnp.array(1.0), jnp.array(3.0)))\n",
    "print(\"Jax Hessian using jax.jacobian:\")\n",
    "print(jax_hessian)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hessian (vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Functional method\n",
      "tensor([[2., 3., 1.],\n",
      "        [3., 2., 2.],\n",
      "        [1., 2., 0.]])\n",
      "Jax Hessian using jax.hessian:\n",
      "[[2. 3. 1.]\n",
      " [3. 2. 2.]\n",
      " [1. 2. 0.]]\n",
      "Jax Hessian using jax.jacobian:\n",
      "[[2. 3. 1.]\n",
      " [3. 2. 2.]\n",
      " [1. 2. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def f_vectorized(input):\n",
    "    x, y, z = input\n",
    "    return x**2 + y**2 + x * y * z\n",
    "\n",
    "print(\"Torch Functional method\")\n",
    "print(F.hessian(f_vectorized, torch.tensor([2.0, 1.0, 3.0])))\n",
    "\n",
    "print(\"Jax Hessian using jax.hessian:\")\n",
    "print(jax.hessian(f_vectorized)(jnp.array([2.0, 1.0, 3.0])))\n",
    "\n",
    "print(\"Jax Hessian using jax.jacobian:\")\n",
    "print(jax.jacobian(jax.jacobian(f_vectorized))(jnp.array([2.0, 1.0, 3.0])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

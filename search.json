[
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Probabilistic Machine Learning",
    "section": "",
    "text": "Have the initial discussion in the issue itself.\nOnce you are clear on To Dos, fork this repo. It will create a replica of this repo in your GitHub account.\nNever work on the main branch. Create a new branch to solve a new issue.\nBefore making a commit, verify that your commit only modfies the files that are required to solve the issue. If you have made any other changes, remove them from the commit. Usually, modifying uncessary files would be the root cause of merge conflicts.\nAfter the initial part of work is done, open a PR to this repo and instructors will review the PR."
  },
  {
    "objectID": "contributing.html#steps-to-contribute-generelizes-to-any-github-project",
    "href": "contributing.html#steps-to-contribute-generelizes-to-any-github-project",
    "title": "Probabilistic Machine Learning",
    "section": "",
    "text": "Have the initial discussion in the issue itself.\nOnce you are clear on To Dos, fork this repo. It will create a replica of this repo in your GitHub account.\nNever work on the main branch. Create a new branch to solve a new issue.\nBefore making a commit, verify that your commit only modfies the files that are required to solve the issue. If you have made any other changes, remove them from the commit. Usually, modifying uncessary files would be the root cause of merge conflicts.\nAfter the initial part of work is done, open a PR to this repo and instructors will review the PR."
  },
  {
    "objectID": "contributing.html#guidelines",
    "href": "contributing.html#guidelines",
    "title": "Probabilistic Machine Learning",
    "section": "Guidelines",
    "text": "Guidelines\n\nVS code IDE is recommended. Use AI tools like GitHub Co-pilot to speed up your coding. You can get free access if you are a student by following this tutorial.\nUse black as a code formatter. Install using pip install black[jupyter]. Enable Notebook › Format On Save in VS code setting to format your code automatically on save.\nUse PEP-8 code style wherever possible. A few important points are the following:\n\nFile names and variable names are lower case and words are separated by underscores e.g. example_1.py\nClass names are upper case and words start with upper case e.g. BayesianLinearRegression"
  },
  {
    "objectID": "notebooks/laplace-approx.html",
    "href": "notebooks/laplace-approx.html",
    "title": "1D Taylor approximation",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.distributions as dist\nimport torch.autograd.functional as F\nfrom torch.func import grad, jacfwd, hessian\nfrom math import factorial\nfrom ipywidgets import interact\n\n\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_blobs\n\nfrom tueplots.bundles import beamer_moml\nimport matplotlib.pyplot as plt\n\n# Use render mode to run the notebook and save the plots in beamer format\n# Use interactive mode to run the notebook and show the plots in notebook-friendly format\nmode = \"render\"  # \"interactive\" or \"render\"\n\nif mode == \"render\":\n    width = 0.6\n    plt.rcParams.update(beamer_moml(rel_width=width, rel_height=width * 0.8))\n    # update marker size\n    plt.rcParams.update({\"lines.markersize\": 4})\n    plt.rcParams[\"figure.facecolor\"] = \"none\"\nelse:\n    plt.rcdefaults()\ndef plt_show(name=None):\n    if mode == \"interactive\":\n        plt.show()\n    elif mode == \"render\":\n        plt.savefig(f\"../figures/laplace-approx/{name}.pdf\")\n    else:\n        raise ValueError(f\"Unknown mode: {mode}\")\n# Plot\nf = lambda x: torch.sin(x + 1)\n\nx = torch.linspace(-5, 5, 100)\nplt.plot(x, f(x))\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt_show(\"sin\")\ndef get_taylor_1d_fn(f, x, a, ord):\n    term = f(a).repeat(x.size())\n    poly = r\"$\\tilde{f}(x) = $\" + f\"{f(a).item():.2f}\"\n    for i in range(1, ord + 1):\n        f = grad(f)\n        value = f(a)\n        value_str = f\"{abs(value.item()):.2f}\"\n        denominator = factorial(i)\n        poly_term = f\"(x - {a.item():.2f})^{i}\"\n        term = term + value * (x - a) ** i / denominator\n        if i &lt;= 5:\n            poly += (\n                f\"{' - ' if value &lt; 0 else ' + '}\"\n                + value_str\n                + r\"$\\frac{\"\n                + poly_term\n                + \"}{\"\n                + f\"{i}!\"\n                + \"}\"\n                + \"$\"\n            )\n        elif i == 6:\n            poly += \" + ...\"\n    return term.detach().numpy(), poly\n@interact(ord=(0, 12))\ndef plot_1d_taylor(ord):\n    plt.plot(x, f(x), label=\"f(x)\")\n    term, poly = get_taylor_1d_fn(f, x, a=torch.tensor(0.0), ord=ord)\n    plt.plot(\n        x,\n        term,\n        # get_taylor_1d_fn(f, x, a=torch.tensor(0.0), ord=ord),\n        label=f\"Taylor aproximation\\nPolynomial degree: {ord}\",\n        linestyle=\"--\",\n    )\n    plt.xlabel(\"x\")\n    plt.ylabel(\"p(x)\")\n    plt.title(poly)\n    plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n    plt.ylim(-1.5, 1.5)\n    plt_show(f\"sin-taylor-{ord}\")"
  },
  {
    "objectID": "notebooks/laplace-approx.html#nd-taylor-approximation",
    "href": "notebooks/laplace-approx.html#nd-taylor-approximation",
    "title": "1D Taylor approximation",
    "section": "ND Taylor approximation",
    "text": "ND Taylor approximation\nChecking that two times jacobian is a hessian\n\nf = lambda x: torch.sin(x[0] + 1) + torch.cos(x[1] - 1)\ninp = torch.tensor([1.0, 2.0])\n\ndisplay(jacfwd(jacfwd(f))(inp))\ndisplay(hessian(f)(inp))\n\ntensor([[-0.9093, -0.0000],\n        [-0.0000, -0.5403]])\n\n\ntensor([[-0.9093,  0.0000],\n        [ 0.0000, -0.5403]])\n\n\n\n# define a function which is 45 degree rotated sin function\nf = lambda x: torch.sin(x.sum() + 1)\n\nx = torch.linspace(-5, 5, 40)\nX1, X2 = torch.meshgrid(x, x)\nX = torch.stack([X1.ravel(), X2.ravel()], dim=-1)\nY = torch.vmap(f)(X).reshape(X1.shape)\nprint(X.shape, Y.shape)\n\nplt.contourf(X1, X2, Y, levels=20, vmin=-1, vmax=1, cmap=\"coolwarm\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$x_2$\")\nplt.colorbar(label=\"$f(x)$\", ticks=[-1, -0.5, 0, 0.5, 1])\nplt.gca().set_aspect(\"equal\")\nplt_show(\"sin2d\")\n\ntorch.Size([1600, 2]) torch.Size([40, 40])\n\n\n\n\n\n\ndef nd_taylor_approx(x):\n    x = x.reshape(1, -1)\n    term = f(a)\n    if ord &gt;= 1:\n        jacobian = jacfwd(f)(a)\n        term = (term + jacobian @ (x - a).T).squeeze()\n    if ord &gt;= 2:\n        hess = hessian(f)(a.ravel())\n        term = term + (((x - a) @ hess @ (x - a).T) / 2).squeeze()\n    if ord &gt;= 3:\n        raise NotImplementedError\n    return term\n\n\nnd_taylor_approx = torch.vmap(nd_taylor_approx)\n\nord = 2\na = torch.tensor([0.0, 0.0], dtype=torch.float64).reshape(1, 2)\n\nT_Y = nd_taylor_approx(X).reshape(X1.shape)\nfig = plt.figure()\n# add a 3d axis\nax = fig.add_subplot(121, projection=\"3d\")\nax.plot_surface(X1, X2, Y, cmap=\"coolwarm\", vmin=-1.5, vmax=1.5)\nax.set_zlim(-1.5, 1.5)\nax2 = fig.add_subplot(122, projection=\"3d\")\nax2.plot_surface(X1, X2, T_Y, cmap=\"coolwarm\", vmin=-1.5, vmax=1.5)\nax2.set_zlim(-1.5, 1.5)\n# mappable = ax[0].contourf(X1, X2, Y.reshape(X1.shape), levels=20)\n# fig.colorbar(mappable, ax=ax[0])\n# mappable = ax[1].contourf(X1, X2, T_Y, levels=20, vmin=Y.min(), vmax=Y.max())\nax.set_xlabel(\"$x_1$\", labelpad=-2)\nax2.set_xlabel(\"$x_1$\", labelpad=-2)\nax2.set_ylabel(\"$x_2$\", labelpad=-2)\nax.set_ylabel(\"$x_2$\", labelpad=-2)\nfig.suptitle(f\"Taylor approximation\\nPolynomial degree: {ord}\")\nplt_show(f\"sin2d-taylor-{ord}\")\n\n\n\n\n\nplt.plot(x, Y[10, :], label=\"$f(10, x_2)$\")\nplt.plot(x, T_Y[10, :], label=r\"$\\tilde{{f}}(10, x_2)$\")\nplt.ylim(-1.5, 1.5)\nplt.xlabel(\"$x_2$\")\nplt.ylabel(\"$y$\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\nplt.title(\"Cross section at $x_1 = 0$\")\nplt_show(f\"sin2d-taylor-1d-{ord}\")\n\n\n\n\n\n# Simple PDF approximation and assume it to be our prob function\np = torch.distributions.Normal(0, 1)\n\nlogp = lambda x: p.log_prob(x)\n\n# Plot\nx = torch.linspace(-4, 4, 100)\nplt.plot(x, logp(x).exp())\nplt.xlabel(\"x\")\nplt.ylabel(\"p(x)\")\nplt_show(\"standard-normal\")"
  },
  {
    "objectID": "notebooks/laplace-approx.html#beta-prior-for-coin-toss",
    "href": "notebooks/laplace-approx.html#beta-prior-for-coin-toss",
    "title": "1D Taylor approximation",
    "section": "Beta prior for coin toss",
    "text": "Beta prior for coin toss\n\ndata = torch.tensor([1] * 9 + [0] * 1, dtype=torch.float64)\nalpha = 2\nbeta = 2\n\n\ndef neg_log_prior(theta):\n    return -dist.Beta(alpha, beta).log_prob(theta)\n\n\ndef neg_log_likelihood(theta):\n    likelihood = torch.where(data == 1, torch.log(theta), torch.log(1 - theta))\n    #likelihood = dist.Bernoulli(probs=theta).log_prob(data)\n    return -likelihood.sum()\n\n\ndef neg_log_joint(theta):\n    return neg_log_prior(theta) + neg_log_likelihood(theta)\n\n\ntheta_grid = torch.linspace(0.01, 0.99, 100)\n\n\ndef plot_prior_and_lik():\n    prior = torch.exp(-neg_log_prior(theta_grid))\n    fig, ax = plt.subplots()\n    ax.plot(theta_grid, prior, label=\"Prior\", color=\"C0\")\n    twinx = ax.twinx()\n    twinx.plot(\n        theta_grid,\n        torch.exp(-torch.vmap(neg_log_likelihood)(theta_grid)),\n        label=\"Likelihood\",\n        color=\"C1\",\n    )\n    ax.set_xlabel(r\"$\\theta$\")\n    ax.set_ylabel(r\"$p(\\theta)$\")\n    twinx.set_ylabel(r\"$p(D|\\theta)$\")\n    return fig, ax, twinx\n\n\nfig, ax, twinx = plot_prior_and_lik()\nfig.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\nplt_show(\"beta-prior-coin-toss\")\n\n\n\n\n\ntheta_map = torch.tensor(0.5, requires_grad=True)\n\n\ndef optimize(theta, epochs, lr):\n    optimizer = torch.optim.Adam([theta], lr=lr)\n\n    pbar = tqdm(range(epochs))\n    losses = []\n    for epoch in pbar:\n        optimizer.zero_grad()\n        loss = neg_log_joint(theta)\n        loss.backward()\n        optimizer.step()\n        pbar.set_description(f\"loss={loss.item():.2f}\")\n        losses.append(loss.item())\n    return losses\n\n\nlosses = optimize(theta_map, epochs=500, lr=0.01)\nprint(theta_map)\nplt.plot(losses)\nplt.show()\n\nloss=3.61: 100%|██████████| 500/500 [00:00&lt;00:00, 694.99it/s]\n\n\ntensor(0.8333, requires_grad=True)\n\n\n\n\n\n\nfig, ax, twinx = plot_prior_and_lik()\nwith torch.no_grad():\n    ax.vlines(theta_map.item(), *ax.get_ylim(), label=\"MAP\", color=\"C2\")\n\nfig.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\nplt_show(\"beta-prior-coin-toss-map\")\n\n\n\n\n\nhess = hessian(neg_log_joint)(theta_map)\nposterior_variance = 1 / hess\napprox_posterior = dist.Normal(theta_map, posterior_variance**0.5)\n\n\ntrue_posterior = dist.Beta(alpha + data.sum(), beta + len(data) - data.sum())\nfig, ax, twinx = plot_prior_and_lik()\nwith torch.no_grad():\n    ax.plot(\n        theta_grid,\n        torch.exp(approx_posterior.log_prob(theta_grid)),\n        label=\"Laplace Approx Posterior\",\n        color=\"C3\",\n        linestyle=\"--\",\n    )\n    ax.plot(\n        theta_grid,\n        torch.exp(true_posterior.log_prob(theta_grid)),\n        label=\"True Posterior\",\n        color=\"C4\",\n    )\n    ax.vlines(theta_map.item(), *ax.get_ylim(), label=\"MAP\", color=\"C2\")\n\nfig.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\nplt_show(\"beta-prior-coin-toss-laplace\")"
  },
  {
    "objectID": "notebooks/laplace-approx.html#normal-prior-for-coin-toss",
    "href": "notebooks/laplace-approx.html#normal-prior-for-coin-toss",
    "title": "1D Taylor approximation",
    "section": "Normal prior for coin toss",
    "text": "Normal prior for coin toss\n\ndata = torch.tensor([1] * 9 + [0] * 1, dtype=torch.float64)\nprior_mean = torch.tensor(-2.0)\nprior_variance = torch.tensor(1.0)\n\n\ndef neg_log_prior(theta):\n    return -dist.Normal(prior_mean, prior_variance**0.5).log_prob(theta).squeeze()\n\n\ndef neg_log_likelihood(theta):\n    preds = torch.sigmoid(theta)\n    likelihood = torch.where(data == 1, torch.log(preds), torch.log(1 - preds))\n    return -likelihood.sum()\n\n\ndef neg_log_joint(theta):\n    return neg_log_prior(theta) + neg_log_likelihood(theta)\n\n\ntheta_grid = torch.linspace(-5, 5, 100)\nfig, ax, twinx = plot_prior_and_lik()\nfig.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\nplt_show(\"normal-prior-coin-toss\")\n\n\n\n\n\ntheta_map = torch.tensor(-2.0, requires_grad=True)\nlosses = optimize(theta_map, epochs=500, lr=0.01)\n\nprint(theta_map)\nplt.plot(losses)\nplt.show()\n\nloss=9.28: 100%|██████████| 500/500 [00:00&lt;00:00, 729.16it/s] \n\n\ntensor(0.5143, requires_grad=True)\n\n\n\n\n\n\nfig, ax, twinx = plot_prior_and_lik()\nwith torch.no_grad():\n    ax.vlines(theta_map.item(), *ax.get_ylim(), label=\"MAP\", color=\"C2\")\n\nfig.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\nplt_show(\"normal-prior-coin-toss-map\")\n\n\n\n\n\nhess = hessian(neg_log_joint)(theta_map)\nposterior_variance = 1 / hess\napprox_posterior = dist.Normal(theta_map, posterior_variance**0.5)\n\n\nfig, ax, twinx = plot_prior_and_lik()\nwith torch.no_grad():\n    ax.plot(\n        theta_grid,\n        torch.exp(approx_posterior.log_prob(theta_grid)),\n        label=\"Laplace Approx Posterior\",\n        color=\"C3\",\n        linestyle=\"--\",\n    )\n    ax.vlines(theta_map.item(), *ax.get_ylim(), label=\"MAP\", color=\"C2\")\n\n# monte carlo estimation of posterior\nunnorm_p = torch.exp(-torch.stack([neg_log_joint(theta) for theta in theta_grid]))\nprint(unnorm_p.shape)\n\nprior = dist.Normal(prior_mean, prior_variance**0.5)\nsamples = prior.sample((100000,))\nlik = torch.exp(-torch.vmap(neg_log_likelihood)(samples))\napprox_evidence = lik.mean()\nnorm_p = unnorm_p / approx_evidence\n\nax.plot(theta_grid, norm_p, label=\"True (MC) Posterior\", color=\"C4\")\n\nfig.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\nplt_show(\"normal-prior-coin-toss-laplace\")\n\ntorch.Size([100])"
  },
  {
    "objectID": "notebooks/laplace-approx.html#multi-mode",
    "href": "notebooks/laplace-approx.html#multi-mode",
    "title": "1D Taylor approximation",
    "section": "Multi-Mode",
    "text": "Multi-Mode\n\ntarget = dist.MixtureSameFamily(\n    torch.distributions.Categorical(torch.tensor([0.7, 0.3])),\n    dist.Normal(torch.tensor([-2.0, 2.0]), torch.tensor([1.0, 1.0])),\n)\n\nplt.plot(theta_grid, torch.exp(target.log_prob(theta_grid)), label=\"Target\")\nplt.xlabel(r\"$\\theta$\")\nplt.ylabel(r\"$p(\\theta)$\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\nplt_show(\"mixture-density\")\n\n\n\n\n\nneg_log_joint = lambda x: -target.log_prob(x)\ntheta_map = torch.tensor(0.0, requires_grad=True)\nlosses = optimize(theta_map, epochs=500, lr=0.01)\nplt.plot(losses)\n\nloss=1.28: 100%|██████████| 500/500 [00:00&lt;00:00, 829.85it/s]\n\n\n\n\n\n\nhess = hessian(neg_log_joint)(theta_map)\nposterior_variance = 1 / hess\napprox_posterior = dist.Normal(theta_map, posterior_variance**0.5)\n\n\nplt.plot(theta_grid, torch.exp(target.log_prob(theta_grid)), label=\"Target\")\nwith torch.no_grad():\n    plt.plot(\n        theta_grid,\n        torch.exp(approx_posterior.log_prob(theta_grid)),\n        label=\"Laplace\\nApproximation\",\n        linestyle=\"--\",\n    )\nplt.xlabel(r\"$\\theta$\")\nplt.ylabel(r\"$p(\\theta)$\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\nplt_show(\"mixture-density-laplace\")"
  },
  {
    "objectID": "notebooks/laplace-approx.html#old",
    "href": "notebooks/laplace-approx.html#old",
    "title": "1D Taylor approximation",
    "section": "Old",
    "text": "Old\n\n# Optimize logp using SGD\ntheta = torch.tensor(4.0, requires_grad=True)\noptimizer = torch.optim.AdamW([theta], lr=0.01)\n\nfor i in range(2000):\n    optimizer.zero_grad()\n    loss = -logp(theta)\n    if i % 100 == 0:\n        print(i, theta.item(), loss.item())\n    loss.backward()\n    optimizer.step()\n\n0 4.0 8.918938636779785\n100 3.0124826431274414 5.4564642906188965\n200 2.1752238273620605 3.284738063812256\n300 1.497883915901184 2.040766716003418\n400 0.9779170751571655 1.397099494934082\n500 0.6023826003074646 1.1003708839416504\n600 0.3488367199897766 0.9797820448875427\n700 0.18942442536354065 0.9368793368339539\n800 0.09626010805368423 0.9235715270042419\n900 0.045690640807151794 0.9199823141098022\n1000 0.02021404542028904 0.9191428422927856\n1100 0.008314372971653938 0.9189730882644653\n1200 0.003170008771121502 0.9189435243606567\n1300 0.0011164519237354398 0.9189391136169434\n1400 0.0003617757756728679 0.9189385771751404\n1500 0.00010737218690337613 0.9189385175704956\n1600 2.9038295906502753e-05 0.9189385175704956\n1700 7.114796062523965e-06 0.9189385175704956\n1800 1.5690019381509046e-06 0.9189385175704956\n1900 3.091245162067935e-07 0.9189385175704956\n\n\n\ntheta_map = theta.detach()\ntheta_map\n\ntensor(5.3954e-08)\n\n\n\nplt.plot(x, p.log_prob(x).exp(), label=\"True PDF\")\n\n# Plot theta_map point\nplt.scatter(\n    0,\n    p.log_prob(theta_map).exp(),\n    label=r\"$\\theta_\\textrm{MAP}$\",\n    color=\"C1\",\n    zorder=10,\n)\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f6a30d168e0&gt;\n\n\nError in callback &lt;function flush_figures at 0x7f6a8325d0d0&gt; (for post_execute):\n\n\n\nhessian = F.hessian(logp, theta_map)\nhessian\n\n\nscale = 1 / torch.sqrt(-hessian)\nscale\n\n\n# Approximate the PDF using the Laplace approximation\napprox_p = dist.Normal(theta_map, scale)\napprox_p\n\n\n# Plot original PDF\nx = torch.linspace(-10, 10, 100)\nplt.plot(x, p.log_prob(x).exp(), label=\"True PDF\")\n# Plot Laplace approximation\nplt.plot(x, approx_p.log_prob(x).exp(), label=\"Laplace Approximation\", linestyle=\"-.\")\nplt.legend()\n\n\ndef laplace_approximation(logp, theta_init, lr=0.01, n_iter=2000):\n    # Optimize logp using an optimizer\n    theta = torch.tensor(theta_init, requires_grad=True)\n    optimizer = torch.optim.AdamW([theta], lr=lr)\n    for i in range(n_iter):\n        optimizer.zero_grad()\n        loss = -logp(theta)\n        loss.backward()\n        optimizer.step()\n    theta_map = theta.detach()\n    hessian = F.hessian(logp, theta_map)\n    scale = 1 / torch.sqrt(-hessian)\n    return dist.Normal(theta_map, scale)\n\n\ndef plot_orig_approx(logp, approx_p, min_x=-10, max_x=10):\n    # Plot original PDF\n    x = torch.linspace(min_x, max_x, 500)\n    plt.plot(x, p.log_prob(x).exp(), label=\"True PDF\")\n    # Plot Laplace approximation\n    plt.plot(\n        x, approx_p.log_prob(x).exp(), label=\"Laplace Approximation\", linestyle=\"-.\"\n    )\n    plt.legend()\n\n\n# Create a Student's t-distribution\np = dist.StudentT(5, 0, 1)\nlogp = lambda x: p.log_prob(x)\n\napprox_p = laplace_approximation(logp, 4.0)\nplot_orig_approx(logp, approx_p)\n\n\np = dist.LogNormal(0, 1)\nlogp = lambda x: p.log_prob(x)\n\napprox_p = laplace_approximation(logp, 4.0)\nplot_orig_approx(logp, approx_p, min_x=0.01)\n\n\np = dist.Beta(2, 2)\nlogp = lambda x: p.log_prob(x)\n\napprox_p = laplace_approximation(logp, 0.5)\nplot_orig_approx(logp, approx_p, min_x=0, max_x=1)"
  },
  {
    "objectID": "notebooks/nn-variants_zeel.html",
    "href": "notebooks/nn-variants_zeel.html",
    "title": "Capturing uncertainty in neural nets:",
    "section": "",
    "text": "aleatoric\n\nhomoskedastic (fixed)\nhomoskedastic (learnt)\nheteroskedastic\n\nepistemic uncertainty (Laplace approximation)\nboth aleatoric and epistemic uncertainty"
  },
  {
    "objectID": "notebooks/nn-variants_zeel.html#notation",
    "href": "notebooks/nn-variants_zeel.html#notation",
    "title": "Capturing uncertainty in neural nets:",
    "section": "Notation",
    "text": "Notation"
  },
  {
    "objectID": "notebooks/nn-variants_zeel.html#models-capturing-aleatoric-uncertainty",
    "href": "notebooks/nn-variants_zeel.html#models-capturing-aleatoric-uncertainty",
    "title": "Capturing uncertainty in neural nets:",
    "section": "Models capturing aleatoric uncertainty",
    "text": "Models capturing aleatoric uncertainty\n\nDataset containing homoskedastic noise\n\ntorch.manual_seed(42)\nN = 100\nx_lin = torch.linspace(-1, 1, N)\n\nf = lambda x: 0.5 * x**2 + 0.25 * x**3\n\neps = torch.randn(N) * 0.2\n\ny = f(x_lin) + eps\n\n# Move to GPU\nx_lin = x_lin.to(device)\ny = y.to(device)\n\n\n# Plot data and true function\nplt.plot(x_lin.cpu(), y.cpu(), \"o\", label=\"data\")\nplt.plot(x_lin.cpu(), f(x_lin).cpu(), label=\"true function\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7faa2c06efa0&gt;\n\n\n\n\n\n\n\nCase 1.1: Models assuming Homoskedastic noise\n\nCase 1.1.1: Homoskedastic noise is fixed beforehand and not learned\n\n\nclass MeanEstimateNN(torch.nn.Module):\n    def __init__(self, n_hidden=10):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(1, n_hidden)\n        self.fc2 = torch.nn.Linear(n_hidden, n_hidden)\n        self.fc3 = torch.nn.Linear(n_hidden, 1)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = torch.relu(x)\n        x = self.fc2(x)\n        x = torch.relu(x)\n        mu_hat = self.fc3(x)\n        return mu_hat\n\n\ndef loss_homoskedastic_noise(model, x, y, params):\n    log_noise_std = params[\"log_noise_std\"]\n    mu_hat = model(x).squeeze()\n    assert mu_hat.shape == y.shape\n    noise_std = torch.exp(log_noise_std).expand_as(mu_hat)\n    dist = torch.distributions.Normal(mu_hat, noise_std)\n    return -dist.log_prob(y).mean()\n\n\nhomo_model_homo_noise_fixed = MeanEstimateNN().to(device)\nhomo_model_homo_noise_fixed\n\nMeanEstimateNN(\n  (fc1): Linear(in_features=1, out_features=10, bias=True)\n  (fc2): Linear(in_features=10, out_features=10, bias=True)\n  (fc3): Linear(in_features=10, out_features=1, bias=True)\n)\n\n\n\nfixed_log_noise_std = torch.log(torch.tensor(0.5)).to(device)\nparams = {\n    \"nn_params\": homo_model_homo_noise_fixed.state_dict(),\n    \"log_noise_std\": fixed_log_noise_std,\n}\nloss_homoskedastic_noise(homo_model_homo_noise_fixed, x_lin[:, None], y, params)\n\ntensor(0.3774, device='cuda:0', grad_fn=&lt;NegBackward0&gt;)\n\n\n\ndef plot_results(y_hat, epistemic_std=None, aleatoric_std=None, model_name=\"\"):\n    plt.scatter(x_lin.cpu(), y.cpu(), s=10, color=\"C0\", label=\"Data\")\n    plt.plot(x_lin.cpu(), f(x_lin.cpu()), color=\"C1\", label=\"True function\")\n    plt.plot(x_lin.cpu(), y_hat.cpu(), color=\"C2\", label=model_name)\n    if epistemic_std is not None:\n        plt.fill_between(\n            x_lin.cpu(),\n            (y_hat - 2 * epistemic_std).cpu(),\n            (y_hat + 2 * epistemic_std).cpu(),\n            alpha=0.3,\n            color=\"C3\",\n            label=\"Epistemic uncertainty\",\n        )\n    if aleatoric_std is not None:\n        plt.fill_between(\n            x_lin.cpu(),\n            (y_hat - 2 * aleatoric_std).cpu(),\n            (y_hat + 2 * aleatoric_std).cpu(),\n            alpha=0.3,\n            color=\"C2\",\n            label=\"Aleatoric uncertainty\",\n        )\n\n    plt.xlabel(\"$x$\")\n    plt.ylabel(\"$y$\")\n    plt.legend()\n\n\nwith torch.no_grad():\n    y_hat = homo_model_homo_noise_fixed(x_lin[:, None]).squeeze()\n\nplot_results(\n    y_hat, aleatoric_std=torch.exp(fixed_log_noise_std), model_name=\"Untrained model\"\n)\n\nfindfont: Font family ['cursive'] not found. Falling back to DejaVu Sans.\nfindfont: Generic family 'cursive' not found because none of the following families were found: Apple Chancery, Textile, Zapf Chancery, Sand, Script MT, Felipa, Comic Neue, Comic Sans MS, cursive\n\n\n\n\n\n\ndef train_fn(model, loss_func, params, x, y, n_epochs=1000, lr=0.01):\n    parameter_leaves = jtu.tree_leaves(params)\n    optimizer = torch.optim.Adam(parameter_leaves, lr=lr)\n    for epoch in range(n_epochs):\n        optimizer.zero_grad()\n        loss = loss_func(model, x, y, params)\n        loss.backward()\n        optimizer.step()\n        # Print every 10 epochs\n        if epoch % 50 == 0:\n            print(f\"Epoch {epoch}: loss {loss.item():.3f}\")\n    return loss.item()\n\n\nhomo_model_homo_noise_fixed = MeanEstimateNN().to(device)\nparams = {\n    \"nn_params\": list(homo_model_homo_noise_fixed.parameters()),\n    \"log_noise_std\": fixed_log_noise_std,\n}\ntrain_fn(\n    homo_model_homo_noise_fixed,\n    loss_homoskedastic_noise,\n    params,\n    x_lin[:, None],\n    y,\n    n_epochs=1000,\n    lr=0.001,\n)\n\nEpoch 0: loss 0.451\nEpoch 50: loss 0.361\nEpoch 100: loss 0.343\nEpoch 150: loss 0.332\nEpoch 200: loss 0.321\nEpoch 250: loss 0.313\nEpoch 300: loss 0.308\nEpoch 350: loss 0.305\nEpoch 400: loss 0.303\nEpoch 450: loss 0.303\nEpoch 500: loss 0.302\nEpoch 550: loss 0.302\nEpoch 600: loss 0.301\nEpoch 650: loss 0.301\nEpoch 700: loss 0.301\nEpoch 750: loss 0.301\nEpoch 800: loss 0.300\nEpoch 850: loss 0.300\nEpoch 900: loss 0.300\nEpoch 950: loss 0.299\n\n\n0.29893454909324646\n\n\n\nwith torch.no_grad():\n    y_hat = homo_model_homo_noise_fixed(x_lin[:, None]).squeeze()\n    aleatoric_std = torch.exp(fixed_log_noise_std)\n\nplot_results(y_hat, aleatoric_std=aleatoric_std, model_name=\"Trained model\")\n\n\n\n\n\n\nCase 1.1.2: Homoskedastic noise is learnt from the data\n\nThe model is the same as in case 1.1.1, but the noise is learned from the data.\n\nhomo_model_homo_noise_learnable = MeanEstimateNN().to(device)\nlog_noise_std = torch.nn.Parameter(torch.tensor(0.0).to(device))\n\nhomo_model_homo_noise_learnable\n\nMeanEstimateNN(\n  (fc1): Linear(in_features=1, out_features=10, bias=True)\n  (fc2): Linear(in_features=10, out_features=10, bias=True)\n  (fc3): Linear(in_features=10, out_features=1, bias=True)\n)\n\n\n\n# Plot the untrained model\nwith torch.no_grad():\n    y_hat = homo_model_homo_noise_learnable(x_lin[:, None]).squeeze()\n    aleatoric_std = torch.exp(log_noise_std)\n\nplot_results(y_hat, aleatoric_std=aleatoric_std, model_name=\"Untrained model\")\n\n\n\n\n\n# Train the model\nhomo_model_homo_noise_learnable = MeanEstimateNN().to(device)\nparams = {\n    \"nn_params\": list(homo_model_homo_noise_learnable.parameters()),\n    \"log_noise_std\": log_noise_std,\n}\n\ntrain_fn(\n    homo_model_homo_noise_learnable,\n    loss_homoskedastic_noise,\n    params,\n    x_lin[:, None],\n    y,\n    n_epochs=1000,\n    lr=0.01,\n)\n\nEpoch 0: loss 0.993\nEpoch 50: loss 0.478\nEpoch 100: loss 0.074\nEpoch 150: loss -0.167\nEpoch 200: loss -0.231\nEpoch 250: loss -0.235\nEpoch 300: loss -0.235\nEpoch 350: loss -0.235\nEpoch 400: loss -0.235\nEpoch 450: loss -0.235\nEpoch 500: loss -0.235\nEpoch 550: loss -0.235\nEpoch 600: loss -0.235\nEpoch 650: loss -0.235\nEpoch 700: loss -0.235\nEpoch 750: loss -0.235\nEpoch 800: loss -0.235\nEpoch 850: loss -0.235\nEpoch 900: loss -0.235\nEpoch 950: loss -0.235\n\n\n-0.2348887175321579\n\n\n\n# Plot the trained model\nwith torch.no_grad():\n    y_hat = homo_model_homo_noise_learnable(x_lin[:, None]).squeeze()\n    aleatoric_std = torch.exp(log_noise_std)\n\nplot_results(y_hat, aleatoric_std=aleatoric_std, model_name=\"Trained model\")\n\n\n\n\n\n\n\nCase 1.2: Models assuming heteroskedastic noise\n\n\nclass HeteroskedasticNN(torch.nn.Module):\n    def __init__(self, n_hidden=10):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(1, n_hidden)\n        self.fc2 = torch.nn.Linear(n_hidden, n_hidden)\n        self.fc3 = torch.nn.Linear(n_hidden, 2)  # we learn both mu and log_noise_std\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = torch.relu(x)\n        x = self.fc2(x)\n        x = torch.relu(x)\n        z = self.fc3(x)\n        mu_hat = z[:, 0]\n        log_noise_std = z[:, 1]\n        return mu_hat, log_noise_std\n\n\nhetero_model_homo_noise_learnable = HeteroskedasticNN().to(device)\nhetero_model_homo_noise_learnable\n\nHeteroskedasticNN(\n  (fc1): Linear(in_features=1, out_features=10, bias=True)\n  (fc2): Linear(in_features=10, out_features=10, bias=True)\n  (fc3): Linear(in_features=10, out_features=2, bias=True)\n)\n\n\n\n# Plot the untrained model\nwith torch.no_grad():\n    y_hat, log_noise_std = hetero_model_homo_noise_learnable(x_lin[:, None])\n    aleatoric_std = torch.exp(log_noise_std)\n\nplot_results(y_hat, aleatoric_std=aleatoric_std, model_name=\"Untrained model\")\n\n\n\n\n\ndef loss_heteroskedastic(model, x, y, params):\n    mu_hat, log_noise_std = model(x)\n    noise_std = torch.exp(log_noise_std)\n    dist = torch.distributions.Normal(mu_hat, noise_std)\n    return -dist.log_prob(y).mean()\n\n\nparams = list(hetero_model_homo_noise_learnable.parameters())\ntrain_fn(\n    hetero_model_homo_noise_learnable,\n    loss_heteroskedastic,\n    params,\n    x_lin[:, None],\n    y,\n    n_epochs=1000,\n    lr=0.01,\n)\n\nEpoch 0: loss 0.947\nEpoch 50: loss -0.185\nEpoch 100: loss -0.214\nEpoch 150: loss -0.228\nEpoch 200: loss -0.244\nEpoch 250: loss -0.258\nEpoch 300: loss -0.265\nEpoch 350: loss -0.266\nEpoch 400: loss -0.271\nEpoch 450: loss -0.287\nEpoch 500: loss -0.329\nEpoch 550: loss -0.333\nEpoch 600: loss -0.338\nEpoch 650: loss -0.341\nEpoch 700: loss -0.342\nEpoch 750: loss -0.344\nEpoch 800: loss -0.345\nEpoch 850: loss -0.343\nEpoch 900: loss -0.345\nEpoch 950: loss -0.349\n\n\n-0.347493439912796\n\n\n\n# Plot the trained model\nwith torch.no_grad():\n    y_hat, log_noise_std = hetero_model_homo_noise_learnable(x_lin[:, None])\n    aleatoric_std = torch.exp(log_noise_std)\n\nplot_results(y_hat, aleatoric_std=aleatoric_std, model_name=\"Untrained model\")\n\n\n\n\n\n\nData with heteroskedastic noise\n\ntorch.manual_seed(42)\nN = 100\nx_lin = torch.linspace(-1, 1, N)\n\nf = lambda x: 0.5 * x**2 + 0.25 * x**3\n\neps = torch.randn(N) * (0.1 + 0.4 * x_lin)\n\ny = f(x_lin) + eps\n\n# Move to GPU\nx_lin = x_lin.to(device)\ny = y.to(device)\n\n# Plot data and true function\nplt.plot(x_lin.cpu(), y.cpu(), \"o\", label=\"data\")\nplt.plot(x_lin.cpu(), f(x_lin).cpu(), label=\"true function\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7faa2587bb20&gt;\n\n\n\n\n\n\nhetero_model_homo_noise_fixed = MeanEstimateNN().to(device)\nfixed_log_noise_std = torch.log(torch.tensor(0.5)).to(device)\n\n# Plot the untrained model\nwith torch.no_grad():\n    y_hat = hetero_model_homo_noise_fixed(x_lin[:, None]).squeeze()\n    aleatoric_std = torch.exp(fixed_log_noise_std)\n\nplot_results(y_hat, aleatoric_std=aleatoric_std, model_name=\"Untrained model\")\n\n\n\n\n\nparams = {\n    \"nn_params\": list(hetero_model_homo_noise_fixed.parameters()),\n    \"log_noise_std\": fixed_log_noise_std,\n}\ntrain_fn(\n    hetero_model_homo_noise_fixed,\n    loss_homoskedastic_noise,\n    params,\n    x_lin[:, None],\n    y,\n    n_epochs=1000,\n    lr=0.001,\n)\n\n# Plot the trained model\nwith torch.no_grad():\n    y_hat = hetero_model_homo_noise_fixed(x_lin[:, None]).squeeze()\n    aleatoric_std = torch.exp(fixed_log_noise_std)\n\nplot_results(y_hat, aleatoric_std=aleatoric_std, model_name=\"Trained model\")\n\nEpoch 0: loss 0.404\nEpoch 50: loss 0.393\nEpoch 100: loss 0.386\nEpoch 150: loss 0.376\nEpoch 200: loss 0.365\nEpoch 250: loss 0.355\nEpoch 300: loss 0.346\nEpoch 350: loss 0.339\nEpoch 400: loss 0.335\nEpoch 450: loss 0.332\nEpoch 500: loss 0.331\nEpoch 550: loss 0.330\nEpoch 600: loss 0.329\nEpoch 650: loss 0.329\nEpoch 700: loss 0.329\nEpoch 750: loss 0.328\nEpoch 800: loss 0.328\nEpoch 850: loss 0.328\nEpoch 900: loss 0.328\nEpoch 950: loss 0.327\n\n\n\n\n\n\n# Now, fit the homoskedastic model with learned noise\n\nhomo_model_hetero_noise_learnable = MeanEstimateNN().to(device)\nlog_noise_std = torch.nn.Parameter(torch.tensor(0.0).to(device))\n\n# Plot the untrained model\nwith torch.no_grad():\n    y_hat = homo_model_hetero_noise_learnable(x_lin[:, None]).squeeze()\n    aleatoric_std = torch.exp(log_noise_std)\n\nplot_results(y_hat, aleatoric_std=aleatoric_std, model_name=\"Untrained model\")\n\n\n\n\n\n# Train the model\nparams = {\n    \"nn_params\": list(homo_model_hetero_noise_learnable.parameters()),\n    \"log_noise_std\": log_noise_std,\n}\n\ntrain_fn(\n    homo_model_hetero_noise_learnable,\n    loss_homoskedastic_noise,\n    params,\n    x_lin[:, None],\n    y,\n    n_epochs=1000,\n    lr=0.01,\n)\n\n# Plot the trained model\nwith torch.no_grad():\n    y_hat = homo_model_hetero_noise_learnable(x_lin[:, None]).squeeze()\n    aleatoric_std = torch.exp(log_noise_std)\n\nplot_results(y_hat, aleatoric_std=aleatoric_std, model_name=\"Trained model\")\n\nEpoch 0: loss 0.980\nEpoch 50: loss 0.487\nEpoch 100: loss 0.105\nEpoch 150: loss -0.137\nEpoch 200: loss -0.225\nEpoch 250: loss -0.245\nEpoch 300: loss -0.242\nEpoch 350: loss -0.248\nEpoch 400: loss -0.254\nEpoch 450: loss -0.253\nEpoch 500: loss -0.252\nEpoch 550: loss -0.254\nEpoch 600: loss -0.251\nEpoch 650: loss -0.255\nEpoch 700: loss -0.255\nEpoch 750: loss -0.254\nEpoch 800: loss -0.252\nEpoch 850: loss -0.254\nEpoch 900: loss -0.250\nEpoch 950: loss -0.255\n\n\n\n\n\n\n# Now, fit the heteroskedastic model\n\nhetero_model_hetero_noise_learnable = HeteroskedasticNN().to(device)\n\n# Plot the untrained model\nwith torch.no_grad():\n    y_hat, log_noise_std = hetero_model_hetero_noise_learnable(x_lin[:, None])\n    aleatoric_std = torch.exp(log_noise_std)\n\nplot_results(y_hat, aleatoric_std=aleatoric_std, model_name=\"Untrained model\")\n\n\n\n\n\n# Train the model\nparams = list(hetero_model_hetero_noise_learnable.parameters())\n\ntrain_fn(\n    hetero_model_hetero_noise_learnable,\n    loss_heteroskedastic,\n    params,\n    x_lin[:, None],\n    y,\n    n_epochs=1000,\n    lr=0.01,\n)\n\nEpoch 0: loss 0.857\nEpoch 50: loss -0.142\nEpoch 100: loss -0.373\nEpoch 150: loss -0.456\nEpoch 200: loss -0.541\nEpoch 250: loss -0.577\nEpoch 300: loss -0.594\nEpoch 350: loss -0.603\nEpoch 400: loss -0.608\nEpoch 450: loss -0.611\nEpoch 500: loss -0.612\nEpoch 550: loss -0.622\nEpoch 600: loss -0.623\nEpoch 650: loss -0.621\nEpoch 700: loss -0.624\nEpoch 750: loss -0.632\nEpoch 800: loss -0.628\nEpoch 850: loss -0.637\nEpoch 900: loss -0.638\nEpoch 950: loss -0.636\n\n\n-0.6415937542915344\n\n\n\n# Plot the trained model\nwith torch.no_grad():\n    y_hat, log_noise_std = hetero_model_hetero_noise_learnable(x_lin[:, None])\n    aleatoric_std = torch.exp(log_noise_std)\n\nplot_results(y_hat, aleatoric_std=aleatoric_std, model_name=\"Trained model\")\n\n\n\n\n\n\nEpistemic Uncertainty: Bayesian NN with Laplace approximation\n\nMAP estimation\n\ndef negative_log_prior(model):\n    log_prior = 0\n\n    for param in model.parameters():\n        log_prior += torch.distributions.Normal(0, 1).log_prob(param).sum()\n    return -log_prior\n\n\ndef negative_log_likelihood(model, x, y, log_noise_std):\n    mu_hat = model(x).squeeze()\n    assert mu_hat.shape == y.shape\n    noise_std = torch.exp(log_noise_std).expand_as(mu_hat)\n    dist = torch.distributions.Normal(mu_hat, noise_std)\n    return -dist.log_prob(y).sum()\n\n\ndef negative_log_joint(model, x, y, log_noise_std):\n    return negative_log_likelihood(model, x, y, log_noise_std) + negative_log_prior(\n        model\n    )\n\n\ndef custom_loss_fn(model, x, y, params):\n    log_noise_std = params[\"log_noise_std\"]\n    return negative_log_joint(model, x, y, log_noise_std)\n\n\ntorch.manual_seed(3)\nlaplace_model = MeanEstimateNN().to(device)\nfixed_log_noise_std = torch.log(torch.tensor(0.2)).to(device)\nparams = {\n    \"log_noise_std\": fixed_log_noise_std,\n    \"nn_params\": list(laplace_model.parameters()),\n}\n\ntrain_fn(\n    laplace_model,\n    custom_loss_fn,\n    params,\n    x_lin[:, None],\n    y,\n    n_epochs=1000,\n    lr=0.01,\n)\n\nEpoch 0: loss 439.902\nEpoch 50: loss 138.376\nEpoch 100: loss 131.914\nEpoch 150: loss 131.173\nEpoch 200: loss 130.548\nEpoch 250: loss 126.990\nEpoch 300: loss 125.752\nEpoch 350: loss 125.163\nEpoch 400: loss 123.350\nEpoch 450: loss 122.710\nEpoch 500: loss 122.472\nEpoch 550: loss 122.251\nEpoch 600: loss 122.128\nEpoch 650: loss 122.113\nEpoch 700: loss 121.922\nEpoch 750: loss 121.868\nEpoch 800: loss 121.792\nEpoch 850: loss 121.794\nEpoch 900: loss 121.696\nEpoch 950: loss 121.726\n\n\n121.6390151977539\n\n\n\nwith torch.no_grad():\n    y_hat = laplace_model(x_lin[:, None]).squeeze()\n    aleatoric_std = torch.exp(fixed_log_noise_std)\n\nplot_results(y_hat, aleatoric_std=aleatoric_std, model_name=\"MAP estimate\")\n\n\n\n\n\n\n\nWhat weighs to consider?\n\n\n\nGoal: Compute the Hessian of the negative log joint wrt the last layer weights\n\n\nChallenge: The negative log joint is a function of all the weights, not just the last layer weights\n\n\nAside on functools.partial\n\nThe functools module is for higher-order functions: functions that act on or return other functions. In general, any callable object can be treated as a function for the purposes of this module.\n\n\nprint(int(\"1001\", base=2), int(\"1001\", base=4), int(\"1001\"))\n\nfrom functools import partial\n\nbase_two = partial(int, base=2)\nbase_two.__doc__ = \"Convert base 2 string to an int.\"\n\nprint(base_two)\nprint(base_two.__doc__)\nprint(help(base_two))\nprint(base_two(\"1001\"))\n\n9 65 1001\nfunctools.partial(&lt;class 'int'&gt;, base=2)\nConvert base 2 string to an int.\nHelp on partial:\n\nfunctools.partial(&lt;class 'int'&gt;, base=2)\n    Convert base 2 string to an int.\n\nNone\n9\n\n\n\n\nA primer on functional calls to PyTorch\n\ntiny_model = torch.nn.Linear(3, 1)\ninput = torch.randn(2, 3)\ntarget = torch.randn(2, 1)\ntiny_model\n\nLinear(in_features=3, out_features=1, bias=True)\n\n\n\noutput = tiny_model(input)\noutput\n\ntensor([[ 0.0756],\n        [-0.2420]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\nloss_fn = torch.nn.MSELoss()\nloss = loss_fn(output, target)\nloss.backward()\n\ngrad_dict = {\"weight\": tiny_model.weight.grad, \"bias\": tiny_model.bias.grad}\ngrad_dict\n\n{'weight': tensor([[ 2.2469, -1.7073, -0.6623]]), 'bias': tensor([-1.9448])}\n\n\n\nparams = dict(tiny_model.named_parameters())\noutput = torch.func.functional_call(tiny_model, params, input)\noutput\n\ntensor([[ 0.0756],\n        [-0.2420]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\ndef custom_loss_fn(params):\n    output = torch.func.functional_call(tiny_model, params, input)\n    return loss_fn(output, target)\n\n\ntorch.func.grad(custom_loss_fn, argnums=0)(params)\n\n{'weight': tensor([[ 2.2469, -1.7073, -0.6623]], grad_fn=&lt;TBackward0&gt;),\n 'bias': tensor([-1.9448], grad_fn=&lt;ViewBackward0&gt;)}\n\n\nIt is also possible to get the gradients/hessian with respect to only a few weights.\n\ndef custom_loss_fn(partial_params, params):\n    params.update(partial_params)\n    output = torch.func.functional_call(tiny_model, params, input)\n    return loss_fn(output, target)\n\n\npartial_params = {\"bias\": params[\"bias\"]}\ntorch.func.grad(custom_loss_fn, argnums=0)(partial_params, params)\n\n{'bias': tensor([-1.9448], grad_fn=&lt;ViewBackward0&gt;)}\n\n\n\n\n\nLast layer Laplace approximation\n\ndef functional_negative_log_prior(partial_params):\n    partial_parameter_leaves = jtu.tree_leaves(partial_params)\n\n    log_prior = 0.0\n    for param in partial_parameter_leaves:\n        log_prior += torch.distributions.Normal(0, 1).log_prob(param).sum()\n    return -log_prior\n\n\ndef functional_negative_log_likelihood(\n    partial_params, params, model, x, y, log_noise_std\n):\n    params.update(partial_params)\n\n    mu_hat = torch.func.functional_call(model, params, x).squeeze()\n    assert mu_hat.shape == y.shape, f\"{mu_hat.shape} != {y.shape}\"\n\n    noise_std = torch.exp(log_noise_std).expand_as(mu_hat)\n    dist = torch.distributions.Normal(mu_hat, noise_std)\n    return -dist.log_prob(y).sum()\n\n\ndef functional_negative_log_joint(partial_params, params, model, x, y, log_noise_std):\n    return functional_negative_log_likelihood(\n        partial_params, params, model, x, y, log_noise_std\n    ) + functional_negative_log_prior(partial_params)\n\n\nparams = dict(laplace_model.named_parameters())\nparams.keys()\n\ndict_keys(['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n\n\n\npartial_params = {\"fc3.weight\": params[\"fc3.weight\"]}\n\n\nprint(\"Full negative log prior\", negative_log_prior(laplace_model))\nprint(\n    \"Partial negative log prior\",\n    functional_negative_log_prior(partial_params),\n)\n\nFull negative log prior tensor(135.0255, device='cuda:0', grad_fn=&lt;NegBackward0&gt;)\nPartial negative log prior tensor(10.9603, device='cuda:0', grad_fn=&lt;NegBackward0&gt;)\n\n\n\nprint(\n    \"Full negative log likelihood\",\n    negative_log_likelihood(laplace_model, x_lin[:, None], y, fixed_log_noise_std),\n)\nprint(\n    \"Partial negative log likelihood\",\n    functional_negative_log_likelihood(\n        partial_params, params, laplace_model, x_lin[:, None], y, fixed_log_noise_std\n    ),\n)\n\nFull negative log likelihood tensor(-13.3980, device='cuda:0', grad_fn=&lt;NegBackward0&gt;)\nPartial negative log likelihood tensor(-13.3980, device='cuda:0', grad_fn=&lt;NegBackward0&gt;)\n\n\n\nprint(\n    \"Full negative log joint\",\n    negative_log_joint(laplace_model, x_lin[:, None], y, fixed_log_noise_std),\n)\nprint(\n    \"Partial negative log joint\",\n    functional_negative_log_joint(\n        partial_params, params, laplace_model, x_lin[:, None], y, fixed_log_noise_std\n    ),\n)\n\nFull negative log joint tensor(121.6275, device='cuda:0', grad_fn=&lt;AddBackward0&gt;)\nPartial negative log joint tensor(-2.4377, device='cuda:0', grad_fn=&lt;AddBackward0&gt;)\n\n\n\nmap_params = dict(laplace_model.named_parameters())\nlast_layer_params = {\"fc3.weight\": map_params[\"fc3.weight\"]}\n\npartial_func = partial(\n    functional_negative_log_joint,\n    params=map_params,\n    model=laplace_model,\n    x=x_lin[:, None],\n    y=y,\n    log_noise_std=fixed_log_noise_std,\n)\n\nH = torch.func.hessian(partial_func)(last_layer_params)[\"fc3.weight\"][\"fc3.weight\"]\nprint(H.shape)\n\nH = H[0, :, 0, :]\nprint(H.shape)\n\ntorch.Size([1, 10, 1, 10])\ntorch.Size([10, 10])\n\n\n\nimport seaborn as sns\n\nwith torch.no_grad():\n    cov = torch.inverse(H + 1e-3 * torch.eye(H.shape[0]).to(device))\n\nplt.figure(figsize=(15, 3))\nsns.heatmap(cov.cpu().numpy(), annot=True, fmt=\".2f\", cmap=\"viridis\")\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nlaplace_posterior = torch.distributions.MultivariateNormal(\n    last_layer_params[\"fc3.weight\"].ravel(), cov\n)\nlast_layer_weights_samples = laplace_posterior.sample((501,))[..., None]\nlast_layer_weights_samples.shape\n\ntorch.Size([501, 10, 1])\n\n\n\ndef forward_pass(last_layer_weight, params):\n    params.update({\"fc3.weight\": last_layer_weight.reshape(1, -1)})\n    return torch.func.functional_call(laplace_model, params, x_lin[:, None]).squeeze()\n\n\nforward_pass(last_layer_weights_samples[0], params).shape\n\ntorch.Size([100])\n\n\n\nmc_outputs = torch.vmap(lambda x: forward_pass(x, params))(last_layer_weights_samples)\nprint(mc_outputs.shape)\n\ntorch.Size([501, 100])\n\n\n\nmean_mc_outputs = mc_outputs.mean(0)\nstd_mc_outputs = mc_outputs.std(0)\nmean_mc_outputs.shape, std_mc_outputs.shape\n\n(torch.Size([100]), torch.Size([100]))\n\n\n\nwith torch.no_grad():\n    epistemic_std = std_mc_outputs\n    aleatoric_std = torch.exp(fixed_log_noise_std) + epistemic_std\n\n    plot_results(\n        mean_mc_outputs,\n        epistemic_std=epistemic_std,\n        aleatoric_std=aleatoric_std,\n        model_name=\"Laplace approximation\",\n    )\n\n\n\n\n\nwith torch.no_grad():\n    epistemic_std = std_mc_outputs\n    aleatoric_std = torch.exp(fixed_log_noise_std) + epistemic_std\n\n    plot_results(\n        mean_mc_outputs,\n        epistemic_std=None,\n        aleatoric_std=None,\n        model_name=\"Laplace approximation\",\n    )\n\n    for i in range(10):\n        plt.plot(\n            x_lin.cpu(),\n            mc_outputs[i].cpu(),\n            alpha=0.3,\n            color=\"C2\",\n            label=\"Laplace approximation\",\n        )"
  },
  {
    "objectID": "notebooks/normal_likelihood_lin_reg.html",
    "href": "notebooks/normal_likelihood_lin_reg.html",
    "title": "Probabilistic Machine Learning",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Retina display\n%config InlineBackend.figure_format = 'retina'\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom matplotlib.animation import FuncAnimation\nfrom IPython.display import HTML\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n\ndef true_function(x):\n    return 2 * x + 3\n\n\nx_values = torch.linspace(0, 10, 100)\nnoise = torch.randn(100) * 2\ny_values = true_function(x_values) + noise\nx_values = x_values.view(-1, 1)\ny_values = y_values.view(-1, 1)\n\nmean = 3\nstddev = 0.7\nx = np.linspace(0, 6, 100)\npdf_values = (1.0 / (stddev * np.sqrt(2 * np.pi))) * \\\n    np.exp(-0.5 * ((x - mean) / stddev)**2)\n\nplt.scatter(x_values, y_values, label='Data with Noise', color='gray')\nplt.plot(x_values, true_function(x_values),\n         label='True Function', color='tab:blue')\nplt.plot(pdf_values+3, x+6.1, color='tab:orange')\nplt.plot(pdf_values+6, x+12.1, color='tab:orange')\nplt.plot(pdf_values+5, x+10.2, color='tab:orange')\nplt.scatter(3, 9, color='tab:orange')\nplt.scatter(6, 15, color='tab:orange')\nplt.scatter(5, 13, color='tab:orange')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Linear Regression')\nplt.legend()\nplt.savefig('figures/mle/true_function_noise_normal_lin_reg.pdf')\n\nFileNotFoundError: [Errno 2] No such file or directory: 'figures/mle/true_function_noise_normal_lin_reg.pdf'\n\n\n\n\n\n\ndef animate(frame):\n    plt.clf()\n\n    if frame == 0:\n        plt.plot(x_values, true_function(x_values),\n                 label='True Function', color='tab:blue')\n    elif frame == 1:\n        plt.plot(x_values, true_function(x_values),\n                 label='True Function', color='tab:blue')\n        plt.scatter(x_values, y_values, label='Data with Noise', color='gray')\n    elif frame == 2:\n        plt.plot(x_values, true_function(x_values),\n                 label='True Function', color='tab:blue')\n        plt.scatter(x_values, y_values, label='Data with Noise', color='gray')\n        plt.plot(pdf_values+3, x+6.1, color='tab:orange')\n        plt.plot(pdf_values+6, x+12.1, color='tab:orange')\n        plt.plot(pdf_values+5, x+10.2, color='tab:orange')\n        plt.scatter(3, 9, color='tab:orange')\n        plt.scatter(6, 15, color='tab:orange')\n        plt.scatter(5, 13, color='tab:orange')\n\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title('Linear Regression')\n    plt.legend()\n\n\nfig = plt.figure()\nfig.set_facecolor('white')\nanim = FuncAnimation(fig, animate, frames=3, interval=2000, repeat=True)\nHTML(anim.to_jshtml())\n\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "notebooks/visualise_normal.html",
    "href": "notebooks/visualise_normal.html",
    "title": "Probabilistic Machine Learning",
    "section": "",
    "text": "import torch\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nfrom ipywidgets import interact, FloatSlider\n\ndef visualize_bivariate_gaussian(mu_0, mu_1, cov_00, cov_01, cov_10, cov_11):\n    mean = torch.tensor([mu_0, mu_1])\n    covariance_matrix = torch.tensor([[cov_00, cov_01],\n                                      [cov_10, cov_11]])\n    bivariate_dist = torch.distributions.MultivariateNormal(mean, covariance_matrix)\n    N = 200\n    theta_0 = torch.linspace(-3, 3, N)\n    theta_1 = torch.linspace(-3, 3, N)\n    Theta_0, Theta_1 = torch.meshgrid(theta_0, theta_1)\n    pos = torch.stack((Theta_0, Theta_1), dim=2)\n    density = torch.exp(bivariate_dist.log_prob(pos))\n\n    custom_cmap = cm.get_cmap('viridis')\n\n    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n\n    contour = axs[0].contourf(Theta_0, Theta_1, density, cmap=custom_cmap, levels=20)\n    fig.colorbar(contour, ax=axs[0])\n    axs[0].set_xlabel('θ\\u2080')\n    axs[0].set_ylabel('θ\\u2081')\n    axs[0].set_title(f'Bivariate Gaussian Contour\\nμ = {mean.tolist()}, Covariance = {covariance_matrix.tolist()}')\n    axs[0].set_aspect('equal')  # Set equal aspect ratio\n\n    axs[1] = fig.add_subplot(122, projection='3d')\n    surface = axs[1].plot_surface(Theta_0, Theta_1, density, cmap=custom_cmap)\n    axs[1].set_xlabel('θ\\u2080')\n    axs[1].set_ylabel('θ\\u2081')\n    axs[1].set_zlabel('Density')\n    axs[1].set_title(f'Bivariate Gaussian Surface\\nμ = {mean.tolist()}, Covariance = {covariance_matrix.tolist()}')\n\n    plt.tight_layout()\n    plt.show()\n\n\nvisualize_bivariate_gaussian(mu_0=0.0, mu_1=0.0, cov_00=1.0, cov_01=0.0, cov_10=0.0, cov_11=1.0)\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n\n\n\n\n\n\n# Create interactive version\ninteract(visualize_bivariate_gaussian,\n         mu_0=FloatSlider(value=0.0, min=-2.0, max=2.0, step=0.1, description='μ\\u2080'),\n         mu_1=FloatSlider(value=0.0, min=-2.0, max=2.0, step=0.1, description='μ\\u2081'),\n         cov_00=FloatSlider(value=1.0, min=0.1, max=2.0, step=0.1, description='σ\\u2080\\u2080'),\n         cov_01=FloatSlider(value=0.5, min=-1.0, max=1.0, step=0.1, description='σ\\u2080\\u2081'),\n         cov_10=FloatSlider(value=0.5, min=-1.0, max=1.0, step=0.1, description='σ\\u2081\\u2080'),\n         cov_11=FloatSlider(value=1.0, min=0.1, max=2.0, step=0.1, description='σ\\u2081\\u2081'))\n\n\n\n\n&lt;function __main__.visualize_bivariate_gaussian(mu_0, mu_1, cov_00, cov_01, cov_10, cov_11)&gt;"
  },
  {
    "objectID": "notebooks/blr_posterior_figures.html",
    "href": "notebooks/blr_posterior_figures.html",
    "title": "Bayesian Linear Regression Posterior Figures",
    "section": "",
    "text": "# Bayesian inference for simple linear regression with known noise variance\n# The goal is to reproduce fig 3.7 from Bishop's book.\n# We fit the linear model f(x,w) = w0 + w1*x and plot the posterior over w.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\n# try:\n#     import probml_utils as pml\n# except ModuleNotFoundError:\n#     %pip install -qq git+https://github.com/probml/probml-utils.git\n#     import probml_utils as pml\nfrom scipy.stats import uniform, norm, multivariate_normal\nfrom tueplots import bundles\nplt.rcParams.update(bundles.icml2022())\n\n\nplt.rcParams.update({'figure.figsize': (4.5, 2.0086104634371584)})\n\n\nnp.random.seed(0)\n\n# Number of samples to draw from posterior distribution of parameters.\nNSamples = 10\n\n# Each of these corresponds to a row in the graphic and an amount of data the posterior will reflect.\n# First one must be zero, for the prior.\nDataIndices = [0, 1, 2, 5, 10, 20, 100]\n\n# True regression parameters that we wish to recover. Do not set these outside the range of [-1,1]\na0 = -0.3\na1 = 0.5\n\nNPoints = 100  # Number of (x,y) training points\nnoiseSD = 0.2  # True noise standard deviation\npriorPrecision = 2.0  # Fix the prior precision, alpha. We will use a zero-mean isotropic Gaussian.\nlikelihoodSD = noiseSD  # Assume the likelihood precision, beta, is known.\nlikelihoodPrecision = 1.0 / (likelihoodSD**2)\n\n# Because of how axises are set up, x and y values should be in the same range as the coefficients.\n\nx = 2 * uniform().rvs(NPoints) - 1\ny = a0 + a1 * x + norm(0, noiseSD).rvs(NPoints)\n\n\ndef MeanCovPost(x, y):\n    # Given data vectors x and y, this returns the posterior mean and covariance.\n    X = np.array([[1, x1] for x1 in x])\n    Precision = np.diag([priorPrecision] * 2) + likelihoodPrecision * X.T.dot(X)\n    Cov = np.linalg.inv(Precision)\n    Mean = likelihoodPrecision * Cov.dot(X.T.dot(y))\n    return {\"Mean\": Mean, \"Cov\": Cov}\n\n\ndef GaussPdfMaker(mean, cov):\n    # For a given (mean, cov) pair, this returns a vectorized pdf function.\n    def out(w1, w2):\n        return multivariate_normal.pdf([w1, w2], mean=mean, cov=cov)\n\n    return np.vectorize(out)\n\n\ndef LikeFMaker(x0, y0):\n    # For a given (x,y) pair, this returns a vectorized likelhood function.\n    def out(w1, w2):\n        err = y0 - (w1 + w2 * x0)\n        return norm.pdf(err, loc=0, scale=likelihoodSD)\n\n    return np.vectorize(out)\n\n\n# Grid space for which values will be determined, which is shared between the coefficient space and data space.\ngrid = np.linspace(-1, 1, 50)\nXg = np.array([[1, g] for g in grid])\nG1, G2 = np.meshgrid(grid, grid)\n\n# If we have many samples of lines, we make them a bit transparent.\nalph = 5.0 / NSamples if NSamples &gt; 50 else 1.0\n\n# A function to make some common adjustments to our subplots.\ndef adjustgraph(whitemark):\n    if whitemark:\n        plt.ylabel(r\"$\\theta_0$\")\n        plt.xlabel(r\"$\\theta_1$\")\n        # plt.scatter(a0, a1, marker=\"+\", color=\"white\", s=100)\n    else:\n        plt.ylabel(\"y\")\n        plt.xlabel(\"x\")\n    plt.ylim([-1, 1])\n    plt.xlim([-1, 1])\n    plt.xticks([-1, 0, 1])\n    plt.yticks([-1, 0, 1])\n    return None\n\n\ndef create_figure(data_index):\n    fig = plt.figure()\n    \n    # Left graph\n    if(data_index==0):\n      ax1 = fig.add_subplot(1, 3, 1)\n      ax1.set_title(\"likelihood\")\n      ax1.axis(\"off\")\n    else:\n      ax1 = fig.add_subplot(1, 3, 1)\n      likfunc = LikeFMaker(x[di - 1], y[di - 1])\n      ax1.contourf(G1, G2, likfunc(G1, G2), 100)\n      adjustgraph(True)\n      ax1.set_title(\"likelihood\")\n      # ax1.axis(\"off\")\n    \n    # Middle graph\n    ax2 = fig.add_subplot(1, 3, 2)\n    postfunc = GaussPdfMaker(postM, postCov)\n    ax2.contourf(G1, G2, postfunc(G1, G2), 100)\n    adjustgraph(True)\n    ax2.set_title(\"prior/posterior\")\n    \n    # Right graph\n    ax3 = fig.add_subplot(1, 3, 3)\n    Samples = multivariate_normal(postM, postCov).rvs(NSamples)\n    Lines = Xg.dot(Samples.T)\n    if data_index != 0:\n        ax3.scatter(x[:data_index], y[:data_index], s=140, facecolors=\"none\", edgecolors=\"b\")\n    for j in range(Lines.shape[1]):\n        ax3.plot(grid, Lines[:, j], linewidth=2, color=\"r\", alpha=alph)\n    adjustgraph(False)\n    ax3.set_title('data space')\n    \n    return fig\n\n# Loop through DataIndices to create separate figures for each row\nfor di in DataIndices:\n    if di == 0:\n        postM = [0, 0]\n        postCov = np.diag([1.0 / priorPrecision] * 2)\n    else:\n        Post = MeanCovPost(x[:di], y[:di])\n        postM = Post[\"Mean\"]\n        postCov = Post[\"Cov\"]\n    \n    fig = create_figure(di)\n    \n    plt.tight_layout()\n    plt.savefig(f\"blr_{di}.png\", dpi=900)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm, multivariate_normal\n\nnp.random.seed(0)\n\nNSamples = 10\nDataIndices = [0, 1, 2, 5, 10, 20, 100]\na0 = -0.3\na1 = 0.5\nNPoints = 100\nnoiseSD = 0.2\npriorPrecision = 2.0\nlikelihoodSD = noiseSD\nlikelihoodPrecision = 1.0 / (likelihoodSD**2)\n\nx = 2 * np.random.random(NPoints) - 1\ny = a0 + a1 * x + np.random.normal(0, noiseSD, NPoints)\n\ndef MeanCovPost(x, y):\n    X = np.array([[1, x1] for x1 in x])\n    Precision = np.diag([priorPrecision] * 2) + likelihoodPrecision * X.T.dot(X)\n    Cov = np.linalg.inv(Precision)\n    Mean = likelihoodPrecision * Cov.dot(X.T.dot(y))\n    return {\"Mean\": Mean, \"Cov\": Cov}\n\ndef GaussPdfMaker(mean, cov):\n    def out(w1, w2):\n        return multivariate_normal.pdf([w1, w2], mean=mean, cov=cov)\n    return np.vectorize(out)\n\ndef LikeFMaker(x0, y0):\n    def out(w1, w2):\n        err = y0 - (w1 + w2 * x0)\n        return norm.pdf(err, loc=0, scale=likelihoodSD)\n    return np.vectorize(out)\n\ngrid = np.linspace(-1, 1, 50)\nXg = np.array([[1, g] for g in grid])\nG1, G2 = np.meshgrid(grid, grid)\n\nalph = 5.0 / NSamples if NSamples &gt; 50 else 1.0\n\ndef adjustgraph(whitemark):\n    if whitemark:\n        plt.ylabel(r\"$\\theta_0$\")\n        plt.xlabel(r\"$\\theta_1$\")\n    else:\n        plt.ylabel(\"y\")\n        plt.xlabel(\"x\")\n    plt.ylim([-1, 1])\n    plt.xlim([-1, 1])\n    plt.xticks([-1, 0, 1])\n    plt.yticks([-1, 0, 1])\n    return None\n\ndef create_figure(data_index):\n    fig = plt.figure(figsize=(15, 5))\n    \n    ax1 = fig.add_subplot(1, 3, 1)\n    if data_index == 0:\n        ax1.set_title(\"likelihood\")\n        ax1.axis(\"off\")\n    else:\n        likfunc = LikeFMaker(x[di - 1], y[di - 1])\n        ax1.contourf(G1, G2, likfunc(G1, G2), 100)\n        adjustgraph(True)\n        ax1.set_title(\"likelihood\")\n    \n    ax2 = fig.add_subplot(1, 3, 2)\n    postfunc = GaussPdfMaker(postM, postCov)\n    ax2.contourf(G1, G2, postfunc(G1, G2), 100)\n    adjustgraph(True)\n    ax2.set_title(\"prior/posterior\")\n    \n    ax3 = fig.add_subplot(1, 3, 3)\n    Samples = multivariate_normal(postM, postCov).rvs(NSamples)\n    Lines = Xg.dot(Samples.T)\n    if data_index != 0:\n        ax3.scatter(x[:data_index], y[:data_index], s=140, facecolors=\"none\", edgecolors=\"b\")\n    for j in range(Lines.shape[1]):\n        ax3.plot(grid, Lines[:, j], linewidth=2, color=\"r\", alpha=alph)\n    adjustgraph(False)\n    ax3.set_title('data space')\n    \n    plt.tight_layout()\n    return fig\n\nfor di in DataIndices:\n    if di == 0:\n        postM = [0, 0]\n        postCov = np.diag([1.0 / priorPrecision] * 2)\n    else:\n        Post = MeanCovPost(x[:di], y[:di])\n        postM = Post[\"Mean\"]\n        postCov = Post[\"Cov\"]\n    \n    fig = create_figure(di)\n    #plt.savefig(f\"blr_{di}.png\", dpi=300)\n    plt.show()\n\n/tmp/ipykernel_3006897/2174210570.py:86: UserWarning: This figure was using constrained_layout, but that is incompatible with subplots_adjust and/or tight_layout; disabling constrained_layout.\n  plt.tight_layout()\n/tmp/ipykernel_3006897/2174210570.py:86: UserWarning: This figure was using constrained_layout, but that is incompatible with subplots_adjust and/or tight_layout; disabling constrained_layout.\n  plt.tight_layout()\n/tmp/ipykernel_3006897/2174210570.py:86: UserWarning: This figure was using constrained_layout, but that is incompatible with subplots_adjust and/or tight_layout; disabling constrained_layout.\n  plt.tight_layout()\n/tmp/ipykernel_3006897/2174210570.py:86: UserWarning: This figure was using constrained_layout, but that is incompatible with subplots_adjust and/or tight_layout; disabling constrained_layout.\n  plt.tight_layout()\n/tmp/ipykernel_3006897/2174210570.py:86: UserWarning: This figure was using constrained_layout, but that is incompatible with subplots_adjust and/or tight_layout; disabling constrained_layout.\n  plt.tight_layout()\n/tmp/ipykernel_3006897/2174210570.py:86: UserWarning: This figure was using constrained_layout, but that is incompatible with subplots_adjust and/or tight_layout; disabling constrained_layout.\n  plt.tight_layout()\n/tmp/ipykernel_3006897/2174210570.py:86: UserWarning: This figure was using constrained_layout, but that is incompatible with subplots_adjust and/or tight_layout; disabling constrained_layout.\n  plt.tight_layout()"
  },
  {
    "objectID": "notebooks/information-theory.html",
    "href": "notebooks/information-theory.html",
    "title": "Closed form solution for prior predictive distribution",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n%matplotlib inline\n\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.icml2022())\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\n\n# Self information function\ndef self_information(p):\n    return -np.log2(p)\n\n# Plot self information function from 0 to 1\nx = np.linspace(0.0001, 1, 500)\ny = self_information(x)\nplt.plot(x, y)\nplt.xlabel('Probability (p)')\nplt.ylabel('Self information (bits)')\nplt.savefig('figures/information-theory/self-information.pdf', bbox_inches='tight')\n\n\n\n\n\ncategorical_1 = torch.distributions.Categorical(probs = torch.tensor([0.25, 0.25, 0.25, 0.25]))\n\nser_1 = pd.Series(index=['A', 'B', 'C', 'D'], data=categorical_1.probs.detach().numpy())\nser_1.plot.bar(rot=0)\nplt.ylabel('Probability')\nplt.savefig('figures/information-theory/categorical-uniform.pdf', bbox_inches='tight')\n\n\n\n\n\ncategorical_2 = torch.distributions.Categorical(probs = torch.tensor([0.5, 0.25, 0.125, 0.125]))\n\nser_2 = pd.Series(index=['A', 'B', 'C', 'D'], data=categorical_2.probs.detach().numpy())\nser_2.plot.bar(rot=0)\nplt.ylabel('Probability')\nplt.savefig('figures/information-theory/categorical-nonuniform.pdf', bbox_inches='tight')\n\n\n\n\n\n# Entropy for a bernoulli distribution\ndef entropy_bernoulli(p):\n    return -(p * np.log2(p) + (1 - p) * np.log2(1 - p))\n\n# Plot entropy for a bernoulli distribution\nx = np.linspace(0.0001, 0.9999, 500)\ny = entropy_bernoulli(x)\nplt.plot(x, y)\nplt.xlabel('Probability (p)')\nplt.ylabel('Entropy (bits)')\nplt.savefig('figures/information-theory/entropy-bernoulli.pdf', bbox_inches='tight')\n\n\n\n\n\n# Figure to take 3 categoriacl distributions on four symbols and title them with their entropy\n# Make it two column figure for TUEplots\n\nplt.rcParams.update(bundles.icml2022(nrows=1, ncols=3))\n\n\nfig, axs = plt.subplots(1, 3, sharey=True)\nser_1.plot.bar(rot=0, ax=axs[0])\naxs[0].set_ylabel('Probability')\n\naxs[0].set_title('Entropy: {:.2f} bits'.format(categorical_1.entropy().item()/np.log(2))) # convert loge to log2\n# Manually also calculate the entropy\np = categorical_1.probs.detach().numpy()\nentropy = -(p * np.log2(p)).sum()\nprint(entropy)\n\nser_2.plot.bar(rot=0, ax=axs[1])\naxs[1].set_title('Entropy: {:.2f} bits'.format(categorical_2.entropy().item()/np.log(2)))\n\ncategorical_3 = torch.distributions.Categorical(probs = torch.tensor([0.9, 0.05, 0.025, 0.025]))\nser_3 = pd.Series(index=['A', 'B', 'C', 'D'], data=categorical_3.probs.detach().numpy())\nser_3.plot.bar(rot=0, ax=axs[2])\naxs[2].set_title('Entropy: {:.2f} bits'.format(categorical_3.entropy().item()/np.log(2)))\n\n\nplt.savefig('figures/information-theory/categorical-entropy.pdf', bbox_inches='tight')\n\n2.0\n\n\n\n\n\n\np = {\"A\": 0.4, \"B\": 0.3, \"C\": 0.2, \"D\": 0.1}\nq = {\"A\": 0.15, \"B\": 0.55, \"C\": 0.05, \"D\": 0.25}\n\ndef entropy(p):\n    return -(np.array(list(p.values())) * np.log2(np.array(list(p.values())))).sum()\n\ndef cross_entropy(p, q):\n    return -(np.array(list(p.values())) * np.log2(np.array(list(q.values())))).sum()\n\ndef kl_divergence(p, q):\n    return cross_entropy(p, q) - entropy(p)\n\nprint(entropy(p))\nprint(cross_entropy(p, q))\nprint(kl_divergence(p, q))\n\n1.8464393446710154\n2.417920799518975\n0.5714814548479596\n\n\n\n#  Average length of a code\n\ncategorical_1.probs\n\ntensor([0.2500, 0.2500, 0.2500, 0.2500])\n\n\n\nsymbols = ['A', 'B', 'C', 'D']\nprobs = [0.25, 0.25, 0.25, 0.25]\ncodes = ['00', '01', '10', '11']\n\ndef average_length(probs, codes, symbols):\n    # Create a dictionary with the symbols and their probabilities\n    symbol_probs = dict(zip(symbols, probs))\n\n    # Create a dictionary with the symbols and their codes\n    symbol_codes = dict(zip(symbols, codes))\n\n    symbol_codes_length = {k: len(v) for k, v in symbol_codes.items()}\n\n    # Calculate the average length of a code\n    average_length_code = sum([symbol_probs[symbol] * symbol_codes_length[symbol] for symbol in symbols])\n    return average_length_code\n\n\naverage_length([0.25, 0.25, 0.25, 0.25], ['00', '01', '10', '11'], ['A', 'B', 'C', 'D'])\n\n2.0\n\n\n\naverage_length([0.5, 0.25, 0.125, 0.125], ['00', '01', '10', '11'], ['A', 'B', 'C', 'D'])\n\n2.0\n\n\n\nimport pygraphviz as pgv\nfrom IPython.display import Image\n# Heapq imports\nimport heapq\n\n\nclass Node:\n    def __init__(self, symbol, probability):\n        self.symbol = symbol\n        self.probability = probability\n        self.left = None\n        self.right = None\n\n    def __lt__(self, other):\n        return self.probability &lt; other.probability\n\ndef huffman_encoding(symbols):\n    # Step 1: Calculate probabilities\n    probabilities = {}\n    total_symbols = len(symbols)\n    for symbol in symbols:\n        probabilities[symbol] = symbols.count(symbol) / total_symbols\n\n    # Step 2: Create initial nodes\n    nodes = [Node(symbol, probability) for symbol, probability in probabilities.items()]\n\n    # Step 3: Sort nodes\n    heapq.heapify(nodes)\n\n    # Step 4: Build Huffman tree\n    while len(nodes) &gt; 1:\n        left = heapq.heappop(nodes)\n        right = heapq.heappop(nodes)\n        parent = Node(None, left.probability + right.probability)\n        parent.left = left\n        parent.right = right\n        heapq.heappush(nodes, parent)\n\n    root = nodes[0]\n\n    # Step 6: Assign binary codes\n    codes = {}\n    def assign_codes(node, code):\n        if node.symbol:\n            codes[node.symbol] = code\n        else:\n            assign_codes(node.left, code + '0')\n            assign_codes(node.right, code + '1')\n\n    assign_codes(root, '')\n\n    # Step 7: Convert dot format to graph\n    G = pgv.AGraph(tree_dot)\n    G.layout(prog='dot')\n\n    return codes, G\n\n# Example usage\nsymbols = ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C']\ncodes, tree_dot = huffman_encoding(symbols)\n\nprint(\"Symbol\\tCode\")\nfor symbol, code in codes.items():\n    print(f\"{symbol}\\t{code}\")\n\n# Plotting the Huffman tree\ntree_dot.draw('huffman_tree.png')\n\nModuleNotFoundError: No module named 'pygraphviz'\n\n\n\n\n\nCleanShot 2023-06-05 at 17.47.27@2x.png\n\n\n\nsigma = 1.0\n\ndef prior_predictive(x, sigma, prior_mean, prior_cov):\n    \"\"\"Closed form prior predictive distribution for linear regression.\"\"\"\n    prior_pred_mean = prior_mean[0] + prior_mean[1] * x\n    prior_pred_cov = sigma ** 2 + x ** 2 * prior_cov[1, 1]\n    return prior_pred_mean, prior_pred_cov"
  },
  {
    "objectID": "notebooks/inverse-cdf.html",
    "href": "notebooks/inverse-cdf.html",
    "title": "Probabilistic Machine Learning",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport arviz as az\n%matplotlib inline\n# Retina display\n%config InlineBackend.figure_format = 'retina'\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\n\n# Define values of lambda\nlambdas = [0.5, 1.0, 1.5]\nx = torch.linspace(0, 5, 100)\n\n# Create a figure with two subplots\nfig, (ax1, ax2) = plt.subplots(1, 2,)\n\n# Plot PDFs in the first subplot\nfor lam in lambdas:\n    exponential_dist = torch.distributions.Exponential(rate=lam)\n    pdf = exponential_dist.log_prob(x).exp()\n    ax1.plot(x, pdf, label=f'$\\lambda = {lam}$')\n\nax1.set_xlabel('x')\nax1.set_ylabel('PDF')\nax1.set_title('Exponential Distribution PDF')\nax1.legend()\n\n# Plot CDFs in the second subplot\nfor lam in lambdas:\n    exponential_dist = torch.distributions.Exponential(rate=lam)\n    cdf = exponential_dist.cdf(x)\n    ax2.plot(x, cdf, label=f'$\\lambda = {lam}$')\n\nax2.set_xlabel('x')\nax2.set_ylabel('CDF')\nax2.set_title('Exponential Distribution CDF')\nax2.legend()\n\n# Show the plots\nplt.tight_layout()\nplt.savefig(\"../figures/sampling/exp-cdf.pdf\")\n\n\n\n\n\nimport torch\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nimport numpy as np\n\n# Create the exponential distribution\n\n\n\n\n\ndef plot_n_samples(n):\n    exponential_dist = torch.distributions.Exponential(rate=1)\n    x = torch.linspace(0, 5, 100)\n\n    # Create the CDF plot\n    fig, ax = plt.subplots()\n    ax.set_xlabel('x')\n    ax.set_ylabel('CDF')\n    ax.set_title('Exponential Distribution CDF')\n\n    # Initialize an empty line for the animated point\n    point, = ax.plot([], [], 'ko', ms=5)\n    # Draw the CDF curve\n    cdf = exponential_dist.cdf(x)\n    ax.plot(x, cdf, label='CDF', lw=2)\n    torch.manual_seed(42)\n    u = torch.rand(n)\n    # Calculate the corresponding point on the CDF\n    point_x = exponential_dist.icdf(u)\n    point_y = u\n    # Add the point to the plot\n    point.set_data(point_x, point_y)\n\n    # Add vertical and horizontal dashed lines\n    ax.vlines(point_x, 0, point_y, linestyle='--', lw=1)\n    ax.hlines(point_y, 0, point_x, linestyle='--', lw=1)\n    plt.savefig(f\"../figures/sampling/exp-cdf-samples-{n}.pdf\")\n\n\n\nfor i in range(1, 10):\n    plot_n_samples(i)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Compare KDE to the true PDF\n\nexpon = torch.distributions.Exponential(rate=1)\nx = torch.linspace(0, 5, 100)\npdf = expon.log_prob(x).exp()\n\n# Sample from the distribution\ntorch.manual_seed(42)\nrandom_numbers = expon.sample((10000,))\n\naz.plot_kde(np.array(random_numbers), rug=False, label='KDE from Torch')\n\nplt.plot(x, pdf, label='True PDF', color='C1')\nplt.legend()\n\n# Samples using inverse CDF method\nu = torch.rand(10000)\nsamples = expon.icdf(u)\naz.plot_kde(np.array(samples), rug=False, label='KDE using inverse CDF', plot_kwargs={\"color\":'C2', \"ls\":\"--\"})\n\nplt.xlim(0, 5)\n\n(0.0, 5.0)\n\n\n\n\n\n\n# Similarly generating samples from a normal distribution\n\nnormal = torch.distributions.Normal(loc=0, scale=1)\nx = torch.linspace(-5, 5, 100)\npdf = normal.log_prob(x).exp()\n\ncdf = normal.cdf(x)\n\nplt.plot(x, pdf, label='PDF')\nplt.plot(x, cdf, label='CDF')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f3eaa3452b0&gt;\n\n\n\n\n\n\nu = torch.rand(10000)\nsamples = normal.icdf(u)\n\naz.plot_kde(np.array(samples), rug=False, label='KDE using inverse CDF', plot_kwargs={\"color\":'C2', \"ls\":\"--\"})\n\nplt.plot(x, pdf, label='True PDF', color='C1')\n\nplt.xlim(-5, 5)\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f3eaa2b6040&gt;\n\n\n\n\n\n\ndef inverse_cdf(u, lam):\n    return -torch.log(1 - u) / lam\n\ndef sample_exponential(n_samples, lam):\n    u = torch.rand(n_samples)\n    return inverse_cdf(u, lam)\n\n\nclass SimplePRNG:\n    def __init__(self, seed=0):\n        self.seed = seed\n        self.a = 1664525\n        self.c = 1013904223\n        self.m = 2**32\n\n    def random(self):\n        self.seed = (self.a * self.seed + self.c) % self.m\n        return self.seed / self.m\n\n    def generate_N_random_numbers(self, N):\n        random_numbers = []\n        for _ in range(N):\n            random_numbers.append(self.random())\n        return random_numbers\n\n# Usage\nprng = SimplePRNG(seed=42)  # You can change the seed value\nN = 10000  # Change N to the number of random numbers you want to generate\nrandom_numbers = prng.generate_N_random_numbers(N)\n\n\n\n_ = plt.hist(random_numbers, bins=10)\n\n\n\n\n\naz.plot_kde(np.array(random_numbers), rug=False)\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\n_ = plt.hist(np.random.rand(10000), bins=10)\n\n\n\n\n\naz.plot_kde(np.random.rand(10000), rug=False)\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\n### Uniform (a, b)\n\na = -2\nb = 2\n\nrandom_numbers_a_b = a + (b - a) * np.array(random_numbers)\n\n\nplt.hist(random_numbers_a_b, bins=10)\n\n(array([1006., 1012.,  963.,  964., 1008.,  939., 1039., 1012., 1031.,\n        1026.]),\n array([-1.99864224e+00, -1.59883546e+00, -1.19902869e+00, -7.99221917e-01,\n        -3.99415144e-01,  3.91628593e-04,  4.00198402e-01,  8.00005174e-01,\n         1.19981195e+00,  1.59961872e+00,  1.99942549e+00]),\n &lt;BarContainer object of 10 artists&gt;)"
  },
  {
    "objectID": "notebooks/bayesian-logistic-regression.html",
    "href": "notebooks/bayesian-logistic-regression.html",
    "title": "Bayesian Logistic Regression",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.distributions as dist\nfrom torch.func import jacfwd, hessian\n\nimport tqdm\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_blobs\n\nfrom tueplots.bundles import beamer_moml\nimport matplotlib.pyplot as plt\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n\n# Use render mode to run the notebook and save the plots in beamer format\n# Use interactive mode to run the notebook and show the plots in notebook-friendly format\nmode = \"render\"  # \"interactive\" or \"render\"\n\nif mode == \"render\":\n    width = 0.6\n    plt.rcParams.update(beamer_moml(rel_width=width, rel_height=width * 0.8))\n    # update marker size\n    plt.rcParams.update({\"lines.markersize\": 4})\n    plt.rcParams[\"figure.facecolor\"] = \"none\"\nelse:\n    plt.rcdefaults()\n\n\ndef plt_show(name=None):\n    if mode == \"interactive\":\n        plt.show()\n    elif mode == \"render\":\n        plt.savefig(f\"../figures/bayesian-logistic-regression/{name}.pdf\")\n    else:\n        raise ValueError(f\"Unknown mode: {mode}\")"
  },
  {
    "objectID": "notebooks/bayesian-logistic-regression.html#imports",
    "href": "notebooks/bayesian-logistic-regression.html#imports",
    "title": "Bayesian Logistic Regression",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.distributions as dist\nfrom torch.func import jacfwd, hessian\n\nimport tqdm\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_blobs\n\nfrom tueplots.bundles import beamer_moml\nimport matplotlib.pyplot as plt\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n\n# Use render mode to run the notebook and save the plots in beamer format\n# Use interactive mode to run the notebook and show the plots in notebook-friendly format\nmode = \"render\"  # \"interactive\" or \"render\"\n\nif mode == \"render\":\n    width = 0.6\n    plt.rcParams.update(beamer_moml(rel_width=width, rel_height=width * 0.8))\n    # update marker size\n    plt.rcParams.update({\"lines.markersize\": 4})\n    plt.rcParams[\"figure.facecolor\"] = \"none\"\nelse:\n    plt.rcdefaults()\n\n\ndef plt_show(name=None):\n    if mode == \"interactive\":\n        plt.show()\n    elif mode == \"render\":\n        plt.savefig(f\"../figures/bayesian-logistic-regression/{name}.pdf\")\n    else:\n        raise ValueError(f\"Unknown mode: {mode}\")"
  },
  {
    "objectID": "notebooks/bayesian-logistic-regression.html#data",
    "href": "notebooks/bayesian-logistic-regression.html#data",
    "title": "Bayesian Logistic Regression",
    "section": "Data",
    "text": "Data\n\nX_np, y_np = make_blobs(\n    n_samples=20, n_features=2, centers=2, random_state=11, cluster_std=1\n)\ndf = pd.DataFrame(dict(x1=X_np[:, 0], x2=X_np[:, 1], y=y_np))\n\ndf.head(4)\n\n\n\n\n\n\n\n\nx1\nx2\ny\n\n\n\n\n0\n-5.973556\n-10.676098\n0\n\n\n1\n-0.439282\n5.901450\n1\n\n\n2\n-0.972880\n3.266332\n1\n\n\n3\n-5.298977\n-9.920072\n0\n\n\n\n\n\n\n\n\nones_df = df[df.y == 1]\nzeros_df = df[df.y == 0]\nplt.scatter(ones_df.x1, ones_df.x2, marker=\"o\", label=\"$y=1$\", c=\"tab:blue\")\nplt.scatter(zeros_df.x1, zeros_df.x2, marker=\"x\", label=\"$y=0$\", c=\"tab:red\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$x_2$\")\nplt.legend(bbox_to_anchor=(1.3, 1))\nplt_show(\"data\")\n\n\n\n\n\nif mode == \"render\":\n    print(df.head(4).style.format(\"{:.2f}\").to_latex())\n\n\\begin{tabular}{lrrr}\n & x1 & x2 & y \\\\\n0 & -5.97 & -10.68 & 0.00 \\\\\n1 & -0.44 & 5.90 & 1.00 \\\\\n2 & -0.97 & 3.27 & 1.00 \\\\\n3 & -5.30 & -9.92 & 0.00 \\\\\n\\end{tabular}\n\n\n\n\nX, y = map(lambda x: torch.tensor(x, dtype=torch.float32), (X_np, y_np))\ny = y.reshape(-1, 1)\nX.shape, y.shape\n\n(torch.Size([20, 2]), torch.Size([20, 1]))"
  },
  {
    "objectID": "notebooks/bayesian-logistic-regression.html#mle",
    "href": "notebooks/bayesian-logistic-regression.html#mle",
    "title": "Bayesian Logistic Regression",
    "section": "MLE",
    "text": "MLE\n\ndef negative_log_likelihood(theta, y):\n    probs = torch.sigmoid(X @ theta)\n    # print(probs.shape, y.shape)\n    return -torch.sum(y * torch.log(probs) + (1 - y) * torch.log(1 - probs))\n\n\ntorch.manual_seed(42)\ntheta = torch.randn(2, 1, requires_grad=True)\nepochs = 1000\n\noptimizer = torch.optim.Adam([theta], lr=0.01)\n\nlosses = []\npbar = tqdm.trange(epochs)\nfor epoch in pbar:\n    optimizer.zero_grad()\n    loss = negative_log_likelihood(theta, y)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n    pbar.set_description(f\"loss: {loss.item():.2f}\")\n\nloss: 0.02: 100%|██████████| 1000/1000 [00:01&lt;00:00, 886.72it/s]\n\n\n\nplt.plot(losses)\n\n\n\n\n\nPlot decision boundary\n\ndef plot_decision_boundary(name):\n    x0_grid = torch.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 20).reshape(-1, 1)\n    x1_grid = torch.linspace(X[:, 1].min() - 1, X[:, 1].max() + 1, 20).reshape(-1, 1)\n    X0, X1 = torch.meshgrid(x0_grid.ravel(), x1_grid.ravel())\n\n    f = lambda x1, x2: torch.sigmoid(theta[0] * x1 + theta[1] * x2).squeeze()\n    f = torch.vmap(torch.vmap(f))\n\n    with torch.no_grad():\n        probs = f(X0, X1)\n\n    fig, ax = plt.subplots(1, 2, gridspec_kw=dict(width_ratios=[1, 0.05]))\n    ax[0].scatter(ones_df.x1, ones_df.x2, marker=\"o\", label=\"$y=1$\", c=\"tab:blue\")\n    ax[0].scatter(zeros_df.x1, zeros_df.x2, marker=\"x\", label=\"$y=0$\", c=\"tab:red\")\n    mappable = ax[0].contourf(\n        X0, X1, probs, levels=20, alpha=0.5, cmap=\"RdBu\", vmin=0, vmax=1\n    )\n    ax[0].set_xlabel(\"$x_1$\")\n    ax[0].set_ylabel(\"$x_2$\")\n    sigma_term = \"$\\sigma^2$\" + f\" = {variance:.3f}, \" if name.startswith(\"map\") else \"\"\n    ax[0].set_title(\n        sigma_term\n        + \"$\\\\theta_1 = $\"\n        + f\"{theta[0].item():.3f}, $\\\\theta_2 = $\"\n        + f\"{theta[1].item():.3f}\"\n    )\n    cbar = fig.colorbar(mappable, ticks=np.linspace(0, 1, 5), cax=ax[1])\n    fig.legend(bbox_to_anchor=(1.2, 1))\n    plt_show(name)\n\n\nplot_decision_boundary(\"mle\")"
  },
  {
    "objectID": "notebooks/bayesian-logistic-regression.html#map",
    "href": "notebooks/bayesian-logistic-regression.html#map",
    "title": "Bayesian Logistic Regression",
    "section": "MAP",
    "text": "MAP\n\ndef neg_log_prior(theta, variance):\n    return -dist.Normal(0, variance**0.5).log_prob(theta).sum()\n\n\ndef negative_log_joint(theta, variance):\n    return (\n        negative_log_likelihood(theta, y).sum() + neg_log_prior(theta, variance)\n    ).squeeze()\n\n\ntorch.manual_seed(42)\nvariance = 0.01\ntheta = torch.randn(2, 1, requires_grad=True)\nepochs = 500\n\noptimizer = torch.optim.Adam([theta], lr=0.01)\n\nlosses = []\npbar = tqdm.trange(epochs)\nfor epoch in pbar:\n    optimizer.zero_grad()\n    loss = negative_log_joint(theta, variance).mean()\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n    pbar.set_description(f\"loss: {loss.item():.2f}\")\n\nloss: 3.78: 100%|██████████| 500/500 [00:00&lt;00:00, 658.82it/s]\n\n\n\nplt.plot(losses)\n\n\n\n\n\nPlot decision boundary\n\nplot_decision_boundary(f\"map_{variance}\")"
  },
  {
    "objectID": "notebooks/bayesian-logistic-regression.html#laplace-approximation",
    "href": "notebooks/bayesian-logistic-regression.html#laplace-approximation",
    "title": "Bayesian Logistic Regression",
    "section": "Laplace approximation",
    "text": "Laplace approximation\n\nwith torch.no_grad():\n    probs = torch.sigmoid(X @ theta)\nhess = hessian(negative_log_joint)(theta.ravel(), torch.tensor(variance))\n\ncov = torch.inverse(hess)\nprint(cov)\nposterior = dist.MultivariateNormal(theta.ravel(), cov)\n\ntensor([[ 0.0020, -0.0007],\n        [-0.0007,  0.0006]], grad_fn=&lt;LinalgInvExBackward0&gt;)\n\n\n\nname = f\"map_laplace-{variance}\"\nx0_grid = torch.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 20).reshape(-1, 1)\nx1_grid = torch.linspace(X[:, 1].min() - 1, X[:, 1].max() + 1, 20).reshape(-1, 1)\nX0, X1 = torch.meshgrid(x0_grid.ravel(), x1_grid.ravel())\n\n\ndef get_probs(theta):\n    f = lambda x1, x2: torch.sigmoid(theta[0] * x1 + theta[1] * x2).squeeze()\n    f = torch.vmap(torch.vmap(f))\n\n    with torch.no_grad():\n        probs = f(X0, X1)\n    return probs\n\n\ntheta_samples = posterior.sample((1000,))\nprobs = torch.stack([get_probs(theta) for theta in theta_samples])\nprint(probs.shape)\nprobs = probs.mean(0)\n\nfig, ax = plt.subplots(1, 2, gridspec_kw=dict(width_ratios=[1, 0.05]))\nax[0].scatter(ones_df.x1, ones_df.x2, marker=\"o\", label=\"$y=1$\", c=\"tab:blue\")\nax[0].scatter(zeros_df.x1, zeros_df.x2, marker=\"x\", label=\"$y=0$\", c=\"tab:red\")\nmappable = ax[0].contourf(\n    X0, X1, probs, levels=20, alpha=0.5, cmap=\"RdBu\", vmin=0, vmax=1\n)\nax[0].set_xlabel(\"$x_1$\")\nax[0].set_ylabel(\"$x_2$\")\nsigma_term = \"$\\sigma^2$\" + f\" = {variance:.3f}, \" if name.startswith(\"map\") else \"\"\nax[0].set_title(\n    sigma_term\n    + \"$\\\\theta_1 = $\"\n    + f\"{theta[0].item():.3f}, $\\\\theta_2 = $\"\n    + f\"{theta[1].item():.3f}\"\n)\ncbar = fig.colorbar(mappable, ticks=np.linspace(0, 1, 5), cax=ax[1])\nfig.legend(bbox_to_anchor=(1.2, 1))\nplt_show(name)\n\ntorch.Size([1000, 20, 20])"
  },
  {
    "objectID": "notebooks/bayesian-logistic-regression.html#appendix",
    "href": "notebooks/bayesian-logistic-regression.html#appendix",
    "title": "Bayesian Logistic Regression",
    "section": "Appendix",
    "text": "Appendix\nCan we find closed form MLE solution for Bayesian Logistic Regression? It seems, yes. Stay tuned!"
  },
  {
    "objectID": "notebooks/mc_sampling_intro.html",
    "href": "notebooks/mc_sampling_intro.html",
    "title": "Probabilistic Machine Learning",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n# Retina display\n%config InlineBackend.figure_format = 'retina'\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\n\ntrue_pi = torch.pi\ntrue_pi\n\n3.141592653589793\n\n\n\nimport torch\nimport torch.distributions as dist\n\ndef estimate_pi(N, seed):\n    torch.manual_seed(seed)\n    xy = torch.rand(N, 2)  # Generate N random points in [0, 1] × [0, 1]\n    distance = torch.sqrt(xy[:, 0]**2 + xy[:, 1]**2)  # Calculate distance from the origin\n    inside_circle = distance &lt;= 1.0  # Check if point falls inside the quarter circle\n    points_inside = torch.sum(inside_circle).item()  # Count points inside the quarter circle\n    pi_estimate = (points_inside / N) * 4.0  # Calculate the π estimate\n    return pi_estimate\n\n\nN = 10000\nseed = 42\ntorch.manual_seed(seed)\nxy = torch.rand(N, 2) \nx1 = xy[:, 0]\nx2 = xy[:, 1]\n\ndistances = torch.sqrt(x1**2 + x2**2)\n\nc = distances &lt;= 1.0\n\nplt.scatter(x1, x2, c = c.float())\nplt.gca().set_aspect('equal')\n\nplt.title((c.sum().item()/N)*4)\n\nText(0.5, 1.0, '3.1464')\n\n\n\n\n\n\n# Different random seeds and sample sizes\nrandom_seeds = [0, 1, 2, 3, 4, 5]\nlog_sample_sizes = [1, 2, 3, 4, 5, 6, 7, 8]\nsample_sizes = [10**i for i in log_sample_sizes]\n\npi_estimates = []\nfor seed in random_seeds:\n    for N in sample_sizes:\n        pi_estimate = estimate_pi(N, seed)\n        pi_estimates.append((seed, N, pi_estimate))\n\n\n\n\n\n\n\n\n\nseed\nN\npi_estimate\n\n\n\n\n0\n0\n10\n3.200000\n\n\n1\n0\n100\n3.200000\n\n\n2\n0\n1000\n3.140000\n\n\n3\n0\n10000\n3.139200\n\n\n4\n0\n100000\n3.137200\n\n\n5\n0\n1000000\n3.140544\n\n\n6\n0\n10000000\n3.141136\n\n\n7\n0\n100000000\n3.141476\n\n\n8\n1\n10\n3.600000\n\n\n9\n1\n100\n2.880000\n\n\n\n\n\n\n\n\n# Create a Pandas DataFrame from the list of tuples\ndf = pd.DataFrame(pi_estimates, columns=[\"seed\", \"N\", \"pi_estimate\"])\ndf.head(20)\n\n\n\n\n\n\n\n\nseed\nN\npi_estimate\n\n\n\n\n0\n0\n10\n3.200000\n\n\n1\n0\n100\n3.200000\n\n\n2\n0\n1000\n3.140000\n\n\n3\n0\n10000\n3.139200\n\n\n4\n0\n100000\n3.137200\n\n\n5\n0\n1000000\n3.140544\n\n\n6\n0\n10000000\n3.141136\n\n\n7\n0\n100000000\n3.141476\n\n\n8\n1\n10\n3.600000\n\n\n9\n1\n100\n2.880000\n\n\n10\n1\n1000\n3.096000\n\n\n11\n1\n10000\n3.144000\n\n\n12\n1\n100000\n3.139640\n\n\n13\n1\n1000000\n3.143724\n\n\n14\n1\n10000000\n3.142105\n\n\n15\n1\n100000000\n3.141681\n\n\n16\n2\n10\n3.600000\n\n\n17\n2\n100\n3.120000\n\n\n18\n2\n1000\n3.140000\n\n\n19\n2\n10000\n3.144000\n\n\n\n\n\n\n\n\ndf_grouped = df.groupby(\"N\").agg([\"mean\", \"std\"])[\"pi_estimate\"]\ndf_grouped\n\n\n\n\n\n\n\n\nmean\nstd\n\n\nN\n\n\n\n\n\n\n10\n3.200000\n0.357771\n\n\n100\n3.120000\n0.153883\n\n\n1000\n3.147333\n0.090099\n\n\n10000\n3.147200\n0.007065\n\n\n100000\n3.140580\n0.007081\n\n\n1000000\n3.142121\n0.001377\n\n\n10000000\n3.141548\n0.000712\n\n\n100000000\n3.141659\n0.000162\n\n\n\n\n\n\n\n\n# Plot the estimates using mean and standard deviation\nplt.figure(figsize=(8, 6))\ndf_grouped[\"mean\"].plot(yerr=df_grouped[\"std\"], capsize=5, marker=\"o\")\nplt.axhline(true_pi, color=\"black\", linestyle=\"--\")\nplt.xlabel(\"Sample Size\")\nplt.ylabel(\"Estimated π\")\nplt.title(\"Estimating π using Monte Carlo Sampling\")\n# log scale on x-axis\nplt.xscale(\"log\")\n\nfindfont: Font family ['cursive'] not found. Falling back to DejaVu Sans.\nfindfont: Generic family 'cursive' not found because none of the following families were found: Apple Chancery, Textile, Zapf Chancery, Sand, Script MT, Felipa, Comic Neue, Comic Sans MS, cursive\n\n\n\n\n\n\n# Plot only 10^4 and higher\nplt.figure(figsize=(8, 6))\nsubset = df_grouped.query(\"N &gt;= 10**4\")\nsubset[\"mean\"].plot(yerr=subset[\"std\"], capsize=5, marker=\"o\")\nplt.xlabel(\"Sample Size\")\nplt.ylabel(\"Estimated π\")\nplt.title(\"Estimating π using Monte Carlo Sampling\")\n# log scale on x-axis\nplt.xscale(\"log\")\nplt.yscale(\"log\")\n\nplt.axhline(true_pi, color=\"black\", linestyle=\"--\")\n\n# plot true_pi +- 0.0001\nplt.axhspan(true_pi - 0.0001, true_pi + 0.0001, color=\"black\", alpha=0.2, label=\"True π ± 0.0001\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f6b93cb3d90&gt;"
  },
  {
    "objectID": "notebooks/mcmc-hamiltorch.html",
    "href": "notebooks/mcmc-hamiltorch.html",
    "title": "Sampling from an unnormalized distribution",
    "section": "",
    "text": "import torch\nimport torch.autograd.functional as F\nimport torch.distributions as dist\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\n\nimport ipywidgets as widgets\n\n\nimport seaborn as sns\n\nimport pandas as pd\n%matplotlib inline\n\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n#plt.rcParams.update(bundles.icml2022())\n\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\n\nimport hamiltorch\n\n\nhamiltorch.set_random_seed(123)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\ndevice\n\ndevice(type='cuda')\n\n\n\ngt_distribution = torch.distributions.Normal(0, 1)\n\n# Samples from the ground truth distribution\ndef sample_gt(n):\n    return gt_distribution.sample((n,))\n\nsamples = sample_gt(1000)\n\n\nx_lin = torch.linspace(-3, 3, 1000)\ny_lin = torch.exp(gt_distribution.log_prob(x_lin))\n\nplt.plot(x_lin, y_lin, label='Ground truth')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7efdfc51dd30&gt;\n\n\n\n\n\n\n# Logprob function to be passed to Hamiltorch sampler\ndef logprob(x):\n    return gt_distribution.log_prob(x)\n\n\nlogprob(torch.tensor([0.0]))\n\ntensor([-0.9189])\n\n\n\n# Markov chain\nx_start = torch.tensor([0.0])\nsamples = []\nfor i in range(100):\n    prop = torch.distributions.Normal(x_start, 10).sample()\n    samples.append(prop)\n    x_start = prop\n\n\n\nplt.plot(torch.stack(samples).ravel())\n\n\n\n\n\n# Initial state\nx0 = torch.tensor([0.0])\nnum_samples = 5000\nstep_size = 0.3\nnum_steps_per_sample = 5\nhamiltorch.set_random_seed(123)\n\n\nparams_hmc = hamiltorch.sample(log_prob_func=logprob, params_init=x0,  \n                               num_samples=num_samples, step_size=step_size, \n                               num_steps_per_sample=num_steps_per_sample)\n\nSampling (Sampler.HMC; Integrator.IMPLICIT)\nTime spent  | Time remain.| Progress             | Samples   | Samples/sec\n0d:00:00:07 | 0d:00:00:00 | #################### | 5000/5000 | 686.98       \nAcceptance Rate 0.99\n\n\n\nparams_hmc = torch.tensor(params_hmc)\n\n\ndef run_hmc(logprob, x0, num_samples, step_size, num_steps_per_sample):\n    torch.manual_seed(123)\n    params_hmc = hamiltorch.sample(log_prob_func=logprob, params_init=x0,\n                                    num_samples=num_samples, step_size=step_size,\n                                    num_steps_per_sample=num_steps_per_sample)\n    return torch.stack(params_hmc)\n\n\nparams_hmc = run_hmc(logprob, x0, num_samples, step_size, num_steps_per_sample)\n\nSampling (Sampler.HMC; Integrator.IMPLICIT)\nTime spent  | Time remain.| Progress             | Samples   | Samples/sec\n0d:00:00:07 | 0d:00:00:00 | #################### | 5000/5000 | 676.50       \nAcceptance Rate 0.99\n\n\n\nparams_hmc.shape\n\ntorch.Size([5000, 1])\n\n\n\n# Trace plot\nplt.plot(params_hmc, label='Trace')\nplt.xlabel('Iteration')\nplt.ylabel('Parameter value')\n\nText(0, 0.5, 'Parameter value')\n\n\n\n\n\n\n# view first 500 samples\nplt.plot(params_hmc[:50], label='Trace')\nplt.xlabel('Iteration')\nplt.ylabel('Parameter value')\n\nText(0, 0.5, 'Parameter value')\n\n\n\n\n\n\n# KDE plot\nimport seaborn as sns\nplt.figure()\nsns.kdeplot(params_hmc.ravel().detach().numpy(), label='Samples', shade=True, color='C1')\nplt.plot(x_lin, y_lin, label='Ground truth')\nplt.xlabel('Parameter value')\nplt.ylabel('Density')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f2dbc637d30&gt;\n\n\n\n\n\n\n# Create MP4 HTML5 video showing sampling process\n\ndef create_mp4_samples(samples, x_lin, y_lin, filename='samples.mp4', dpi=600):\n    fig, ax = plt.subplots(figsize=(4,2), dpi=dpi)\n    ax.set_xlim(-3, 3)\n    ax.set_ylim(0, 1)\n    ax.set_xlabel('Parameter value')\n    ax.set_ylabel('Density')\n    ax.plot(x_lin, y_lin, label='Ground truth')\n    ax.legend()\n\n    # Add a \"x\" marker to the plot for each sample at y=0\n    x_marker, = ax.plot([], [], 'x', color='C1', label='Samples')\n    ax.legend()\n\n    def init():\n        x_marker.set_data([], [])\n        return x_marker,\n\n    def animate(i):\n        x_marker.set_data(samples[:i], torch.zeros(i))\n        return x_marker,\n\n    anim = FuncAnimation(fig, animate, init_func=init,\n                                     frames=len(samples), interval=20, blit=True)\n    anim.save(filename, dpi=dpi, writer='ffmpeg')\n\ncreate_mp4_samples(params_hmc[:100], x_lin, y_lin, filename='../figures/sampling/mcmc/normal.mp4', dpi=600)\n\n\n\n\n\nfrom IPython.display import Video\nVideo('../figures/sampling/mcmc/normal.mp4', width=400)\n\n\n      Your browser does not support the video element.\n    \n\n\n\n# sample from Mixture of Gaussians\n\nmog = dist.MixtureSameFamily(\n    mixture_distribution=dist.Categorical(torch.tensor([0.3, 0.7])),\n    component_distribution=dist.Normal(torch.tensor([-2.0, 2.0]), torch.tensor([1.0, 0.5]))\n)\n\nsamples = mog.sample((1000,))\nsns.kdeplot(samples.numpy(), label='Samples', shade=True, color='C1')\n\n&lt;AxesSubplot:ylabel='Density'&gt;\n\n\n\n\n\n\n# Logprob function to be passed to Hamiltorch sampler\ndef logprob(x):\n    return mog.log_prob(x)\n\nlogprob(torch.tensor([0.0]))\n\ntensor([-4.1114])\n\n\n\nparams_hmc = run_hmc(logprob, x0, num_samples, step_size, num_steps_per_sample)\n\nSampling (Sampler.HMC; Integrator.IMPLICIT)\nTime spent  | Time remain.| Progress             | Samples   | Samples/sec\n0d:00:00:10 | 0d:00:00:00 | #################### | 5000/5000 | 459.19       \nAcceptance Rate 0.99\n\n\n\n# Trace plot\nplt.plot(params_hmc, label='Trace')\n\n\n\n\n\ny_lin = torch.exp(mog.log_prob(x_lin))\n\n\n# KDE plot\nplt.figure()\nsns.kdeplot(params_hmc.ravel().detach().numpy(), label='Samples', shade=True, color='C1')\n# Limit KDE plot to range of ground truth\nplt.xlim(-3, 3)\nplt.plot(x_lin, y_lin, label='Ground truth')\nplt.xlabel('Parameter value')\nplt.ylabel('Density')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f2dbc3a65b0&gt;\n\n\n\n\n\n\n# Create MP4 HTML5 video showing sampling process\ncreate_mp4_samples(params_hmc[:500], x_lin, y_lin, filename='../figures/sampling/mcmc/mog.mp4', dpi=600)\n\n\n\n\n\nVideo('../figures/sampling/mcmc/mog.mp4', width=400)\n\n\n      Your browser does not support the video element.\n    \n\n\n\ndef p_tilde(x):\n    # normalising constant for standard normal distribution\n    Z = torch.sqrt(torch.tensor(2*np.pi))\n    return dist.Normal(0, 1).log_prob(x).exp()*Z\n\ndef p_tilde_log_prob(x):\n    # normalising constant for standard normal distribution\n    Z = torch.sqrt(torch.tensor(2*np.pi))\n    return dist.Normal(0, 1).log_prob(x) + torch.log(Z)\n\n\n# Plot unnormalized distribution\nx_lin = torch.linspace(-3, 3, 1000)\ny_lin = p_tilde(x_lin)\nplt.plot(x_lin, y_lin, label='Unnormalized distribution')\n# Plot normalized distribution\nplt.plot(x_lin, dist.Normal(0, 1).log_prob(x_lin).exp(), label='Normalized distribution')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f2dbc385c70&gt;\n\n\n\n\n\n\n# HMC over unnormalized distribution\n\n# Logprob function to be passed to Hamiltorch sampler\ndef logprob(x):\n    return p_tilde_log_prob(x)\n\n\n# HMC\nparams_hmc = run_hmc(logprob, x0, num_samples, step_size, num_steps_per_sample)\n\nSampling (Sampler.HMC; Integrator.IMPLICIT)\nTime spent  | Time remain.| Progress             | Samples   | Samples/sec\n0d:00:00:10 | 0d:00:00:00 | #################### | 5000/5000 | 479.28       \nAcceptance Rate 0.99\n\n\n\n# Trace plot\nplt.plot(params_hmc[:500], label='Trace')\n\n\n\n\n\n# KDE plot\nsns.kdeplot(params_hmc.ravel().detach().numpy(), label='Samples', shade=True, color='C1')\nplt.plot(x_lin, y_lin, label='Unnormalized distribution', lw=2)\nplt.plot(x_lin, dist.Normal(0, 1).log_prob(x_lin).exp(), label='Normalized distribution', lw=2)\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f2dbc2657f0&gt;\n\n\n\n\n\n\nCoin Toss\n\nWorking with probabilities\n\nprior = dist.Beta(1, 1)\ndata = torch.tensor([1.0, 1.0, 1.0, 0.0, 0.0])\nn = len(data)\n\ndef log_prior(theta):\n    return prior.log_prob(theta)\n\ndef log_likelihood(theta):\n    return dist.Bernoulli(theta).log_prob(data).sum()\n\ndef log_joint(theta):\n    return log_prior(theta) + log_likelihood(theta)\n\n\ntry:\n    params_hmc_theta = run_hmc(log_joint, torch.tensor([0.5]), 5000, 0.3, 5)\nexcept Exception as e:\n    print(e)\n\nSampling (Sampler.HMC; Integrator.IMPLICIT)\nTime spent  | Time remain.| Progress             | Samples   | Samples/sec\nExpected value argument (Tensor of shape (1,)) to be within the support (Interval(lower_bound=0.0, upper_bound=1.0)) of the distribution Beta(), but found invalid values:\ntensor([-0.1017], requires_grad=True)\n\n\n\n\nWorking with logits\n\n# Let us work instead with logits\ndef log_prior(logits):\n    return prior.log_prob(torch.sigmoid(logits)).sum()\n\ndef log_likelihood(logits):\n    return dist.Bernoulli(logits=logits).log_prob(data).sum()\n\ndef log_joint(logits):\n    return (log_prior(logits) + log_likelihood(logits))\n\n\nparams_hmc_logits = run_hmc(log_joint, torch.tensor([0.0]), 1000, 0.3, 5)\n\nSampling (Sampler.HMC; Integrator.IMPLICIT)\nTime spent  | Time remain.| Progress             | Samples   | Samples/sec\n0d:00:00:03 | 0d:00:00:00 | #################### | 1000/1000 | 275.76       \nAcceptance Rate 0.99\n\n\n\nfig, ax = plt.subplots(nrows=2, sharex=True)\nax[0].plot(params_hmc_logits[:500], label='Trace for logits')\nax[1].plot(torch.sigmoid(params_hmc_logits[:500]), label='Trace for probabilities')\nax[0].set_ylabel('Logits')\nax[1].set_ylabel('Computed Probabilities\\n (From Logits)')\nax[1].set_xlabel('Iteration')\n\nText(0.5, 0, 'Iteration')\n\n\n\n\n\n\n# Create a function to update the KDE plot with the specified bw_adjust value\ndef update_kde_plot(bw_adjust):\n    plt.clf()  # Clear the previous plot\n    plt.hist(torch.sigmoid(params_hmc_logits[:, 0]).detach().numpy(), bins=100, density=True, label='Samples (Histogram)', color='C2', alpha=0.5, lw=1)\n    sns.kdeplot(torch.sigmoid(params_hmc_logits[:, 0]).detach().numpy(), label='Samples (KDE)', shade=False, color='C1', clip=(0, 1), bw_adjust=bw_adjust, lw=2)\n    x_lin = torch.linspace(0, 1, 1000)\n    y_lin = dist.Beta(1+3, 1+2).log_prob(x_lin).exp()\n    plt.plot(x_lin, y_lin, label='True posterior')\n    plt.legend()\n\n# Create the slider widget for bw_adjust\nbw_adjust_slider = widgets.FloatSlider(value=0.1, min=0.01, max=4.0, step=0.01, description='bw_adjust:')\n\n# Create the interactive plot\ninteractive_plot = widgets.interactive(update_kde_plot, bw_adjust=bw_adjust_slider)\n\n# Display the interactive plot\ndisplay(interactive_plot)\n\n\n\n\n\n# Plot histogram of samples\nplt.hist(torch.sigmoid(params_hmc_logits[:, 0]).detach().numpy(), bins=100, density=True, label='Samples (Histogram)', color='C2', alpha=0.5, lw=1 )\n\n# Plot posterior KDE using seaborn but clip to [0, 1]\nsns.kdeplot(torch.sigmoid(params_hmc_logits[:, 0]).detach().numpy(), label='Samples (KDE)', shade=False, color='C1', clip=(0, 1), bw_adjust=0.1, lw=2)\n# True posterior\nx_lin = torch.linspace(0, 1, 1000)\ny_lin = dist.Beta(1+3, 1+2).log_prob(x_lin).exp()\nplt.plot(x_lin, y_lin, label='True posterior')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f1850e258b0&gt;\n\n\n\n\n\n\n# Linear regression for 1 dimensional input using HMC\n\nx_lin = torch.linspace(-3, 3, 90)\ntheta_0_true = torch.tensor([2.0])\ntheta_1_true = torch.tensor([3.0])\nf = lambda x: theta_0_true + theta_1_true * x\neps = torch.randn_like(x_lin) *1.0\ny_lin = f(x_lin) + eps\n\nplt.scatter(x_lin, y_lin, label='Data', color='C0')\nplt.plot(x_lin, f(x_lin), label='Ground truth')\nplt.xlabel('x')\nplt.ylabel('y')\n\nText(0, 0.5, 'y')\n\n\n\n\n\n\n# Esimate theta_0, theta_1 using HMC assuming noise variance is known to be 1\ndef logprob(theta):\n    y_pred = theta[0] + x_lin * theta[1]\n    return dist.Normal(y_pred, 1).log_prob(y_lin).sum()\n\ndef log_prior(theta):\n    return dist.Normal(0, 1).log_prob(theta).sum()\n\ndef log_posterior(theta):\n    return logprob(theta) + log_prior(theta)\n\n\nparams_hmc_lin_reg = run_hmc(log_posterior, torch.tensor([0.0, 0.0]), 1000, 0.05, 10)\n\nSampling (Sampler.HMC; Integrator.IMPLICIT)\nTime spent  | Time remain.| Progress             | Samples   | Samples/sec\n0d:00:00:06 | 0d:00:00:00 | #################### | 1000/1000 | 157.83       \nAcceptance Rate 0.95\n\n\n\nparams_hmc_lin_reg\n\ntensor([[0.0000, 0.0000],\n        [1.8463, 1.5993],\n        [2.1721, 3.8480],\n        ...,\n        [1.9128, 2.9478],\n        [2.1689, 2.9928],\n        [1.8748, 3.0217]])\n\n\n\nlps = []\nfor p in params_hmc_lin_reg:\n    lps.append(log_posterior(p))\n\n\nplt.plot(torch.stack(lps).ravel()[100:])\n\n\n\n\n\nlog_posterior(params_hmc_lin_reg[0]), log_posterior(params_hmc_lin_reg[1]), log_posterior(params_hmc_lin_reg[2])\n\n(tensor(-1554.2708), tensor(-392.7923), tensor(-242.6276))\n\n\n\n# Plot the traces corresponding to the two parameters\nfig, axes = plt.subplots(2, 1, sharex=True)\n\nfor i, param_vals in enumerate(params_hmc_lin_reg.T):\n    axes[i].plot(param_vals, label='Trace')\n    axes[i].set_xlabel('Iteration')\n    axes[i].set_ylabel(fr'$\\theta_{i}$')\n\n# Plot the true values as well\nfor i, param_vals in enumerate([theta_0_true, theta_1_true]):\n    axes[i].axhline(param_vals.numpy(), color='C1', label='Ground truth')\n    axes[i].legend()\n\nfindfont: Font family ['cursive'] not found. Falling back to DejaVu Sans.\nfindfont: Generic family 'cursive' not found because none of the following families were found: Apple Chancery, Textile, Zapf Chancery, Sand, Script MT, Felipa, Comic Neue, Comic Sans MS, cursive\n\n\n\n\n\n\n# Plot KDE of the samples for the two parameters\nfig, axes = plt.subplots(2, 1, sharex=True)\n\nfor i, param_vals in enumerate(params_hmc_lin_reg.T):\n    sns.kdeplot(param_vals.detach().numpy(), label='Samples', shade=True, color='C1', ax=axes[i])\n    axes[i].set_ylabel(fr'$\\theta_{i}$')\n\n# Plot the true values as well\nfor i, param_vals in enumerate([theta_0_true, theta_1_true]):\n    axes[i].axvline(param_vals.numpy(), color='C0', label='Ground truth')\n    axes[i].legend()\n\n\n\n\n\n# Plot the posterior predictive distribution\nplt.figure()\nplt.scatter(x_lin, y_lin, label='Data', color='C0')\nplt.plot(x_lin, f(x_lin), label='Ground truth', color='C1', linestyle='--')\nplt.xlabel('x')\nplt.ylabel('y')\n\n# Get posterior samples. Thin first 100 samples to remove burn-in\nposterior_samples = params_hmc_lin_reg[100:].detach()\ny_hat = posterior_samples[:, 0].unsqueeze(1) + x_lin * posterior_samples[:, 1].unsqueeze(1)\n\n# Plot mean and 95% confidence interval\n\nplt.plot(x_lin, y_hat.mean(axis=0), label='Mean', color='C2')\nplt.fill_between(x_lin, y_hat.mean(axis=0) - 2 * y_hat.std(axis=0), y_hat.mean(axis=0) + 2 * y_hat.std(axis=0), alpha=0.5, label='95% CI', color='C2')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f2db1983640&gt;\n\n\n\n\n\n\n# Using a neural network with HMC\n\nclass Net(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(1, 1)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        return x\n\n\nnet = Net()\nnet\n\nNet(\n  (fc1): Linear(in_features=1, out_features=1, bias=True)\n)\n\n\n\nnet.state_dict()\n\nOrderedDict([('fc1.weight', tensor([[0.7689]])),\n             ('fc1.bias', tensor([0.2034]))])\n\n\n\nhamiltorch.util.flatten(net)\n\ntensor([0.7689, 0.2034], grad_fn=&lt;CatBackward0&gt;)\n\n\n\ntheta_params = hamiltorch.util.flatten(net) + 1.0\ntheta_params\n\ntensor([1.7689, 1.2034], grad_fn=&lt;AddBackward0&gt;)\n\n\n\nfrom nn_manual_hmc import log_joint as log_joint_nn\n\n\nparams_hmc = run_hmc(log_joint_nn, torch.tensor([0.2, 0.5]), 1000, 0.05, 5)\n\nSampling (Sampler.HMC; Integrator.IMPLICIT)\nTime spent  | Time remain.| Progress             | Samples   | Samples/sec\n0d:00:00:04 | 0d:00:00:00 | #################### | 1000/1000 | 202.36       \nAcceptance Rate 0.94\n\n\n\n# Plot the traces corresponding to the two parameters\nfig, axes = plt.subplots(2, 1, sharex=True)\n\nfor i, param_vals in enumerate(params_hmc.T):\n    axes[i].plot(param_vals, label='Trace')\n    axes[i].set_xlabel('Iteration')\n    axes[i].set_ylabel(fr'$\\theta_{i}$')\n\n\n\n\n\n# Plot KDE of the samples for the two parameters\nfig, axes = plt.subplots(2, 1, sharex=True)\n\nfor i, param_vals in enumerate(params_hmc.T):\n    sns.kdeplot(param_vals.detach().numpy(), label='Samples', shade=True, color='C1', ax=axes[i])\n    axes[i].set_ylabel(fr'$\\theta_{i}$')\n\n# Mark the true values\naxes[0].axvline(theta_0_true.numpy(), color='C0', label='Ground truth')\naxes[1].axvline(theta_1_true.numpy(), color='C0', label='Ground truth')\n\n&lt;matplotlib.lines.Line2D at 0x7f2db24ad8e0&gt;\n\n\n\n\n\n\n# Get posterior samples. Thin first 100 samples to remove burn-in\nposterior_samples = params_hmc.detach()\ny_preds = []\nwith torch.no_grad():\n    for theta in posterior_samples:\n        params_list = hamiltorch.util.unflatten(net, theta)\n        params = net.state_dict()\n        for i, (name, _) in enumerate(params.items()):\n            params[name] = params_list[i]\n        y_pred = torch.func.functional_call(net, params, x_lin.unsqueeze(1)).squeeze()\n\n        y_preds.append(y_pred)\n\n\ntorch.stack(y_preds).shape\n\ntorch.Size([1000, 90])\n\n\n\ny_mean = torch.stack(y_preds).mean(axis=0)\ny_std = torch.stack(y_preds).std(axis=0)\n\nplt.plot(x_lin, y_mean, label='Mean', color='C2')\nplt.fill_between(x_lin, y_mean - 2 * y_std, y_mean + 2 * y_std, alpha=0.5, label='95% CI', color='C2')\n\nplt.scatter(x_lin, y_lin, label='Data', color='C0')\nplt.plot(x_lin, f(x_lin), label='Ground truth', color='C1', linestyle='--')\nplt.xlabel('x')\nplt.ylabel('y')\n\n\nText(0, 0.5, 'y')\n\n\n\n\n\n\nstep_size = 0.0005\nnum_samples = 1000\nL = 30\nburn = -1\nstore_on_GPU = False\ndebug = False\nmodel_loss = 'regression'\nmass = 1.0\n\n# Effect of tau\n# Set to tau = 1000. to see a function that is less bendy (weights restricted to small bends)\n# Set to tau = 1. for more flexible\n\ntau = 1.0 # Prior Precision\ntau_out = 110.4439498986428 # Output Precision\nr = 0 # Random seed\n\n\ntau_list = []\nfor w in net.parameters():\n    tau_list.append(tau) # set the prior precision to be the same for each set of weights\ntau_list = torch.tensor(tau_list)\n\n# Set initial weights\nparams_init = hamiltorch.util.flatten(net).clone()\n# Set the Inverse of the Mass matrix\ninv_mass = torch.ones(params_init.shape) / mass\n\n\n\nintegrator = hamiltorch.Integrator.EXPLICIT\nsampler = hamiltorch.Sampler.HMC\n\nhamiltorch.set_random_seed(r)\nparams_hmc_f = hamiltorch.sample_model(net, x_lin.view(-1, 1), y_lin.view(-1, 1), params_init=params_init,\n                                       model_loss=model_loss, num_samples=100,\n                                       burn = burn, inv_mass=inv_mass,step_size=step_size,\n                                       num_steps_per_sample=L,tau_out=tau_out, tau_list=tau_list,\n                                       debug=debug, store_on_GPU=store_on_GPU,\n                                       sampler = sampler)\n\n# At the moment, params_hmc_f is on the CPU so we move to GPU\n\nparams_hmc_gpu = [ll for ll in params_hmc_f[1:]]\n\n\n\nSampling (Sampler.HMC; Integrator.IMPLICIT)\nTime spent  | Time remain.| Progress             | Samples | Samples/sec\n0d:00:00:02 | 0d:00:00:00 | #################### | 100/100 | 49.73       \nAcceptance Rate 1.00\n\n\n\ntorch.stack(params_hmc_gpu).shape\n\ntorch.Size([100, 2])\n\n\n\n# Let's predict over the entire test range [-2,2]\npred_list, log_probs_f = hamiltorch.predict_model(net, x = x_lin.view(-1, 1), y = y_lin.view(-1, 1), samples=params_hmc_gpu,\n                                                  model_loss=model_loss, tau_out=tau_out,\n                                                  tau_list=tau_list)\n\n\npred_list.shape\n\ntorch.Size([100, 90, 1])\n\n\n\nplt.plot(x_lin, pred_list.mean(axis=0).ravel())\n# Plot the true function\nplt.plot(x_lin, f(x_lin), label='Ground truth', color='C1', linestyle='--')\n\n# Plot standard deviation\nplt.fill_between(x_lin, pred_list.mean(axis=0).ravel() - 2 * pred_list.std(axis=0).ravel(), pred_list.mean(axis=0).ravel() + 2 * pred_list.std(axis=0).ravel(), alpha=0.5, label='95% CI', color='C2')\n\nplt.scatter(x_lin, y_lin, label='Data', color='C0')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f1a94574c70&gt;\n\n\n\n\n\n\nfrom nn_manual_hmc_classification import log_joint, x_moon, y_moon, net_classification\n\n\nplt.scatter(x_moon[:, 0].cpu().numpy(), x_moon[:, 1].cpu().numpy(), c=y_moon.cpu().numpy(), cmap='bwr', alpha=0.5)\n\n&lt;matplotlib.collections.PathCollection at 0x7efdcec96820&gt;\n\n\n\n\n\n\nnet_classification\n\nNet_Classification(\n  (fc1): Linear(in_features=2, out_features=5, bias=True)\n  (fc2): Linear(in_features=5, out_features=5, bias=True)\n  (fc3): Linear(in_features=5, out_features=1, bias=True)\n)\n\n\n\nhamiltorch.util.flatten(net_classification).shape\n\ntorch.Size([51])\n\n\n\n# number of params in the network\nD = hamiltorch.util.flatten(net_classification).shape[0]\n\n\nlog_joint(torch.zeros(D).to(device))\n\ntensor(-740.0131, device='cuda:0')\n\n\n\nparams_hmc = run_hmc(log_joint, torch.tensor(torch.zeros(D).to(device)), 2000, 0.01, 2)\n\nSampling (Sampler.HMC; Integrator.IMPLICIT)\nTime spent  | Time remain.| Progress             | Samples   | Samples/sec\n0d:00:00:21 | 0d:00:00:00 | #################### | 2000/2000 | 93.82       \nAcceptance Rate 0.96\n\n\n/tmp/ipykernel_948360/3753994728.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  params_hmc = run_hmc(log_joint, torch.tensor(torch.zeros(D).to(device)), 2000, 0.01, 2)\n\n\n\nparams_hmc.shape\n\ntorch.Size([2000, 51])\n\n\n\nplt.plot(params_hmc[:, 2].cpu().numpy())\n\n\n\n\n\n# Get posterior predictive over the 2D grid\nposterior_samples = params_hmc.detach()\n# Consider burning the first 100 samples\nposterior_samples = posterior_samples[1000:]\ny_preds = []\nn_grid = 200\nlims = 4\ntwod_grid = torch.tensor(np.meshgrid(np.linspace(-lims, lims, n_grid), np.linspace(-lims, lims, n_grid))).float().to(device)\nwith torch.no_grad():\n    for theta in posterior_samples:\n        params_list = hamiltorch.util.unflatten(net_classification, theta)\n        params = net_classification.state_dict()\n        for i, (name, _) in enumerate(params.items()):\n            params[name] = params_list[i]\n        y_pred = torch.func.functional_call(net_classification, params, twod_grid.view(2, -1).T).squeeze()\n\n        y_preds.append(y_pred)\n\n\nx_moon.shape\n\ntorch.Size([1000, 2])\n\n\n\ny_preds[0].shape\n\ntorch.Size([40000])\n\n\n\nlogits = torch.stack(y_preds).mean(axis=0).reshape(n_grid, n_grid)\nlogits\n\ntensor([[-20.4181, -19.7507, -19.0839,  ...,   4.1155,   4.1154,   4.1154],\n        [-20.7736, -20.1060, -19.4386,  ...,   4.1154,   4.1154,   4.1154],\n        [-21.1293, -20.4614, -19.7938,  ...,   4.1154,   4.1153,   4.1153],\n        ...,\n        [-98.6203, -98.0886, -97.5601,  ...,  -5.8248,  -5.5091,  -5.2027],\n        [-99.1825, -98.6539, -98.1300,  ...,  -6.1079,  -5.7854,  -5.4731],\n        [-99.7477, -99.2237, -98.7036,  ...,  -6.3935,  -6.0654,  -5.7471]],\n       device='cuda:0')\n\n\n\nprobs = torch.sigmoid(logits)\nprobs\n\ntensor([[1.3568e-09, 2.6447e-09, 5.1520e-09,  ..., 9.8394e-01, 9.8394e-01,\n         9.8394e-01],\n        [9.5090e-10, 1.8539e-09, 3.6136e-09,  ..., 9.8394e-01, 9.8394e-01,\n         9.8394e-01],\n        [6.6629e-10, 1.2994e-09, 2.5332e-09,  ..., 9.8394e-01, 9.8394e-01,\n         9.8394e-01],\n        ...,\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.9446e-03, 4.0335e-03,\n         5.4718e-03],\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.2204e-03, 3.0628e-03,\n         4.1806e-03],\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.6697e-03, 2.3164e-03,\n         3.1817e-03]], device='cuda:0')\n\n\n\n# Plot the posterior predictive distribution decision boundary\nplt.figure()\nplt.contourf(twod_grid[0].cpu().numpy(), twod_grid[1].cpu().numpy(), probs.cpu().numpy(), cmap='bwr', alpha=0.5)\nplt.colorbar()\nplt.scatter(x_moon[:, 0].cpu().numpy(), x_moon[:, 1].cpu().numpy(), c=y_moon.cpu().numpy(), cmap='bwr', alpha=0.5)\n\n&lt;matplotlib.collections.PathCollection at 0x7efdcd18fe50&gt;\n\n\n\n\n\n\n# Plot the variance of the posterior predictive distribution\nplt.figure()\nplt.contourf(twod_grid[0].cpu().numpy(), twod_grid[1].cpu().numpy(), torch.stack(y_preds).std(axis=0).reshape(n_grid, n_grid).cpu().numpy(), cmap='bwr', alpha=0.5)\nplt.scatter(x_moon[:, 0].cpu().numpy(), x_moon[:, 1].cpu().numpy(), c=y_moon.cpu().numpy(), cmap='bwr', alpha=0.5)\nplt.colorbar()\n\n&lt;matplotlib.colorbar.Colorbar at 0x7efdcd0f4e80&gt;"
  },
  {
    "objectID": "notebooks/ssl.html",
    "href": "notebooks/ssl.html",
    "title": "Imports",
    "section": "",
    "text": "try:\n    from astra.torch.models import ResNetClassifier\nexcept:\n    %pip install git+https://github.com/sustainability-lab/ASTRA\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport os\nimport numpy as np\nimport pandas as pd\n%matplotlib inline\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n\n# Confusion matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nimport torchsummary\nfrom tqdm import tqdm\n\nimport umap\n\n# ASTRA\nfrom astra.torch.data import load_cifar_10\nfrom astra.torch.utils import train_fn\nfrom astra.torch.models import ResNetClassifier\n\n# Netron, ONNX for model visualization\nimport netron\nimport onnx\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice\n\ndevice(type='cuda')"
  },
  {
    "objectID": "notebooks/ssl.html#dataset",
    "href": "notebooks/ssl.html#dataset",
    "title": "Imports",
    "section": "Dataset",
    "text": "Dataset\n\ndataset = load_cifar_10()\ndataset\n\nFiles already downloaded and verified\nFiles already downloaded and verified\n\n\n\nCIFAR-10 Dataset\nlength of dataset: 60000\nshape of images: torch.Size([3, 32, 32])\nlen of classes: 10\nclasses: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\ndtype of images: torch.float32\ndtype of labels: torch.int64\n            \n\n\n\n# Plot some images\nplt.figure(figsize=(6, 6))\nfor i in range(25):\n    plt.subplot(5, 5, i+1)\n    plt.imshow(torch.einsum(\"chw-&gt;hwc\", dataset.data[i].cpu()))\n    plt.axis('off')\n    plt.title(dataset.classes[dataset.targets[i]])\nplt.tight_layout()"
  },
  {
    "objectID": "notebooks/ssl.html#train-val-test-split",
    "href": "notebooks/ssl.html#train-val-test-split",
    "title": "Imports",
    "section": "Train val test split",
    "text": "Train val test split\n\nn_train = 1000\nn_test = 20000\n\nX = dataset.data\ny = dataset.targets\n\nprint(X.shape)\nprint(X.shape, X.dtype)\nprint(X.min(), X.max())\nprint(y.shape, y.dtype)\n\ntorch.Size([60000, 3, 32, 32])\ntorch.Size([60000, 3, 32, 32]) torch.float32\ntensor(0.) tensor(1.)\ntorch.Size([60000]) torch.int64\n\n\n\ntorch.manual_seed(0)\nidx = torch.randperm(len(X))\ntrain_idx = idx[:n_train]\npool_idx = idx[n_train:-n_test]\ntest_idx = idx[-n_test:]\nprint(len(train_idx), len(pool_idx), len(test_idx))\n\n1000 39000 20000\n\n\n\nresnet = ResNetClassifier(models.resnet18, models.ResNet18_Weights.DEFAULT, n_classes=10).to(device)\n\n\nresnet\n\nResNetClassifier(\n  (featurizer): ResNet(\n    (resnet): ResNet(\n      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (layer1): Sequential(\n        (0): BasicBlock(\n          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (1): BasicBlock(\n          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (layer2): Sequential(\n        (0): BasicBlock(\n          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (downsample): Sequential(\n            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): BasicBlock(\n          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (layer3): Sequential(\n        (0): BasicBlock(\n          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (downsample): Sequential(\n            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): BasicBlock(\n          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (layer4): Sequential(\n        (0): BasicBlock(\n          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (downsample): Sequential(\n            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): BasicBlock(\n          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n      (fc): Identity()\n    )\n    (flatten): Flatten(start_dim=1, end_dim=-1)\n  )\n  (classifier): MLPClassifier(\n    (featurizer): MLP(\n      (activation): ReLU()\n      (dropout): Dropout(p=0.0, inplace=True)\n      (input_layer): Linear(in_features=512, out_features=512, bias=True)\n    )\n    (classifier): Linear(in_features=512, out_features=10, bias=True)\n  )\n)\n\n\n\ntorchsummary.summary(resnet, (3, 32, 32))\n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 64, 16, 16]           9,408\n       BatchNorm2d-2           [-1, 64, 16, 16]             128\n              ReLU-3           [-1, 64, 16, 16]               0\n         MaxPool2d-4             [-1, 64, 8, 8]               0\n            Conv2d-5             [-1, 64, 8, 8]          36,864\n       BatchNorm2d-6             [-1, 64, 8, 8]             128\n              ReLU-7             [-1, 64, 8, 8]               0\n            Conv2d-8             [-1, 64, 8, 8]          36,864\n       BatchNorm2d-9             [-1, 64, 8, 8]             128\n             ReLU-10             [-1, 64, 8, 8]               0\n       BasicBlock-11             [-1, 64, 8, 8]               0\n           Conv2d-12             [-1, 64, 8, 8]          36,864\n      BatchNorm2d-13             [-1, 64, 8, 8]             128\n             ReLU-14             [-1, 64, 8, 8]               0\n           Conv2d-15             [-1, 64, 8, 8]          36,864\n      BatchNorm2d-16             [-1, 64, 8, 8]             128\n             ReLU-17             [-1, 64, 8, 8]               0\n       BasicBlock-18             [-1, 64, 8, 8]               0\n           Conv2d-19            [-1, 128, 4, 4]          73,728\n      BatchNorm2d-20            [-1, 128, 4, 4]             256\n             ReLU-21            [-1, 128, 4, 4]               0\n           Conv2d-22            [-1, 128, 4, 4]         147,456\n      BatchNorm2d-23            [-1, 128, 4, 4]             256\n           Conv2d-24            [-1, 128, 4, 4]           8,192\n      BatchNorm2d-25            [-1, 128, 4, 4]             256\n             ReLU-26            [-1, 128, 4, 4]               0\n       BasicBlock-27            [-1, 128, 4, 4]               0\n           Conv2d-28            [-1, 128, 4, 4]         147,456\n      BatchNorm2d-29            [-1, 128, 4, 4]             256\n             ReLU-30            [-1, 128, 4, 4]               0\n           Conv2d-31            [-1, 128, 4, 4]         147,456\n      BatchNorm2d-32            [-1, 128, 4, 4]             256\n             ReLU-33            [-1, 128, 4, 4]               0\n       BasicBlock-34            [-1, 128, 4, 4]               0\n           Conv2d-35            [-1, 256, 2, 2]         294,912\n      BatchNorm2d-36            [-1, 256, 2, 2]             512\n             ReLU-37            [-1, 256, 2, 2]               0\n           Conv2d-38            [-1, 256, 2, 2]         589,824\n      BatchNorm2d-39            [-1, 256, 2, 2]             512\n           Conv2d-40            [-1, 256, 2, 2]          32,768\n      BatchNorm2d-41            [-1, 256, 2, 2]             512\n             ReLU-42            [-1, 256, 2, 2]               0\n       BasicBlock-43            [-1, 256, 2, 2]               0\n           Conv2d-44            [-1, 256, 2, 2]         589,824\n      BatchNorm2d-45            [-1, 256, 2, 2]             512\n             ReLU-46            [-1, 256, 2, 2]               0\n           Conv2d-47            [-1, 256, 2, 2]         589,824\n      BatchNorm2d-48            [-1, 256, 2, 2]             512\n             ReLU-49            [-1, 256, 2, 2]               0\n       BasicBlock-50            [-1, 256, 2, 2]               0\n           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n             ReLU-53            [-1, 512, 1, 1]               0\n           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n           Conv2d-56            [-1, 512, 1, 1]         131,072\n      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n             ReLU-58            [-1, 512, 1, 1]               0\n       BasicBlock-59            [-1, 512, 1, 1]               0\n           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n             ReLU-62            [-1, 512, 1, 1]               0\n           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n             ReLU-65            [-1, 512, 1, 1]               0\n       BasicBlock-66            [-1, 512, 1, 1]               0\nAdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n         Identity-68                  [-1, 512]               0\n           ResNet-69                  [-1, 512]               0\n          Flatten-70                  [-1, 512]               0\n           ResNet-71                  [-1, 512]               0\n           Linear-72                  [-1, 512]         262,656\n             ReLU-73                  [-1, 512]               0\n          Dropout-74                  [-1, 512]               0\n              MLP-75                  [-1, 512]               0\n           Linear-76                   [-1, 10]           5,130\n    MLPClassifier-77                   [-1, 10]               0\n================================================================\nTotal params: 11,444,298\nTrainable params: 11,444,298\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.01\nForward/backward pass size (MB): 1.32\nParams size (MB): 43.66\nEstimated Total Size (MB): 44.98\n----------------------------------------------------------------\n\n\n\n# Export to ONNX and visualize with Netron\ndummy_input = torch.randn(1, 3, 32, 32).to(device)\ntorch.onnx.export(resnet, dummy_input, \"resnet.onnx\", verbose=True)\nnetron.start(\"resnet.onnx\")\n\nExported graph: graph(%input.1 : Float(1, 3, 32, 32, strides=[3072, 1024, 32, 1], requires_grad=0, device=cuda:0),\n      %classifier.featurizer.input_layer.weight : Float(512, 512, strides=[512, 1], requires_grad=1, device=cuda:0),\n      %classifier.featurizer.input_layer.bias : Float(512, strides=[1], requires_grad=1, device=cuda:0),\n      %classifier.classifier.weight : Float(10, 512, strides=[512, 1], requires_grad=1, device=cuda:0),\n      %classifier.classifier.bias : Float(10, strides=[1], requires_grad=1, device=cuda:0),\n      %onnx::Conv_198 : Float(64, 3, 7, 7, strides=[147, 49, 7, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_199 : Float(64, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_201 : Float(64, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_202 : Float(64, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_204 : Float(64, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_205 : Float(64, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_207 : Float(64, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_208 : Float(64, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_210 : Float(64, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_211 : Float(64, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_213 : Float(128, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_214 : Float(128, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_216 : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_217 : Float(128, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_219 : Float(128, 64, 1, 1, strides=[64, 1, 1, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_220 : Float(128, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_222 : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_223 : Float(128, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_225 : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_226 : Float(128, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_228 : Float(256, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_229 : Float(256, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_231 : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_232 : Float(256, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_234 : Float(256, 128, 1, 1, strides=[128, 1, 1, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_235 : Float(256, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_237 : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_238 : Float(256, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_240 : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_241 : Float(256, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_243 : Float(512, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_244 : Float(512, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_246 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_247 : Float(512, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_249 : Float(512, 256, 1, 1, strides=[256, 1, 1, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_250 : Float(512, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_252 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_253 : Float(512, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_255 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_256 : Float(512, strides=[1], requires_grad=0, device=cuda:0)):\n  %/featurizer/resnet/conv1/Conv_output_0 : Float(1, 64, 16, 16, strides=[16384, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[7, 7], pads=[3, 3, 3, 3], strides=[2, 2], onnx_name=\"/featurizer/resnet/conv1/Conv\"](%input.1, %onnx::Conv_198, %onnx::Conv_199), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.conv.Conv2d::conv1 # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n  %/featurizer/resnet/relu/Relu_output_0 : Float(1, 64, 16, 16, strides=[16384, 256, 16, 1], requires_grad=1, device=cuda:0) = onnx::Relu[onnx_name=\"/featurizer/resnet/relu/Relu\"](%/featurizer/resnet/conv1/Conv_output_0), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.activation.ReLU::relu # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n  %/featurizer/resnet/maxpool/MaxPool_output_0 : Float(1, 64, 8, 8, strides=[4096, 64, 8, 1], requires_grad=1, device=cuda:0) = onnx::MaxPool[ceil_mode=0, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2], onnx_name=\"/featurizer/resnet/maxpool/MaxPool\"](%/featurizer/resnet/relu/Relu_output_0), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.pooling.MaxPool2d::maxpool # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/functional.py:782:0\n  %/featurizer/resnet/layer1/layer1.0/conv1/Conv_output_0 : Float(1, 64, 8, 8, strides=[4096, 64, 8, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/featurizer/resnet/layer1/layer1.0/conv1/Conv\"](%/featurizer/resnet/maxpool/MaxPool_output_0, %onnx::Conv_201, %onnx::Conv_202), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer1/torchvision.models.resnet.BasicBlock::layer1.0/torch.nn.modules.conv.Conv2d::conv1 # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n  %/featurizer/resnet/layer1/layer1.0/relu/Relu_output_0 : Float(1, 64, 8, 8, strides=[4096, 64, 8, 1], requires_grad=1, device=cuda:0) = onnx::Relu[onnx_name=\"/featurizer/resnet/layer1/layer1.0/relu/Relu\"](%/featurizer/resnet/layer1/layer1.0/conv1/Conv_output_0), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer1/torchvision.models.resnet.BasicBlock::layer1.0/torch.nn.modules.activation.ReLU::relu # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n  %/featurizer/resnet/layer1/layer1.0/conv2/Conv_output_0 : Float(1, 64, 8, 8, strides=[4096, 64, 8, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/featurizer/resnet/layer1/layer1.0/conv2/Conv\"](%/featurizer/resnet/layer1/layer1.0/relu/Relu_output_0, %onnx::Conv_204, %onnx::Conv_205), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer1/torchvision.models.resnet.BasicBlock::layer1.0/torch.nn.modules.conv.Conv2d::conv2 # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n  %/featurizer/resnet/layer1/layer1.0/Add_output_0 : Float(1, 64, 8, 8, strides=[4096, 64, 8, 1], requires_grad=1, device=cuda:0) = onnx::Add[onnx_name=\"/featurizer/resnet/layer1/layer1.0/Add\"](%/featurizer/resnet/layer1/layer1.0/conv2/Conv_output_0, %/featurizer/resnet/maxpool/MaxPool_output_0), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer1/torchvision.models.resnet.BasicBlock::layer1.0 # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torchvision/models/resnet.py:102:0\n  %/featurizer/resnet/layer1/layer1.0/relu_1/Relu_output_0 : Float(1, 64, 8, 8, strides=[4096, 64, 8, 1], requires_grad=1, device=cuda:0) = onnx::Relu[onnx_name=\"/featurizer/resnet/layer1/layer1.0/relu_1/Relu\"](%/featurizer/resnet/layer1/layer1.0/Add_output_0), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer1/torchvision.models.resnet.BasicBlock::layer1.0/torch.nn.modules.activation.ReLU::relu # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n  %/featurizer/resnet/layer1/layer1.1/conv1/Conv_output_0 : Float(1, 64, 8, 8, strides=[4096, 64, 8, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/featurizer/resnet/layer1/layer1.1/conv1/Conv\"](%/featurizer/resnet/layer1/layer1.0/relu_1/Relu_output_0, %onnx::Conv_207, %onnx::Conv_208), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer1/torchvision.models.resnet.BasicBlock::layer1.1/torch.nn.modules.conv.Conv2d::conv1 # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n  %/featurizer/resnet/layer1/layer1.1/relu/Relu_output_0 : Float(1, 64, 8, 8, strides=[4096, 64, 8, 1], requires_grad=1, device=cuda:0) = onnx::Relu[onnx_name=\"/featurizer/resnet/layer1/layer1.1/relu/Relu\"](%/featurizer/resnet/layer1/layer1.1/conv1/Conv_output_0), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer1/torchvision.models.resnet.BasicBlock::layer1.1/torch.nn.modules.activation.ReLU::relu # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n  %/featurizer/resnet/layer1/layer1.1/conv2/Conv_output_0 : Float(1, 64, 8, 8, strides=[4096, 64, 8, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/featurizer/resnet/layer1/layer1.1/conv2/Conv\"](%/featurizer/resnet/layer1/layer1.1/relu/Relu_output_0, %onnx::Conv_210, %onnx::Conv_211), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer1/torchvision.models.resnet.BasicBlock::layer1.1/torch.nn.modules.conv.Conv2d::conv2 # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n  %/featurizer/resnet/layer1/layer1.1/Add_output_0 : Float(1, 64, 8, 8, strides=[4096, 64, 8, 1], requires_grad=1, device=cuda:0) = onnx::Add[onnx_name=\"/featurizer/resnet/layer1/layer1.1/Add\"](%/featurizer/resnet/layer1/layer1.1/conv2/Conv_output_0, %/featurizer/resnet/layer1/layer1.0/relu_1/Relu_output_0), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer1/torchvision.models.resnet.BasicBlock::layer1.1 # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torchvision/models/resnet.py:102:0\n  %/featurizer/resnet/layer1/layer1.1/relu_1/Relu_output_0 : Float(1, 64, 8, 8, strides=[4096, 64, 8, 1], requires_grad=1, device=cuda:0) = onnx::Relu[onnx_name=\"/featurizer/resnet/layer1/layer1.1/relu_1/Relu\"](%/featurizer/resnet/layer1/layer1.1/Add_output_0), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer1/torchvision.models.resnet.BasicBlock::layer1.1/torch.nn.modules.activation.ReLU::relu # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n  %/featurizer/resnet/layer2/layer2.0/conv1/Conv_output_0 : Float(1, 128, 4, 4, strides=[2048, 16, 4, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2], onnx_name=\"/featurizer/resnet/layer2/layer2.0/conv1/Conv\"](%/featurizer/resnet/layer1/layer1.1/relu_1/Relu_output_0, %onnx::Conv_213, %onnx::Conv_214), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer2/torchvision.models.resnet.BasicBlock::layer2.0/torch.nn.modules.conv.Conv2d::conv1 # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n  %/featurizer/resnet/layer2/layer2.0/relu/Relu_output_0 : Float(1, 128, 4, 4, strides=[2048, 16, 4, 1], requires_grad=1, device=cuda:0) = onnx::Relu[onnx_name=\"/featurizer/resnet/layer2/layer2.0/relu/Relu\"](%/featurizer/resnet/layer2/layer2.0/conv1/Conv_output_0), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer2/torchvision.models.resnet.BasicBlock::layer2.0/torch.nn.modules.activation.ReLU::relu # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n  %/featurizer/resnet/layer2/layer2.0/conv2/Conv_output_0 : Float(1, 128, 4, 4, strides=[2048, 16, 4, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/featurizer/resnet/layer2/layer2.0/conv2/Conv\"](%/featurizer/resnet/layer2/layer2.0/relu/Relu_output_0, %onnx::Conv_216, %onnx::Conv_217), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer2/torchvision.models.resnet.BasicBlock::layer2.0/torch.nn.modules.conv.Conv2d::conv2 # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n  %/featurizer/resnet/layer2/layer2.0/downsample/downsample.0/Conv_output_0 : Float(1, 128, 4, 4, strides=[2048, 16, 4, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/featurizer/resnet/layer2/layer2.0/downsample/downsample.0/Conv\"](%/featurizer/resnet/layer1/layer1.1/relu_1/Relu_output_0, %onnx::Conv_219, %onnx::Conv_220), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer2/torchvision.models.resnet.BasicBlock::layer2.0/torch.nn.modules.container.Sequential::downsample/torch.nn.modules.conv.Conv2d::downsample.0 # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n  %/featurizer/resnet/layer2/layer2.0/Add_output_0 : Float(1, 128, 4, 4, strides=[2048, 16, 4, 1], requires_grad=1, device=cuda:0) = onnx::Add[onnx_name=\"/featurizer/resnet/layer2/layer2.0/Add\"](%/featurizer/resnet/layer2/layer2.0/conv2/Conv_output_0, %/featurizer/resnet/layer2/layer2.0/downsample/downsample.0/Conv_output_0), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer2/torchvision.models.resnet.BasicBlock::layer2.0 # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torchvision/models/resnet.py:102:0\n  %/featurizer/resnet/layer2/layer2.0/relu_1/Relu_output_0 : Float(1, 128, 4, 4, strides=[2048, 16, 4, 1], requires_grad=1, device=cuda:0) = onnx::Relu[onnx_name=\"/featurizer/resnet/layer2/layer2.0/relu_1/Relu\"](%/featurizer/resnet/layer2/layer2.0/Add_output_0), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer2/torchvision.models.resnet.BasicBlock::layer2.0/torch.nn.modules.activation.ReLU::relu # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n  %/featurizer/resnet/layer2/layer2.1/conv1/Conv_output_0 : Float(1, 128, 4, 4, strides=[2048, 16, 4, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/featurizer/resnet/layer2/layer2.1/conv1/Conv\"](%/featurizer/resnet/layer2/layer2.0/relu_1/Relu_output_0, %onnx::Conv_222, %onnx::Conv_223), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer2/torchvision.models.resnet.BasicBlock::layer2.1/torch.nn.modules.conv.Conv2d::conv1 # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n  %/featurizer/resnet/layer2/layer2.1/relu/Relu_output_0 : Float(1, 128, 4, 4, strides=[2048, 16, 4, 1], requires_grad=1, device=cuda:0) = onnx::Relu[onnx_name=\"/featurizer/resnet/layer2/layer2.1/relu/Relu\"](%/featurizer/resnet/layer2/layer2.1/conv1/Conv_output_0), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer2/torchvision.models.resnet.BasicBlock::layer2.1/torch.nn.modules.activation.ReLU::relu # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n  %/featurizer/resnet/layer2/layer2.1/conv2/Conv_output_0 : Float(1, 128, 4, 4, strides=[2048, 16, 4, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/featurizer/resnet/layer2/layer2.1/conv2/Conv\"](%/featurizer/resnet/layer2/layer2.1/relu/Relu_output_0, %onnx::Conv_225, %onnx::Conv_226), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer2/torchvision.models.resnet.BasicBlock::layer2.1/torch.nn.modules.conv.Conv2d::conv2 # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n  %/featurizer/resnet/layer2/layer2.1/Add_output_0 : Float(1, 128, 4, 4, strides=[2048, 16, 4, 1], requires_grad=1, device=cuda:0) = onnx::Add[onnx_name=\"/featurizer/resnet/layer2/layer2.1/Add\"](%/featurizer/resnet/layer2/layer2.1/conv2/Conv_output_0, %/featurizer/resnet/layer2/layer2.0/relu_1/Relu_output_0), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer2/torchvision.models.resnet.BasicBlock::layer2.1 # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torchvision/models/resnet.py:102:0\n  %/featurizer/resnet/layer2/layer2.1/relu_1/Relu_output_0 : Float(1, 128, 4, 4, strides=[2048, 16, 4, 1], requires_grad=1, device=cuda:0) = onnx::Relu[onnx_name=\"/featurizer/resnet/layer2/layer2.1/relu_1/Relu\"](%/featurizer/resnet/layer2/layer2.1/Add_output_0), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer2/torchvision.models.resnet.BasicBlock::layer2.1/torch.nn.modules.activation.ReLU::relu # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n  %/featurizer/resnet/layer3/layer3.0/conv1/Conv_output_0 : Float(1, 256, 2, 2, strides=[1024, 4, 2, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2], onnx_name=\"/featurizer/resnet/layer3/layer3.0/conv1/Conv\"](%/featurizer/resnet/layer2/layer2.1/relu_1/Relu_output_0, %onnx::Conv_228, %onnx::Conv_229), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer3/torchvision.models.resnet.BasicBlock::layer3.0/torch.nn.modules.conv.Conv2d::conv1 # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n  %/featurizer/resnet/layer3/layer3.0/relu/Relu_output_0 : Float(1, 256, 2, 2, strides=[1024, 4, 2, 1], requires_grad=1, device=cuda:0) = onnx::Relu[onnx_name=\"/featurizer/resnet/layer3/layer3.0/relu/Relu\"](%/featurizer/resnet/layer3/layer3.0/conv1/Conv_output_0), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer3/torchvision.models.resnet.BasicBlock::layer3.0/torch.nn.modules.activation.ReLU::relu # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n  %/featurizer/resnet/layer3/layer3.0/conv2/Conv_output_0 : Float(1, 256, 2, 2, strides=[1024, 4, 2, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/featurizer/resnet/layer3/layer3.0/conv2/Conv\"](%/featurizer/resnet/layer3/layer3.0/relu/Relu_output_0, %onnx::Conv_231, %onnx::Conv_232), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer3/torchvision.models.resnet.BasicBlock::layer3.0/torch.nn.modules.conv.Conv2d::conv2 # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n  %/featurizer/resnet/layer3/layer3.0/downsample/downsample.0/Conv_output_0 : Float(1, 256, 2, 2, strides=[1024, 4, 2, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/featurizer/resnet/layer3/layer3.0/downsample/downsample.0/Conv\"](%/featurizer/resnet/layer2/layer2.1/relu_1/Relu_output_0, %onnx::Conv_234, %onnx::Conv_235), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer3/torchvision.models.resnet.BasicBlock::layer3.0/torch.nn.modules.container.Sequential::downsample/torch.nn.modules.conv.Conv2d::downsample.0 # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n  %/featurizer/resnet/layer3/layer3.0/Add_output_0 : Float(1, 256, 2, 2, strides=[1024, 4, 2, 1], requires_grad=1, device=cuda:0) = onnx::Add[onnx_name=\"/featurizer/resnet/layer3/layer3.0/Add\"](%/featurizer/resnet/layer3/layer3.0/conv2/Conv_output_0, %/featurizer/resnet/layer3/layer3.0/downsample/downsample.0/Conv_output_0), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer3/torchvision.models.resnet.BasicBlock::layer3.0 # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torchvision/models/resnet.py:102:0\n  %/featurizer/resnet/layer3/layer3.0/relu_1/Relu_output_0 : Float(1, 256, 2, 2, strides=[1024, 4, 2, 1], requires_grad=1, device=cuda:0) = onnx::Relu[onnx_name=\"/featurizer/resnet/layer3/layer3.0/relu_1/Relu\"](%/featurizer/resnet/layer3/layer3.0/Add_output_0), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer3/torchvision.models.resnet.BasicBlock::layer3.0/torch.nn.modules.activation.ReLU::relu # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n  %/featurizer/resnet/layer3/layer3.1/conv1/Conv_output_0 : Float(1, 256, 2, 2, strides=[1024, 4, 2, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/featurizer/resnet/layer3/layer3.1/conv1/Conv\"](%/featurizer/resnet/layer3/layer3.0/relu_1/Relu_output_0, %onnx::Conv_237, %onnx::Conv_238), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer3/torchvision.models.resnet.BasicBlock::layer3.1/torch.nn.modules.conv.Conv2d::conv1 # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n  %/featurizer/resnet/layer3/layer3.1/relu/Relu_output_0 : Float(1, 256, 2, 2, strides=[1024, 4, 2, 1], requires_grad=1, device=cuda:0) = onnx::Relu[onnx_name=\"/featurizer/resnet/layer3/layer3.1/relu/Relu\"](%/featurizer/resnet/layer3/layer3.1/conv1/Conv_output_0), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer3/torchvision.models.resnet.BasicBlock::layer3.1/torch.nn.modules.activation.ReLU::relu # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n  %/featurizer/resnet/layer3/layer3.1/conv2/Conv_output_0 : Float(1, 256, 2, 2, strides=[1024, 4, 2, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/featurizer/resnet/layer3/layer3.1/conv2/Conv\"](%/featurizer/resnet/layer3/layer3.1/relu/Relu_output_0, %onnx::Conv_240, %onnx::Conv_241), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer3/torchvision.models.resnet.BasicBlock::layer3.1/torch.nn.modules.conv.Conv2d::conv2 # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n  %/featurizer/resnet/layer3/layer3.1/Add_output_0 : Float(1, 256, 2, 2, strides=[1024, 4, 2, 1], requires_grad=1, device=cuda:0) = onnx::Add[onnx_name=\"/featurizer/resnet/layer3/layer3.1/Add\"](%/featurizer/resnet/layer3/layer3.1/conv2/Conv_output_0, %/featurizer/resnet/layer3/layer3.0/relu_1/Relu_output_0), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer3/torchvision.models.resnet.BasicBlock::layer3.1 # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torchvision/models/resnet.py:102:0\n  %/featurizer/resnet/layer3/layer3.1/relu_1/Relu_output_0 : Float(1, 256, 2, 2, strides=[1024, 4, 2, 1], requires_grad=1, device=cuda:0) = onnx::Relu[onnx_name=\"/featurizer/resnet/layer3/layer3.1/relu_1/Relu\"](%/featurizer/resnet/layer3/layer3.1/Add_output_0), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer3/torchvision.models.resnet.BasicBlock::layer3.1/torch.nn.modules.activation.ReLU::relu # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n  %/featurizer/resnet/layer4/layer4.0/conv1/Conv_output_0 : Float(1, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2], onnx_name=\"/featurizer/resnet/layer4/layer4.0/conv1/Conv\"](%/featurizer/resnet/layer3/layer3.1/relu_1/Relu_output_0, %onnx::Conv_243, %onnx::Conv_244), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer4/torchvision.models.resnet.BasicBlock::layer4.0/torch.nn.modules.conv.Conv2d::conv1 # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n  %/featurizer/resnet/layer4/layer4.0/relu/Relu_output_0 : Float(1, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Relu[onnx_name=\"/featurizer/resnet/layer4/layer4.0/relu/Relu\"](%/featurizer/resnet/layer4/layer4.0/conv1/Conv_output_0), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer4/torchvision.models.resnet.BasicBlock::layer4.0/torch.nn.modules.activation.ReLU::relu # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n  %/featurizer/resnet/layer4/layer4.0/conv2/Conv_output_0 : Float(1, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/featurizer/resnet/layer4/layer4.0/conv2/Conv\"](%/featurizer/resnet/layer4/layer4.0/relu/Relu_output_0, %onnx::Conv_246, %onnx::Conv_247), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer4/torchvision.models.resnet.BasicBlock::layer4.0/torch.nn.modules.conv.Conv2d::conv2 # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n  %/featurizer/resnet/layer4/layer4.0/downsample/downsample.0/Conv_output_0 : Float(1, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/featurizer/resnet/layer4/layer4.0/downsample/downsample.0/Conv\"](%/featurizer/resnet/layer3/layer3.1/relu_1/Relu_output_0, %onnx::Conv_249, %onnx::Conv_250), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer4/torchvision.models.resnet.BasicBlock::layer4.0/torch.nn.modules.container.Sequential::downsample/torch.nn.modules.conv.Conv2d::downsample.0 # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n  %/featurizer/resnet/layer4/layer4.0/Add_output_0 : Float(1, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Add[onnx_name=\"/featurizer/resnet/layer4/layer4.0/Add\"](%/featurizer/resnet/layer4/layer4.0/conv2/Conv_output_0, %/featurizer/resnet/layer4/layer4.0/downsample/downsample.0/Conv_output_0), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer4/torchvision.models.resnet.BasicBlock::layer4.0 # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torchvision/models/resnet.py:102:0\n  %/featurizer/resnet/layer4/layer4.0/relu_1/Relu_output_0 : Float(1, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Relu[onnx_name=\"/featurizer/resnet/layer4/layer4.0/relu_1/Relu\"](%/featurizer/resnet/layer4/layer4.0/Add_output_0), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer4/torchvision.models.resnet.BasicBlock::layer4.0/torch.nn.modules.activation.ReLU::relu # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n  %/featurizer/resnet/layer4/layer4.1/conv1/Conv_output_0 : Float(1, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/featurizer/resnet/layer4/layer4.1/conv1/Conv\"](%/featurizer/resnet/layer4/layer4.0/relu_1/Relu_output_0, %onnx::Conv_252, %onnx::Conv_253), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer4/torchvision.models.resnet.BasicBlock::layer4.1/torch.nn.modules.conv.Conv2d::conv1 # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n  %/featurizer/resnet/layer4/layer4.1/relu/Relu_output_0 : Float(1, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Relu[onnx_name=\"/featurizer/resnet/layer4/layer4.1/relu/Relu\"](%/featurizer/resnet/layer4/layer4.1/conv1/Conv_output_0), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer4/torchvision.models.resnet.BasicBlock::layer4.1/torch.nn.modules.activation.ReLU::relu # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n  %/featurizer/resnet/layer4/layer4.1/conv2/Conv_output_0 : Float(1, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/featurizer/resnet/layer4/layer4.1/conv2/Conv\"](%/featurizer/resnet/layer4/layer4.1/relu/Relu_output_0, %onnx::Conv_255, %onnx::Conv_256), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer4/torchvision.models.resnet.BasicBlock::layer4.1/torch.nn.modules.conv.Conv2d::conv2 # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n  %/featurizer/resnet/layer4/layer4.1/Add_output_0 : Float(1, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Add[onnx_name=\"/featurizer/resnet/layer4/layer4.1/Add\"](%/featurizer/resnet/layer4/layer4.1/conv2/Conv_output_0, %/featurizer/resnet/layer4/layer4.0/relu_1/Relu_output_0), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer4/torchvision.models.resnet.BasicBlock::layer4.1 # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torchvision/models/resnet.py:102:0\n  %/featurizer/resnet/layer4/layer4.1/relu_1/Relu_output_0 : Float(1, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::Relu[onnx_name=\"/featurizer/resnet/layer4/layer4.1/relu_1/Relu\"](%/featurizer/resnet/layer4/layer4.1/Add_output_0), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.container.Sequential::layer4/torchvision.models.resnet.BasicBlock::layer4.1/torch.nn.modules.activation.ReLU::relu # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n  %/featurizer/resnet/avgpool/GlobalAveragePool_output_0 : Float(1, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cuda:0) = onnx::GlobalAveragePool[onnx_name=\"/featurizer/resnet/avgpool/GlobalAveragePool\"](%/featurizer/resnet/layer4/layer4.1/relu_1/Relu_output_0), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet/torch.nn.modules.pooling.AdaptiveAvgPool2d::avgpool # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/functional.py:1214:0\n  %/featurizer/resnet/Flatten_output_0 : Float(1, 512, strides=[512, 1], requires_grad=1, device=cuda:0) = onnx::Flatten[axis=1, onnx_name=\"/featurizer/resnet/Flatten\"](%/featurizer/resnet/avgpool/GlobalAveragePool_output_0), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torchvision.models.resnet.ResNet::resnet # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torchvision/models/resnet.py:279:0\n  %/featurizer/flatten/Flatten_output_0 : Float(1, 512, strides=[512, 1], requires_grad=1, device=cuda:0) = onnx::Flatten[axis=1, onnx_name=\"/featurizer/flatten/Flatten\"](%/featurizer/resnet/Flatten_output_0), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.ResNet::featurizer/torch.nn.modules.flatten.Flatten::flatten # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/modules/flatten.py:46:0\n  %/classifier/featurizer/input_layer/Gemm_output_0 : Float(1, 512, strides=[512, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/classifier/featurizer/input_layer/Gemm\"](%/featurizer/flatten/Flatten_output_0, %classifier.featurizer.input_layer.weight, %classifier.featurizer.input_layer.bias), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.MLPClassifier::classifier/astra.torch.models.MLP::featurizer/torch.nn.modules.linear.Linear::input_layer # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/modules/linear.py:114:0\n  %/classifier/featurizer/activation/Relu_output_0 : Float(1, 512, strides=[512, 1], requires_grad=1, device=cuda:0) = onnx::Relu[onnx_name=\"/classifier/featurizer/activation/Relu\"](%/classifier/featurizer/input_layer/Gemm_output_0), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.MLPClassifier::classifier/astra.torch.models.MLP::featurizer/torch.nn.modules.activation.ReLU::activation # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n  %196 : Float(1, 10, strides=[10, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/classifier/classifier/Gemm\"](%/classifier/featurizer/activation/Relu_output_0, %classifier.classifier.weight, %classifier.classifier.bias), scope: astra.torch.models.ResNetClassifier::/astra.torch.models.MLPClassifier::classifier/torch.nn.modules.linear.Linear::classifier # /home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/nn/modules/linear.py:114:0\n  return (%196)\n\n============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\nverbose: False, log level: Level.ERROR\n======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n\nServing 'resnet.onnx' at http://localhost:8081\n\n\n('localhost', 8081)\n\n\n\ndef get_accuracy(net, X, y):\n    with torch.no_grad():\n        logits_pred = net(X)\n        y_pred = logits_pred.argmax(dim=1)\n        acc = (y_pred == y).float().mean()\n        return y_pred, acc\n\ndef predict(net, classes, plot_confusion_matrix=False):\n    for i, (name, idx) in enumerate(zip((\"train\", \"pool\", \"test\"), [train_idx, pool_idx, test_idx])):\n        X_dataset = X[idx].to(device)\n        y_dataset = y[idx].to(device)\n        y_pred, acc = get_accuracy(net, X_dataset, y_dataset)\n        print(f'{name} set accuracy: {acc*100:.2f}%')\n        if plot_confusion_matrix:\n            cm = confusion_matrix(y_dataset.cpu(), y_pred.cpu())\n            cm_display = ConfusionMatrixDisplay(cm, display_labels=classes).plot(values_format='d'\n                                                                                , cmap='Blues')\n            # Rotate the labels on x-axis to make them readable\n            _ = plt.xticks(rotation=90)\n            plt.show()\n\npredict(resnet, dataset.classes, plot_confusion_matrix=True)\n\ntrain set accuracy: 7.70%\npool set accuracy: 8.37%\ntest set accuracy: 8.58%\n\n\n\n\n\n\n\n\n\n\n\n\ndef viz_embeddings(net, X, y, device):\n    reducer = umap.UMAP()\n    with torch.no_grad():\n        emb = net.featurizer(X.to(device))\n    emb = emb.cpu().numpy()\n    emb = reducer.fit_transform(emb)\n    plt.figure(figsize=(4, 4))\n    plt.scatter(emb[:, 0], emb[:, 1], c=y.cpu().numpy(), cmap='tab10')\n    # Add a colorbar legend to mark color to class mapping\n    cb = plt.colorbar(boundaries=np.arange(11)-0.5)\n    cb.set_ticks(np.arange(10))\n    cb.set_ticklabels(dataset.classes)\n    plt.title(\"UMAP embeddings\")\n    plt.tight_layout()\n\nviz_embeddings(resnet, X[train_idx], y[train_idx], device)\n\n\n\n\n\nTrain the model on train set\n\nresnet = ResNetClassifier(models.resnet18, None, n_classes=10, activation=nn.GELU(), dropout=0.1).to(device)\niter_losses, epoch_losses = train_fn(resnet, X[train_idx], y[train_idx], nn.CrossEntropyLoss(), lr=3e-4, \n                                     batch_size=128, epochs=30, verbose=False)\n\n\nplt.plot(iter_losses)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Training loss\")\n\nText(0, 0.5, 'Training loss')\n\n\n\n\n\n\npredict(resnet, dataset.classes, plot_confusion_matrix=True)\n\ntrain set accuracy: 100.00%\npool set accuracy: 35.95%\ntest set accuracy: 36.27%\n\n\n\n\n\n\n\n\n\n\n\n\nviz_embeddings(resnet, X[train_idx], y[train_idx], device)\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/sklearn/manifold/_spectral_embedding.py:273: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n  warnings.warn(\n\n\n\n\n\n\nviz_embeddings(resnet, X[test_idx[:1000]], y[test_idx[:1000]], device)\n\n\n\n\n\n### Train on train + pool\ntrain_plus_pool_idx = torch.cat([train_idx, pool_idx])\n\nresnet = ResNetClassifier(models.resnet18, None, n_classes=10, activation=nn.GELU(), dropout=0.1).to(device)\n\niter_losses, epoch_losses = train_fn(resnet, X[train_plus_pool_idx], y[train_plus_pool_idx], loss_fn=nn.CrossEntropyLoss(),\n                                     lr=3e-4,\n                                        batch_size=1024, epochs=30)\n\nLoss: 1.311430: 100%|██████████| 40/40 [00:01&lt;00:00, 34.22it/s]\nLoss: 1.074545: 100%|██████████| 40/40 [00:01&lt;00:00, 36.99it/s]\nLoss: 1.028258: 100%|██████████| 40/40 [00:01&lt;00:00, 36.88it/s]\nLoss: 0.679134: 100%|██████████| 40/40 [00:01&lt;00:00, 37.03it/s]\nLoss: 0.573962: 100%|██████████| 40/40 [00:01&lt;00:00, 36.61it/s]\nLoss: 0.470612: 100%|██████████| 40/40 [00:01&lt;00:00, 37.01it/s]\nLoss: 0.579732: 100%|██████████| 40/40 [00:01&lt;00:00, 36.57it/s]\nLoss: 0.262016: 100%|██████████| 40/40 [00:01&lt;00:00, 37.01it/s]\nLoss: 0.147717: 100%|██████████| 40/40 [00:01&lt;00:00, 36.54it/s]\nLoss: 0.108952: 100%|██████████| 40/40 [00:01&lt;00:00, 36.02it/s]\nLoss: 0.271355: 100%|██████████| 40/40 [00:01&lt;00:00, 36.75it/s]\nLoss: 0.370208: 100%|██████████| 40/40 [00:01&lt;00:00, 35.67it/s]\nLoss: 0.304398: 100%|██████████| 40/40 [00:01&lt;00:00, 36.01it/s]\nLoss: 0.184816: 100%|██████████| 40/40 [00:01&lt;00:00, 36.37it/s]\nLoss: 0.196016: 100%|██████████| 40/40 [00:01&lt;00:00, 35.36it/s]\nLoss: 0.181180: 100%|██████████| 40/40 [00:01&lt;00:00, 34.56it/s]\nLoss: 0.196182: 100%|██████████| 40/40 [00:01&lt;00:00, 35.60it/s]\nLoss: 0.144379: 100%|██████████| 40/40 [00:01&lt;00:00, 35.92it/s]\nLoss: 0.047389: 100%|██████████| 40/40 [00:01&lt;00:00, 35.78it/s]\nLoss: 0.031721: 100%|██████████| 40/40 [00:01&lt;00:00, 35.50it/s]\nLoss: 0.071309: 100%|██████████| 40/40 [00:01&lt;00:00, 35.90it/s]\nLoss: 0.132249: 100%|██████████| 40/40 [00:01&lt;00:00, 35.88it/s]\nLoss: 0.064951: 100%|██████████| 40/40 [00:01&lt;00:00, 35.89it/s]\nLoss: 0.087784: 100%|██████████| 40/40 [00:01&lt;00:00, 35.95it/s]\nLoss: 0.022345: 100%|██████████| 40/40 [00:01&lt;00:00, 35.38it/s]\nLoss: 0.220102: 100%|██████████| 40/40 [00:01&lt;00:00, 34.77it/s]\nLoss: 0.058476: 100%|██████████| 40/40 [00:01&lt;00:00, 35.88it/s]\nLoss: 0.083048: 100%|██████████| 40/40 [00:01&lt;00:00, 36.26it/s]\nLoss: 0.047481: 100%|██████████| 40/40 [00:01&lt;00:00, 35.44it/s]\nLoss: 0.047544: 100%|██████████| 40/40 [00:01&lt;00:00, 35.65it/s]\n\n\nEpoch 1: 1.6589520935058595\nEpoch 2: 1.2343804992675782\nEpoch 3: 1.0032470321655274\nEpoch 4: 0.8050672988891602\nEpoch 5: 0.6229847793579102\nEpoch 6: 0.48343274230957034\nEpoch 7: 0.3824925224304199\nEpoch 8: 0.3308661190032959\nEpoch 9: 0.19066958694458008\nEpoch 10: 0.1531820728302002\nEpoch 11: 0.13519002685546874\nEpoch 12: 0.1659882785797119\nEpoch 13: 0.23326843185424806\nEpoch 14: 0.16760483856201172\nEpoch 15: 0.09269110498428344\nEpoch 16: 0.08953567190170288\nEpoch 17: 0.10240967988967896\nEpoch 18: 0.1157124849319458\nEpoch 19: 0.08735885505676269\nEpoch 20: 0.032301900148391724\nEpoch 21: 0.02235677945613861\nEpoch 22: 0.05257979347705841\nEpoch 23: 0.06249353976249695\nEpoch 24: 0.05699077892303467\nEpoch 25: 0.0789616925239563\nEpoch 26: 0.03884266901016235\nEpoch 27: 0.10908802990913391\nEpoch 28: 0.06660935344696045\nEpoch 29: 0.042903441429138184\nEpoch 30: 0.03823600392341614\n\n\n\nplt.plot(iter_losses)   \nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Training loss\")\n\nText(0, 0.5, 'Training loss')\n\n\n\n\n\n\nviz_embeddings(resnet, X[train_idx], y[train_idx], device)\n\n\n\n\n\nviz_embeddings(resnet, X[test_idx[:1000]], y[test_idx[:1000]], device)\n\n\n\n\n\npredict(resnet, dataset.classes, plot_confusion_matrix=True)\n\ntrain set accuracy: 99.10%\npool set accuracy: 99.58%\ntest set accuracy: 61.65%\n\n\n\n\n\n\n\n\n\n\n\n\n\nSSL\n\nTask 1: Predict angle of rotation (0, 90, 180, 270) as a classification task\nCreate a dataset with rotated images and corresponding labels. We can now use a much larger dataset\n\nX_train_plus_pool = torch.cat([X[train_idx], X[pool_idx]])\ny_train_plus_pool = torch.cat([y[train_idx], y[pool_idx]])\n\n\nX_train_plus_pool.shape, y_train_plus_pool.shape\n\n(torch.Size([40000, 3, 32, 32]), torch.Size([40000]))\n\n\n\nX_ssl = []\ny_ssl = []\n\nangles_map = {0:0, 90:1, 180:2, 270:3}\nfor angle_rot in angles_map.keys():\n    print(f\"Angle: {angle_rot}\")\n    X_rot = transforms.functional.rotate(X_train_plus_pool, angle_rot)\n    X_ssl.append(X_rot)\n    y_ssl.append(torch.tensor([angles_map[angle_rot]]*len(X_rot)))\n    \nX_ssl = torch.cat(X_ssl)\ny_ssl = torch.cat(y_ssl)\n\nAngle: 0\nAngle: 90\nAngle: 180\nAngle: 270\n\n\n\nX_ssl.shape, y_ssl.shape\n\n(torch.Size([160000, 3, 32, 32]), torch.Size([160000]))\n\n\n\n# Plot same image rotated at different angles\ndef plot_ssl(img_id):\n    plt.figure(figsize=(3, 3))\n    offset = len(X_train_plus_pool)\n    for i in range(4):\n        plt.subplot(2, 2, i+1)\n        plt.imshow(torch.einsum(\"chw-&gt;hwc\", X_ssl[offset*i + img_id]))\n        plt.axis('off')\n        plt.title(f\"Class: {angles_map[i*90]}\\n Angle: {i*90}\")\n    plt.tight_layout()\nplot_ssl(2)\n\n\n\n\n\nssl_angle = ResNetClassifier(models.resnet18, None, n_classes=4, activation=nn.GELU(), dropout=0.1).to(device)\n\n\niter_losses, epoch_losses = train_fn(ssl_angle, X_ssl, y_ssl, lr=3e-4, loss_fn=nn.CrossEntropyLoss(),\n                                        batch_size=1024, epochs=20)\n\nLoss: 0.945101: 100%|██████████| 157/157 [00:04&lt;00:00, 35.35it/s]\nLoss: 0.785275: 100%|██████████| 157/157 [00:04&lt;00:00, 35.88it/s]\nLoss: 0.644941: 100%|██████████| 157/157 [00:04&lt;00:00, 35.64it/s]\nLoss: 0.697314: 100%|██████████| 157/157 [00:04&lt;00:00, 36.29it/s]\nLoss: 0.578679: 100%|██████████| 157/157 [00:04&lt;00:00, 35.66it/s]\nLoss: 0.478466: 100%|██████████| 157/157 [00:04&lt;00:00, 36.11it/s]\nLoss: 0.466891: 100%|██████████| 157/157 [00:04&lt;00:00, 35.93it/s]\nLoss: 0.436392: 100%|██████████| 157/157 [00:04&lt;00:00, 35.65it/s]\nLoss: 0.302828: 100%|██████████| 157/157 [00:04&lt;00:00, 36.11it/s]\nLoss: 0.226936: 100%|██████████| 157/157 [00:04&lt;00:00, 35.73it/s]\nLoss: 0.260345: 100%|██████████| 157/157 [00:04&lt;00:00, 35.70it/s]\nLoss: 0.136557: 100%|██████████| 157/157 [00:04&lt;00:00, 36.13it/s]\nLoss: 0.214188: 100%|██████████| 157/157 [00:04&lt;00:00, 35.74it/s]\nLoss: 0.114707: 100%|██████████| 157/157 [00:04&lt;00:00, 35.61it/s]\nLoss: 0.100094: 100%|██████████| 157/157 [00:04&lt;00:00, 35.93it/s]\nLoss: 0.114667: 100%|██████████| 157/157 [00:04&lt;00:00, 36.11it/s]\nLoss: 0.088765: 100%|██████████| 157/157 [00:04&lt;00:00, 36.15it/s]\nLoss: 0.094059: 100%|██████████| 157/157 [00:04&lt;00:00, 36.16it/s]\nLoss: 0.135386: 100%|██████████| 157/157 [00:04&lt;00:00, 34.21it/s]\nLoss: 0.073386: 100%|██████████| 157/157 [00:04&lt;00:00, 35.75it/s]\n\n\nEpoch 1: 0.9923952384948731\nEpoch 2: 0.8178635284423829\nEpoch 3: 0.7248399837493896\nEpoch 4: 0.644944263458252\nEpoch 5: 0.5666926818847656\nEpoch 6: 0.4871123765945435\nEpoch 7: 0.4118640880584717\nEpoch 8: 0.3425796222686768\nEpoch 9: 0.2795945789337158\nEpoch 10: 0.2271963834762573\nEpoch 11: 0.18415976438522338\nEpoch 12: 0.15323887557983398\nEpoch 13: 0.13012397813796997\nEpoch 14: 0.10969336915016174\nEpoch 15: 0.09902867212295532\nEpoch 16: 0.08491913187503815\nEpoch 17: 0.07862492105960846\nEpoch 18: 0.0721992644071579\nEpoch 19: 0.068406194627285\nEpoch 20: 0.061716662764549256\n\n\n\nplt.plot(iter_losses)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Training loss\")\n\nText(0, 0.5, 'Training loss')\n\n\n\n\n\n\n# Visualise the embeddings of the SSL model trained on angles dataset \n# (but wrt original 10 classes)\nviz_embeddings(ssl_angle, X[train_idx], y[train_idx], device)\n\n\n\n\n\n# Now, we can use the features from SSLAngle model to train the classifier on the original data\n\nnet_pretrained = ResNetClassifier(models.resnet18, None, n_classes=10, activation=nn.GELU(), dropout=0.1).to(device)\nnet_pretrained.featurizer.load_state_dict(ssl_angle.featurizer.state_dict())\n\niter_losses, epoch_losses = train_fn(net_pretrained, X[train_idx], y[train_idx], nn.CrossEntropyLoss(), lr=3e-4, epochs=50, batch_size=128, verbose=False)\n\n\nplt.plot(iter_losses)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Training loss\")\n\nText(0, 0.5, 'Training loss')\n\n\n\n\n\n\nviz_embeddings(net_pretrained, X[train_idx], y[train_idx], device)\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/sklearn/manifold/_spectral_embedding.py:273: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n  warnings.warn(\n\n\n\n\n\n\nviz_embeddings(net_pretrained, X[test_idx[:1000]], y[test_idx[:1000]], device)\n\n\n\n\n\npredict(net_pretrained, dataset.classes, plot_confusion_matrix=True)\n\ntrain set accuracy: 100.00%\npool set accuracy: 47.94%\ntest set accuracy: 47.57%\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary of the test set performance\n\nUntrained model: 9%\nTrain on 1000 labeled samples (train set): 36%\nTrain on 1000 labeled samples + 39000 label samples (pool set): 62%\nTrain on SSL task + Finetune on 1000 labeled samples: 47%"
  },
  {
    "objectID": "notebooks/cats-dogs.html",
    "href": "notebooks/cats-dogs.html",
    "title": "Overconfident Neural Networks",
    "section": "",
    "text": "Reference: https://www.kaggle.com/code/jramkiss/overconfident-neural-networks/notebook\n\nimport torch\nimport torch.nn as nn\n\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import Subset, ConcatDataset, DataLoader\n\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n%matplotlib inline\n\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n\n\n# Read in data from data folder\ntrain_dir = '../data/afhq/train'\nvalid_dir = '../data/afhq/val'\n\n# Show some images\nfrom torchvision import datasets, transforms\n\n\n# Show first image in each folder: cat, dog, wild\ndef show_image(path):\n    files = os.listdir(path)\n    img = plt.imread(os.path.join(path, files[0]))\n    plt.imshow(img)\n    # Remove axis ticks\n    plt.axis('off')\n    plt.show()\n\nshow_image(os.path.join(train_dir, 'cat'))\nshow_image(os.path.join(train_dir, 'dog'))\nshow_image(os.path.join(train_dir, 'wild'))\n\n\n\n\n\n\n\n\n\n\n\n# Show sizes of each image\nfrom PIL import Image\n\ndef show_image_sizes(path):\n    files = os.listdir(path)\n    img = Image.open(os.path.join(path, files[0]))\n    print(img.size)\n\nshow_image_sizes(os.path.join(train_dir, 'cat'))\n\n(512, 512)\n\n\n\n# Simple CNN with 3 convolutional layers, and 3 classes\n\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        # Convolutional layers\n        self.conv1 = nn.Conv2d(3, 8, 3, padding=1)\n        self.conv2 = nn.Conv2d(8, 16, 3, padding=1)\n        self.conv3 = nn.Conv2d(16, 32, 3, padding=1)\n        self.conv4 = nn.Conv2d(32, 32, 3, padding=1)\n        # Max pooling layer\n        self.pool = nn.MaxPool2d(2, 2)\n        # Linear layers\n        self.fc1 = nn.Linear(32 * 32 * 32, 512)\n        self.fc2 = nn.Linear(512, 3)\n        # Dropout layer\n        self.dropout = nn.Dropout(0.25)\n        # Softmax\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, x):\n        # Convolutional layers\n        x = self.conv1(x) # Input is 3x512X512, output is 8x512x512\n        x = nn.functional.relu(x)\n        x = self.pool(x) # Output is 8x256x256\n        x = self.conv2(x) # Input is 8x256x256, output is 16x256x256\n        x = nn.functional.relu(x)\n        x = self.pool(x) # Output is 16x128x128\n        x = self.conv3(x) # Input is 16x128x128, output is 32x128x128\n        x = nn.functional.relu(x)\n        x = self.pool(x) # Output is 32x64x64\n        x = self.conv4(x) # Input is 32x64x64, output is 32x64x64\n        x = nn.functional.relu(x)\n        x = self.pool(x) # Output is 32x32x32\n        # Flatten\n        x = x.view(-1, 32 * 32 * 32)\n        # Linear layers\n        x = self.fc1(x)\n        x = nn.functional.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        # Softmax\n        x = self.softmax(x)\n        return x\n\n\ncnn = SimpleCNN()\nprint(cnn)\n\nSimpleCNN(\n  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (fc1): Linear(in_features=32768, out_features=512, bias=True)\n  (fc2): Linear(in_features=512, out_features=3, bias=True)\n  (dropout): Dropout(p=0.25, inplace=False)\n  (softmax): LogSoftmax(dim=1)\n)\n\n\n\ncnn(torch.randn(1, 3, 512, 512))\n\ntensor([[-1.0659, -1.0911, -1.1402]], grad_fn=&lt;LogSoftmaxBackward0&gt;)\n\n\n\n# Number of \n\n\nbatch_size = 32\n\n\ntransform = transforms.Compose([\n    transforms.Resize((512, 512)),\n    transforms.ToTensor()\n])\n\n\n\n# Load the dataset using ImageFolder and apply the transformation\ntrain_dataset = ImageFolder(train_dir, transform=transform)\nvalid_dataset = ImageFolder(valid_dir, transform=transform)\n\n# Select first 100 samples from cat classes\ncat_indices = [idx for idx, label in enumerate(train_dataset.targets) if label == 0]\ncat_indices = cat_indices[:100]\n\n# Select first 100 samples from dog classes\ndog_indices = [idx for idx, label in enumerate(train_dataset.targets) if label == 1]\ndog_indices = dog_indices[:100]\n\n# Select first 100 samples from wild classes\nwild_indices = [idx for idx, label in enumerate(train_dataset.targets) if label == 2]\nwild_indices = wild_indices[:100]\n\n# Concatenate the indices\nindices = cat_indices + dog_indices + wild_indices\n\n# Create a concatenated dataset\ntrain_subset = Subset(train_dataset, indices)\n\n# Create the data loaders\ntrain_loader = torch.utils.data.DataLoader(\n    train_subset,\n    batch_size=batch_size,\n    shuffle=True\n)\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Iterate over the first batch of the train_loader\nfor images, labels in train_loader:\n    # Convert images and labels to numpy arrays\n    images = images.numpy()\n    labels = labels.numpy()\n\n    # Display the images and labels\n    fig, axes = plt.subplots(figsize=(10, 5), ncols=8)\n    for i, ax in enumerate(axes):\n        ax.imshow(np.transpose(images[i], (1, 2, 0)))\n        ax.set_title(f\"Label: {labels[i]}\")\n        ax.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n    #break  # Break after displaying the first batch\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Train the model\nfrom torch import optim\n\n# Loss function\ncriterion = nn.NLLLoss()\n\n# Optimizer\noptimizer = optim.Adam(cnn.parameters(), lr=0.001)\n\n# Number of epochs\nepochs = 1\n\n# Move model to GPU\ncnn.cuda()\n\n# Train the model\nfor epoch in range(epochs):\n    train_loss = 0.0\n    valid_loss = 0.0\n    cnn.train()\n    for data, target in train_loader:\n        # Move tensors to GPU\n        data, target = data.cuda(), target.cuda()\n        # Clear gradients\n        optimizer.zero_grad()\n        # Forward pass\n        output = cnn(data)\n        # Calculate loss\n        loss = criterion(output, target)\n        # Backward pass\n        loss.backward()\n        # Update weights\n        optimizer.step()\n        # Update training loss\n        print(loss.item() * data.size(0))\n        train_loss += loss.item() * data.size(0)\n    cnn.eval()\n    \n    # Average losses\n    train_loss = train_loss / len(train_loader.dataset)\n    \n\n34.59089279174805\n35.19190216064453\n36.06556701660156\n34.57152557373047\n\n\nKeyboardInterrupt: \n\n\n\n# Predictions\ncnn.eval()\nimages, labels = next(iter(train_loader ))\n\n# Move tensors to GPU\nimages, labels = images.cuda(), labels.cuda()\n\n# Get predictions\npreds = cnn(images)\n\n\n\npreds.argmax(dim=1)\n\ntensor([1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2,\n        2, 2, 1, 1, 2, 2, 1, 2], device='cuda:0')"
  },
  {
    "objectID": "notebooks/mcmc-optimization.html",
    "href": "notebooks/mcmc-optimization.html",
    "title": "Probabilistic Machine Learning",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n# Retina display\n%config InlineBackend.figure_format = 'retina'\nimport warnings\nwarnings.filterwarnings('ignore')\nimport arviz as az\n\nhamiltorch.set_random_seed(123)\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\n\n# mixture of gaussians\n\nm = torch.distributions.MixtureSameFamily(\n    torch.distributions.Categorical(torch.tensor([0.4, 0.6])),\n    torch.distributions.Normal(torch.tensor([-2., 3.]), torch.tensor([0.4, 1.])))\n\n\n# plot the distribution\n\nx = torch.linspace(-10, 10, 1000)\nplt.plot(x, torch.exp(m.log_prob(x)))\n\n\n\n\n\ndef get_samples(m, x0, seed=42):\n    torch.manual_seed(seed)\n    samples = [x0.item()]\n    for i in range(1000):\n        xi = torch.distributions.Normal(torch.tensor(samples[-1]), 5).sample()\n        # Check if log-density is higher at new sample\n        if m.log_prob(xi) &gt; m.log_prob(torch.tensor(samples[-1])):\n            samples.append(xi.item())  \n        else:\n            samples.append(samples[-1])\n    return samples\nx0 = torch.tensor([0.1])  # Initial value\n\n\n\nsamples = get_samples(m, x0, 42)\nplt.plot(samples)\n\n\n\n\n\nsamples = get_samples(m, x0, 10)\nplt.plot(samples)\n\n\n\n\n\ndef get_samples_jump(m, x0, seed=42):\n    torch.manual_seed(seed)\n    samples = [x0.item()]\n    for i in range(10000):\n        xi = torch.distributions.Normal(torch.tensor(samples[-1]), 1).sample()\n        # Find acceptance probability\n        a = m.log_prob(xi) - m.log_prob(torch.tensor(samples[-1]))\n        a = torch.exp(a)\n        # Check if log-density is higher at new sample\n        if a &gt; 1:\n            samples.append(xi.item())\n        else:\n            u = torch.rand(1)\n            if u &lt; a:\n                samples.append(xi.item())\n            else:\n                samples.append(samples[-1])\n    return samples\n    \n\n\nget_samples_jump(m, x0, 42)\nplt.plot(samples)\n\n\n\n\n\nget_samples_jump(m, x0, 2)\nplt.plot(samples)\n\n\n\n\n\nget_samples_jump(m, x0, 10)\nplt.plot(samples)\n\n\n\n\n\naz.plot_kde(np.array(samples))\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\n# use Hamiltorch \nimport hamiltorch\n\ndef log_prob(x):\n    return m.log_prob(x)\n\n\nN = 10000\nstep_size = .3\nL = 5\n\nhamiltorch.set_random_seed(123)\nparams_init = torch.zeros(1)\nparams_hmc = hamiltorch.sample(log_prob_func=log_prob, params_init=params_init, num_samples=N,\n                               step_size=step_size, num_steps_per_sample=L)\n\nSampling (Sampler.HMC; Integrator.IMPLICIT)\nTime spent  | Time remain.| Progress             | Samples     | Samples/sec\n0d:00:00:19 | 0d:00:00:00 | #################### | 10000/10000 | 501.14       \nAcceptance Rate 0.98\n\n\n\nplt.plot(torch.stack(params_hmc).numpy())\n\n\n\n\n\nsns.kdeplot(torch.stack(params_hmc).numpy().flatten())\nplt.plot(x, torch.exp(m.log_prob(x)))\n\n\n\n\n\nimport torch\nimport torch.distributions as dist\n\ndef random_walk_metropolis_hastings(target_dist, num_samples, initial_sample, proposal_std):\n    samples = [initial_sample]\n    current_sample = initial_sample\n\n    for _ in range(num_samples):\n        # Propose a new sample from a normal distribution (random walk)\n        proposal = torch.normal(current_sample, proposal_std)\n\n        # Calculate the acceptance ratio\n        log_acceptance_ratio = target_dist.log_prob(proposal) - target_dist.log_prob(current_sample)\n\n        # Accept or reject the proposal\n        if torch.log(torch.rand(1)) &lt; log_acceptance_ratio:\n            current_sample = proposal\n\n        samples.append(current_sample)\n\n    return torch.stack(samples[1:])\n\n# Example usage:\n# Define your target distribution (e.g., a normal distribution)\n\n#target_distribution = dist.Normal(3.0, 1.0)\ntarget_distribution = m\n\n# Number of samples to generate\nnum_samples = 1000\n\n# Initial sample\ninitial_sample = torch.tensor(0.0)\n\n# Standard deviation of the proposal distribution (controls the step size)\nproposal_std = 0.5\n\n# Generate samples using RWMH\nsamples = random_walk_metropolis_hastings(target_distribution, num_samples, initial_sample, proposal_std)\n\n# Now 'samples' contains samples from the target distribution\n\n\nplt.plot(samples)"
  },
  {
    "objectID": "notebooks/nn-variants.html",
    "href": "notebooks/nn-variants.html",
    "title": "Capturing uncertainty in neural nets:",
    "section": "",
    "text": "aleatoric\n\nhomoskedastic (fixed)\nhomoskedastic (learnt)\nheteroskedastic\n\nepistemic uncertainty (Laplace approximation)\nboth aleatoric and epistemic uncertainty"
  },
  {
    "objectID": "notebooks/nn-variants.html#models-capturing-aleatoric-uncertainty",
    "href": "notebooks/nn-variants.html#models-capturing-aleatoric-uncertainty",
    "title": "Capturing uncertainty in neural nets:",
    "section": "Models capturing aleatoric uncertainty",
    "text": "Models capturing aleatoric uncertainty\n\nDataset containing homoskedastic noise\n\n# Generate data\n\ntorch.manual_seed(42)\nN = 100\nx_lin = torch.linspace(-1, 1, N)\n\nf = lambda x: 0.5 * x**2 + 0.25 * x**3\n\neps = torch.randn(N) * 0.2\n\ny = f(x_lin) + eps\n\n# Move to GPU\nx_lin = x_lin.to(device)\ny = y.to(device)\n\n\n# Plot data and true function\nplt.plot(x_lin.cpu(), y.cpu(), \"o\", label=\"data\")\nplt.plot(x_lin.cpu(), f(x_lin).cpu(), label=\"true function\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fae8c30f3a0&gt;\n\n\n\n\n\n\n\nCase 1.1: Models assuming Homoskedastic noise\n\nCase 1.1.1: Homoskedastic noise is fixed beforehand and not learned\n\nclass HomoskedasticNNFixedNoise(torch.nn.Module):\n    def __init__(self, n_hidden=10):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(1, n_hidden)\n        self.fc2 = torch.nn.Linear(n_hidden, n_hidden)\n        self.fc3 = torch.nn.Linear(n_hidden, 1)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = torch.relu(x)\n        x = self.fc2(x)\n        x = torch.relu(x)\n        mu_hat = self.fc3(x)\n        return mu_hat\n\n\ndef loss_homoskedastic_fixed_noise(model, x, y, log_noise_std):\n    mu_hat = model(x).squeeze()\n    assert mu_hat.shape == y.shape\n    noise_std = torch.exp(log_noise_std).expand_as(mu_hat)\n    dist = torch.distributions.Normal(mu_hat, noise_std)\n    return -dist.log_prob(y).mean()\n\n\nmodel_1 = HomoskedasticNNFixedNoise()\n# Move to GPU\nmodel_1.to(device)\n\nHomoskedasticNNFixedNoise(\n  (fc1): Linear(in_features=1, out_features=10, bias=True)\n  (fc2): Linear(in_features=10, out_features=10, bias=True)\n  (fc3): Linear(in_features=10, out_features=1, bias=True)\n)\n\n\n\nfixed_log_noise_std = torch.log(torch.tensor(0.5)).to(device)\nloss_homoskedastic_fixed_noise(model_1, x_lin[:, None], y, fixed_log_noise_std)\n\ntensor(0.3774, device='cuda:0', grad_fn=&lt;NegBackward0&gt;)\n\n\n\ndef plot_model_1(\n    model, l=\"Untrained model\", log_noise_param=fixed_log_noise_std, aleatoric=True\n):\n    with torch.no_grad():\n        y_hat = model(x_lin[:, None]).squeeze().cpu()\n        std = torch.exp(log_noise_param).cpu()\n\n    plt.scatter(x_lin.cpu(), y.cpu(), s=10, color=\"C0\", label=\"Data\")\n    plt.plot(x_lin.cpu(), f(x_lin.cpu()), color=\"C1\", label=\"True function\")\n    plt.plot(x_lin.cpu(), y_hat.cpu(), color=\"C2\", label=l)\n    if aleatoric:\n        # Plot the +- 2 sigma region (where sigma is fixed to 0.2)\n        plt.fill_between(\n            x_lin.cpu(), y_hat - 2 * std, y_hat + 2 * std, alpha=0.2, color=\"C2\"\n        )\n    plt.xlabel(\"$x$\")\n    plt.ylabel(\"$y$\")\n    plt.legend()\n\n\nplot_model_1(model_1, \"Untrained Model\")\n\n\n\n\n\ndef train(model, loss_func, params, x, y, log_noise_param, n_epochs=1000, lr=0.01):\n    optimizer = torch.optim.Adam(params, lr=lr)\n    for epoch in range(n_epochs):\n        # Print every 10 epochs\n        if epoch % 50 == 0:\n            noise_std = torch.exp(\n                log_noise_param\n            )  # Calculate the noise standard deviation\n            print(f\"Epoch {epoch}: loss {loss_func(model, x, y, log_noise_param)}\")\n        optimizer.zero_grad()\n        loss = loss_func(model, x, y, log_noise_param)\n        loss.backward()\n        optimizer.step()\n    return loss.item()\n\n\nparams = list(model_1.parameters())\ntrain(\n    model_1,\n    loss_homoskedastic_fixed_noise,\n    params,\n    x_lin[:, None],\n    y,\n    fixed_log_noise_std,\n    n_epochs=1000,\n    lr=0.001,\n)\n\nEpoch 0: loss 0.37737852334976196\nEpoch 50: loss 0.36576324701309204\nEpoch 100: loss 0.36115822196006775\nEpoch 150: loss 0.35357466340065\nEpoch 200: loss 0.3445783853530884\nEpoch 250: loss 0.33561965823173523\nEpoch 300: loss 0.3253604769706726\nEpoch 350: loss 0.3168313503265381\nEpoch 400: loss 0.3112250566482544\nEpoch 450: loss 0.30827972292900085\nEpoch 500: loss 0.30689162015914917\nEpoch 550: loss 0.30621206760406494\nEpoch 600: loss 0.3059086203575134\nEpoch 650: loss 0.30568134784698486\nEpoch 700: loss 0.3052031695842743\nEpoch 750: loss 0.305046409368515\nEpoch 800: loss 0.30500170588493347\nEpoch 850: loss 0.3049851953983307\nEpoch 900: loss 0.30497878789901733\nEpoch 950: loss 0.3049766719341278\n\n\n0.3049757182598114\n\n\n\nplot_model_1(model_1, \"Trained Model\")\n\n\n\n\n\n\nCase 1.1.2: Homoskedastic noise is learnt from the data\nThe model is the same as in case 1.1.1, but the noise is learned from the data. Thus, we need to modify the loss function to include the noise parameter \\(\\sigma\\).\n\n# Define the loss function\ndef loss_homoskedastic_learned_noise(model, x, y, noise):\n    mean = model(x)\n    dist = torch.distributions.Normal(mean, noise)\n    return -dist.log_prob(y).mean()\n\n\nmodel_2 = HomoskedasticNNFixedNoise()\nlog_noise_param = torch.nn.Parameter(torch.tensor(0.0).to(device))\n\n# Move to GPU\nmodel_2.to(device)\n\nHomoskedasticNNFixedNoise(\n  (fc1): Linear(in_features=1, out_features=10, bias=True)\n  (fc2): Linear(in_features=10, out_features=10, bias=True)\n  (fc3): Linear(in_features=10, out_features=1, bias=True)\n)\n\n\n\n# Plot the untrained model\nplot_model_1(model_2, \"Untrained Model\", log_noise_param=log_noise_param)\n\n\n\n\n\n# Train the model\nparams = list(model_2.parameters()) + [log_noise_param]\n\ntrain(\n    model_2,\n    loss_homoskedastic_fixed_noise,\n    params,\n    x_lin[:, None],\n    y,\n    log_noise_param,\n    n_epochs=1000,\n    lr=0.01,\n)\n\nEpoch 0: loss 0.9752065539360046\nEpoch 50: loss 0.4708317220211029\nEpoch 100: loss 0.06903044134378433\nEpoch 150: loss -0.19027572870254517\nEpoch 200: loss -0.31068307161331177\nEpoch 250: loss -0.3400699496269226\nEpoch 300: loss -0.3422872722148895\nEpoch 350: loss -0.3424949049949646\nEpoch 400: loss -0.3450787365436554\nEpoch 450: loss -0.348661333322525\nEpoch 500: loss -0.3499276638031006\nEpoch 550: loss -0.34957197308540344\nEpoch 600: loss -0.3541718125343323\nEpoch 650: loss -0.3527887165546417\nEpoch 700: loss -0.3585814833641052\nEpoch 750: loss -0.3589327931404114\nEpoch 800: loss -0.3592424690723419\nEpoch 850: loss -0.3592303693294525\nEpoch 900: loss -0.3538239300251007\nEpoch 950: loss -0.35970067977905273\n\n\n-0.3580436110496521\n\n\n\n# Plot the trained model\nplot_model_1(model_2, \"Trained Model\", log_noise_param=log_noise_param)\n\n\n\n\n\n\n\nCase 1.2: Models assuming heteroskedastic noise\n\n#### Heteroskedastic noise model\n\n\nclass HeteroskedasticNN(torch.nn.Module):\n    def __init__(self, n_hidden=10):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(1, n_hidden)\n        self.fc2 = torch.nn.Linear(n_hidden, n_hidden)\n        self.fc3 = torch.nn.Linear(n_hidden, 2)  # we learn both mu and log_noise_std\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = torch.relu(x)\n        x = self.fc2(x)\n        x = torch.relu(x)\n        z = self.fc3(x)\n        mu_hat = z[:, 0]\n        log_noise_std = z[:, 1]\n        return mu_hat, log_noise_std\n\n\nmodel_3 = HeteroskedasticNN()\n# Move to GPU\nmodel_3.to(device)\n\nHeteroskedasticNN(\n  (fc1): Linear(in_features=1, out_features=10, bias=True)\n  (fc2): Linear(in_features=10, out_features=10, bias=True)\n  (fc3): Linear(in_features=10, out_features=2, bias=True)\n)\n\n\n\ndef _plot(y_hat, std, l=\"Untrained model\", aleatoric=True):\n    plt.scatter(x_lin.cpu(), y.cpu(), s=10, color=\"C0\", label=\"Data\")\n    plt.plot(x_lin.cpu(), f(x_lin.cpu()), color=\"C1\", label=\"True function\")\n    plt.plot(x_lin.cpu(), y_hat.cpu(), color=\"C2\", label=l)\n    if aleatoric:\n        # Plot the +- 2 sigma region (where sigma is fixed to 0.2)\n        plt.fill_between(\n            x_lin.cpu(), y_hat - 2 * std, y_hat + 2 * std, alpha=0.2, color=\"C2\"\n        )\n    \n    plt.xlabel(\"$x$\")\n    plt.ylabel(\"$y$\")\n    plt.legend()\n\n\ndef plot_heteroskedastic_model(\n    model, l=\"Untrained model\", log_noise_param=fixed_log_noise_std\n):\n    with torch.no_grad():\n        y_hat, log_noise_std = model(x_lin[:, None])\n        std = torch.exp(log_noise_std).cpu()\n        y_hat = y_hat.cpu()\n\n    _plot(y_hat, std, l)\n\n\nplot_heteroskedastic_model(model_3, \"Untrained Model\")\n\n\n\n\n\n# Train\nparams = list(model_3.parameters())\n\n\ndef loss_heteroskedastic(model, x, y):\n    mu_hat, log_noise_std = model(x)\n    noise_std = torch.exp(log_noise_std)\n    dist = torch.distributions.Normal(mu_hat, noise_std)\n    return -dist.log_prob(y).mean()\n\n\ndef train_heteroskedastic(model, loss_func, params, x, y, n_epochs=1000, lr=0.01):\n    optimizer = torch.optim.Adam(params, lr=lr)\n    for epoch in range(n_epochs):\n        # Print every 10 epochs\n        if epoch % 50 == 0:\n            print(f\"Epoch {epoch}: loss {loss_func(model, x, y)}\")\n        optimizer.zero_grad()\n        loss = loss_func(model, x, y)\n        loss.backward()\n        optimizer.step()\n    return loss.item()\n\n\ntrain_heteroskedastic(\n    model_3, loss_heteroskedastic, params, x_lin[:, None], y, n_epochs=1000, lr=0.001\n)\n\nEpoch 0: loss 0.8471216559410095\nEpoch 50: loss 0.5730974674224854\nEpoch 100: loss 0.21664416790008545\nEpoch 150: loss -0.0901612937450409\nEpoch 200: loss -0.18870659172534943\nEpoch 250: loss -0.21993499994277954\nEpoch 300: loss -0.23635315895080566\nEpoch 350: loss -0.24628539383411407\nEpoch 400: loss -0.2610110640525818\nEpoch 450: loss -0.2744489908218384\nEpoch 500: loss -0.28417855501174927\nEpoch 550: loss -0.29257726669311523\nEpoch 600: loss -0.30057546496391296\nEpoch 650: loss -0.3061988353729248\nEpoch 700: loss -0.31035324931144714\nEpoch 750: loss -0.3138866126537323\nEpoch 800: loss -0.3167155683040619\nEpoch 850: loss -0.3188284635543823\nEpoch 900: loss -0.32069918513298035\nEpoch 950: loss -0.32232046127319336\n\n\n-0.323951780796051\n\n\n\n# Plot the trained model\nplot_heteroskedastic_model(model_3, \"Trained Model\")\n\n\n\n\n\n\nData with heteroskedastic noise\n\n# Now, let us try these on some data that is not homoskedastic\n\n# Generate data\n\ntorch.manual_seed(42)\nN = 100\nx_lin = torch.linspace(-1, 1, N)\n\nf = lambda x: 0.5 * x**2 + 0.25 * x**3\n\neps = torch.randn(N) * (0.1 + 0.4 * x_lin)\n\ny = f(x_lin) + eps\n\n# Move to GPU\nx_lin = x_lin.to(device)\ny = y.to(device)\n\n# Plot data and true function\nplt.plot(x_lin.cpu(), y.cpu(), \"o\", label=\"data\")\nplt.plot(x_lin.cpu(), f(x_lin).cpu(), label=\"true function\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fae80735370&gt;\n\n\n\n\n\n\n# Now, fit the homoskedastic model\n\nmodel_4 = HomoskedasticNNFixedNoise()\n# Move to GPU\nmodel_4.to(device)\n\nfixed_log_noise_std = torch.log(torch.tensor(0.5)).to(device)\n\n# Plot the untrained model\nplot_model_1(model_4, \"Untrained Model\", log_noise_param=fixed_log_noise_std)\n\n# Train the model\nparams = list(model_4.parameters())\n\n\n\n\n\ntrain(\n    model_4,\n    loss_homoskedastic_fixed_noise,\n    params,\n    x_lin[:, None],\n    y,\n    fixed_log_noise_std,\n    n_epochs=1000,\n    lr=0.001,\n)\n\n# Plot the trained model\nplot_model_1(model_4, \"Trained Model\", log_noise_param=fixed_log_noise_std)\n\nEpoch 0: loss 0.4041783809661865\nEpoch 50: loss 0.3925568163394928\nEpoch 100: loss 0.38583633303642273\nEpoch 150: loss 0.3760771155357361\nEpoch 200: loss 0.3653101623058319\nEpoch 250: loss 0.3552305996417999\nEpoch 300: loss 0.3461504280567169\nEpoch 350: loss 0.3392468988895416\nEpoch 400: loss 0.33482256531715393\nEpoch 450: loss 0.3323317766189575\nEpoch 500: loss 0.3309779167175293\nEpoch 550: loss 0.3298887312412262\nEpoch 600: loss 0.3292028307914734\nEpoch 650: loss 0.32881179451942444\nEpoch 700: loss 0.3285224139690399\nEpoch 750: loss 0.3282739520072937\nEpoch 800: loss 0.32803454995155334\nEpoch 850: loss 0.32778945565223694\nEpoch 900: loss 0.3275414705276489\nEpoch 950: loss 0.3272767663002014\n\n\n\n\n\n\n# Now, fit the homoskedastic model with learned noise\n\nmodel_5 = HomoskedasticNNFixedNoise()\nlog_noise_param = torch.nn.Parameter(torch.tensor(0.0).to(device))\n\n# Move to GPU\nmodel_5.to(device)\n\n# Plot the untrained model\nplot_model_1(model_5, \"Untrained Model\", log_noise_param=log_noise_param)\n\n\n\n\n\n# Train the model\nparams = list(model_5.parameters()) + [log_noise_param]\n\ntrain(\n    model_5,\n    loss_homoskedastic_fixed_noise,\n    params,\n    x_lin[:, None],\n    y,\n    log_noise_param,\n    n_epochs=1000,\n    lr=0.01,\n)\n\n# Plot the trained model\nplot_model_1(model_5, \"Trained Model\", log_noise_param=log_noise_param)\n\nEpoch 0: loss 0.9799039959907532\nEpoch 50: loss 0.487409383058548\nEpoch 100: loss 0.10527730733156204\nEpoch 150: loss -0.13744640350341797\nEpoch 200: loss -0.225139319896698\nEpoch 250: loss -0.24497787654399872\nEpoch 300: loss -0.24193085730075836\nEpoch 350: loss -0.2481481432914734\nEpoch 400: loss -0.2538861632347107\nEpoch 450: loss -0.25253206491470337\nEpoch 500: loss -0.2516896426677704\nEpoch 550: loss -0.2537526786327362\nEpoch 600: loss -0.25108057260513306\nEpoch 650: loss -0.2549664378166199\nEpoch 700: loss -0.2547767460346222\nEpoch 750: loss -0.25436392426490784\nEpoch 800: loss -0.2519540786743164\nEpoch 850: loss -0.25377777218818665\nEpoch 900: loss -0.2500659227371216\nEpoch 950: loss -0.2547629475593567\n\n\n\n\n\n\n# Now, fit the heteroskedastic model\n\nmodel_6 = HeteroskedasticNN()\n# Move to GPU\nmodel_6.to(device)\n\n# Plot the untrained model\nplot_heteroskedastic_model(model_6, \"Untrained Model\")\n\n\n\n\n\n# Train the model\nparams = list(model_6.parameters())\n\ntrain_heteroskedastic(\n    model_6, loss_heteroskedastic, params, x_lin[:, None], y, n_epochs=1000, lr=0.001\n)\n\nEpoch 0: loss 0.857245147228241\nEpoch 50: loss 0.5856861472129822\nEpoch 100: loss 0.24207858741283417\nEpoch 150: loss -0.006745247635990381\nEpoch 200: loss -0.0828532800078392\nEpoch 250: loss -0.13739755749702454\nEpoch 300: loss -0.19300477206707\nEpoch 350: loss -0.24690848588943481\nEpoch 400: loss -0.29788991808891296\nEpoch 450: loss -0.33707040548324585\nEpoch 500: loss -0.368070513010025\nEpoch 550: loss -0.3885715901851654\nEpoch 600: loss -0.40538740158081055\nEpoch 650: loss -0.4239954948425293\nEpoch 700: loss -0.4402863085269928\nEpoch 750: loss -0.45339828729629517\nEpoch 800: loss -0.46312591433525085\nEpoch 850: loss -0.4695749878883362\nEpoch 900: loss -0.47566917538642883\nEpoch 950: loss -0.4823095500469208\n\n\n-0.4882388710975647\n\n\n\n# Plot the trained model\nplot_heteroskedastic_model(model_6, \"Trained Model\")\n\n\n\n\n\n\nEpistemic Uncertainty: Bayesian NN with Laplace approximation\n\nmodel_7 = HomoskedasticNNFixedNoise()\n# Move to GPU\nmodel_7.to(device)\n\n\ndef negative_log_prior_last_layer(model):\n    log_prior = torch.distributions.Normal(0, 1).log_prob(model.fc3.weight).sum()\n    return -log_prior\n\n\ndef negative_log_prior(model):\n    log_prior = 0\n\n    for param in model.parameters():\n        log_prior += torch.distributions.Normal(0, 1).log_prob(param).sum()\n    return -log_prior\n\n\ndef negative_log_likelihood(model, x, y, log_noise_std):\n    mu_hat = model(x).squeeze()\n    assert mu_hat.shape == y.shape\n    noise_std = torch.exp(log_noise_std).expand_as(mu_hat)\n    dist = torch.distributions.Normal(mu_hat, noise_std)\n    return -dist.log_prob(y).sum()\n\n\ndef negative_log_joint(model, x, y, log_noise_std):\n    return negative_log_likelihood(model, x, y, log_noise_std) + negative_log_prior(\n        model\n    )\n\n\ndef negative_log_joint_last_layer(model, x, y, log_noise_std):\n    return negative_log_likelihood(\n        model, x, y, log_noise_std\n    ) + negative_log_prior_last_layer(model)\n\n\nnegative_log_prior(model_7), negative_log_prior_last_layer(model_7)\n\n(tensor(135.7282, device='cuda:0', grad_fn=&lt;NegBackward0&gt;),\n tensor(9.3809, device='cuda:0', grad_fn=&lt;NegBackward0&gt;))\n\n\n\nnegative_log_likelihood(model_7, x_lin[:, None], y, fixed_log_noise_std)\n\ntensor(42.9679, device='cuda:0', grad_fn=&lt;NegBackward0&gt;)\n\n\n\nnegative_log_joint(\n    model_7, x_lin[:, None], y, fixed_log_noise_std\n), negative_log_joint_last_layer(model_7, x_lin[:, None], y, fixed_log_noise_std)\n\n(tensor(178.6961, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n tensor(52.3488, device='cuda:0', grad_fn=&lt;AddBackward0&gt;))\n\n\n\n# Find the MAP estimate\nparams = list(model_7.parameters())\n\ntrain(\n    model_7,\n    negative_log_joint_last_layer,\n    params,\n    x_lin[:, None],\n    y,\n    fixed_log_noise_std,\n    n_epochs=1000,\n    lr=0.01,\n)\n\nEpoch 0: loss 52.34880065917969\nEpoch 50: loss 42.465431213378906\nEpoch 100: loss 41.26828384399414\nEpoch 150: loss 40.83299255371094\nEpoch 200: loss 40.593467712402344\nEpoch 250: loss 40.43766784667969\nEpoch 300: loss 40.352142333984375\nEpoch 350: loss 40.29512405395508\nEpoch 400: loss 40.25150680541992\nEpoch 450: loss 40.21705627441406\nEpoch 500: loss 40.18949890136719\nEpoch 550: loss 40.16746520996094\nEpoch 600: loss 40.14928436279297\nEpoch 650: loss 40.134132385253906\nEpoch 700: loss 40.12129211425781\nEpoch 750: loss 39.98218536376953\nEpoch 800: loss 39.971900939941406\nEpoch 850: loss 39.92092514038086\nEpoch 900: loss 39.89830780029297\nEpoch 950: loss 39.89033889770508\n\n\n39.86236572265625\n\n\n\n# Plot the trained model (MAP)\nplot_model_1(\n    model_7, \"Trained Model (MAP)\", log_noise_param=fixed_log_noise_std, aleatoric=False\n)\n\n\n\n\n\n\nWhat weighs to consider?\n\n\n\nGoal: Compute the Hessian of the negative log joint wrt the last layer weights\n\n\nChallenge: The negative log joint is a function of all the weights, not just the last layer weights\n\n\nAside on functools.partial\n\nThe functools module is for higher-order functions: functions that act on or return other functions. In general, any callable object can be treated as a function for the purposes of this module.\n\n\nprint(int(\"1001\", base=2), int(\"1001\", base=4), int(\"1001\"))\n\nfrom functools import partial\nbase_two = partial(int, base=2)\nbase_two.__doc__ = \"Convert base 2 string to an int.\"\n\nprint(base_two)\nprint(base_two.__doc__)\nprint(help(base_two))\nprint(base_two(\"1001\"))\n\n9 65 1001\nfunctools.partial(&lt;class 'int'&gt;, base=2)\nConvert base 2 string to an int.\nHelp on partial:\n\nfunctools.partial(&lt;class 'int'&gt;, base=2)\n    Convert base 2 string to an int.\n\nNone\n9\n\n\n\ndict(model_7.named_parameters())\n\n{'fc1.weight': Parameter containing:\n tensor([[-1.0390],\n         [ 1.8074],\n         [-0.8156],\n         [ 0.4683],\n         [-0.2185],\n         [-1.9768],\n         [ 0.4070],\n         [ 0.1533],\n         [ 0.4458],\n         [ 1.7506]], device='cuda:0', requires_grad=True),\n 'fc1.bias': Parameter containing:\n tensor([ 1.0401,  1.7753,  0.8183, -0.8602, -0.7015, -1.7802, -0.8812, -0.5012,\n         -0.9206, -1.0502], device='cuda:0', requires_grad=True),\n 'fc2.weight': Parameter containing:\n tensor([[-1.8896e-01, -3.1175e-01, -1.9410e-01,  1.2058e-01,  2.6375e-01,\n          -9.4066e-02, -9.1984e-02,  1.6885e-01, -1.5602e-01, -1.4952e-01],\n         [ 6.1491e-01, -2.3305e+00,  7.1547e-01,  2.7935e-01,  5.4229e-02,\n          -4.4665e+00, -1.8417e-01, -4.3629e-03,  1.7388e-02,  7.7613e-02],\n         [ 2.4445e-01,  2.5985e-01,  8.6094e-02,  9.4790e-03, -1.5800e-01,\n           4.8416e+00, -2.5323e-02, -2.7836e-01,  2.2070e-01, -1.4078e+00],\n         [-7.7526e-01,  2.4683e-01, -1.4079e+00, -1.2232e-01, -6.1607e-02,\n          -4.4529e-01, -2.0109e-01, -5.1614e-02,  2.3994e-01,  1.7788e+00],\n         [ 2.0120e-01, -1.8883e-01, -2.0688e-01,  2.7597e-01,  1.1186e-01,\n           8.4014e-03,  4.2796e-02, -2.5415e-01, -1.0558e-01,  3.0441e-01],\n         [-3.4170e-01,  4.4826e-01, -8.3544e-01, -1.7690e-01, -6.4572e-03,\n          -4.6640e-01, -3.9215e-02,  1.2869e-01, -3.0933e-01,  1.1246e+00],\n         [-2.0909e-01, -1.5434e-01,  1.2140e-01,  2.5144e-01, -8.6428e-02,\n          -1.2983e-01, -2.8594e-01, -1.6307e-01, -2.7690e-01, -7.2375e-02],\n         [ 5.0661e-02, -4.1058e+00,  2.6399e-01,  1.9840e-01, -3.0957e-01,\n           6.6729e+00,  1.0314e-01, -6.4972e-02, -3.4461e-02, -1.4278e-01],\n         [ 1.2533e-01, -9.9739e-01,  1.1985e-01,  2.0404e-02,  6.3569e-02,\n           6.0431e+00, -5.2103e-02, -1.8004e-01, -5.1145e-02,  2.5648e-01],\n         [-4.4407e-01, -1.0588e-01, -5.1852e-01,  1.6580e-01,  1.1684e-01,\n           1.3406e-02,  1.3572e-01,  3.5567e-04,  1.7499e-01,  7.3931e-02]],\n        device='cuda:0', requires_grad=True),\n 'fc2.bias': Parameter containing:\n tensor([-0.0464, -0.0669,  0.2887, -0.1685, -0.2300,  0.3106, -0.0705, -0.7083,\n         -0.8258, -0.2307], device='cuda:0', requires_grad=True),\n 'fc3.weight': Parameter containing:\n tensor([[ 2.8882e-24,  1.9463e-01, -1.0051e-01,  1.8243e-01, -2.4331e-25,\n           1.3989e-01,  2.1068e-24, -3.3861e-01, -2.2423e-01,  9.3494e-16]],\n        device='cuda:0', requires_grad=True),\n 'fc3.bias': Parameter containing:\n tensor([0.1158], device='cuda:0', requires_grad=True)}\n\n\n\n#### Aside on state_dict in PyTorch\n\nmodel_7.state_dict()\n\nOrderedDict([('fc1.weight',\n              tensor([[-1.0390],\n                      [ 1.8074],\n                      [-0.8156],\n                      [ 0.4683],\n                      [-0.2185],\n                      [-1.9768],\n                      [ 0.4070],\n                      [ 0.1533],\n                      [ 0.4458],\n                      [ 1.7506]], device='cuda:0')),\n             ('fc1.bias',\n              tensor([ 1.0401,  1.7753,  0.8183, -0.8602, -0.7015, -1.7802, -0.8812, -0.5012,\n                      -0.9206, -1.0502], device='cuda:0')),\n             ('fc2.weight',\n              tensor([[-1.8896e-01, -3.1175e-01, -1.9410e-01,  1.2058e-01,  2.6375e-01,\n                       -9.4066e-02, -9.1984e-02,  1.6885e-01, -1.5602e-01, -1.4952e-01],\n                      [ 6.1491e-01, -2.3305e+00,  7.1547e-01,  2.7935e-01,  5.4229e-02,\n                       -4.4665e+00, -1.8417e-01, -4.3629e-03,  1.7388e-02,  7.7613e-02],\n                      [ 2.4445e-01,  2.5985e-01,  8.6094e-02,  9.4790e-03, -1.5800e-01,\n                        4.8416e+00, -2.5323e-02, -2.7836e-01,  2.2070e-01, -1.4078e+00],\n                      [-7.7526e-01,  2.4683e-01, -1.4079e+00, -1.2232e-01, -6.1607e-02,\n                       -4.4529e-01, -2.0109e-01, -5.1614e-02,  2.3994e-01,  1.7788e+00],\n                      [ 2.0120e-01, -1.8883e-01, -2.0688e-01,  2.7597e-01,  1.1186e-01,\n                        8.4014e-03,  4.2796e-02, -2.5415e-01, -1.0558e-01,  3.0441e-01],\n                      [-3.4170e-01,  4.4826e-01, -8.3544e-01, -1.7690e-01, -6.4572e-03,\n                       -4.6640e-01, -3.9215e-02,  1.2869e-01, -3.0933e-01,  1.1246e+00],\n                      [-2.0909e-01, -1.5434e-01,  1.2140e-01,  2.5144e-01, -8.6428e-02,\n                       -1.2983e-01, -2.8594e-01, -1.6307e-01, -2.7690e-01, -7.2375e-02],\n                      [ 5.0661e-02, -4.1058e+00,  2.6399e-01,  1.9840e-01, -3.0957e-01,\n                        6.6729e+00,  1.0314e-01, -6.4972e-02, -3.4461e-02, -1.4278e-01],\n                      [ 1.2533e-01, -9.9739e-01,  1.1985e-01,  2.0404e-02,  6.3569e-02,\n                        6.0431e+00, -5.2103e-02, -1.8004e-01, -5.1145e-02,  2.5648e-01],\n                      [-4.4407e-01, -1.0588e-01, -5.1852e-01,  1.6580e-01,  1.1684e-01,\n                        1.3406e-02,  1.3572e-01,  3.5567e-04,  1.7499e-01,  7.3931e-02]],\n                     device='cuda:0')),\n             ('fc2.bias',\n              tensor([-0.0464, -0.0669,  0.2887, -0.1685, -0.2300,  0.3106, -0.0705, -0.7083,\n                      -0.8258, -0.2307], device='cuda:0')),\n             ('fc3.weight',\n              tensor([[ 2.8882e-24,  1.9463e-01, -1.0051e-01,  1.8243e-01, -2.4331e-25,\n                        1.3989e-01,  2.1068e-24, -3.3861e-01, -2.2423e-01,  9.3494e-16]],\n                     device='cuda:0')),\n             ('fc3.bias', tensor([0.1158], device='cuda:0'))])\n\n\n\n\nAside on torch.func.functional_call\n\nimport torch\nimport torch.nn as nn\nfrom torch.func import functional_call, grad\n\nx = torch.randn(4, 3)\nt = torch.randn(4, 3)\nmodel = nn.Linear(3, 3)\n\ny1 = functional_call(model, dict(model.named_parameters()), x)\nprint(dict(model.named_parameters()))\ny2 = model(x)\nprint(\"*\"*20)\nprint(y1)\nprint(y2)\n\n{'weight': Parameter containing:\ntensor([[ 0.5021,  0.3455,  0.0904],\n        [ 0.1903,  0.5480, -0.3725],\n        [-0.2621,  0.4038, -0.3950]], requires_grad=True), 'bias': Parameter containing:\ntensor([-0.3184,  0.4215,  0.1822], requires_grad=True)}\n********************\ntensor([[-0.3097, -0.1970, -0.7683],\n        [-0.0129,  0.6332,  0.0520],\n        [-1.2995, -0.4907, -0.0705],\n        [-0.0834,  0.6312,  0.1858]], grad_fn=&lt;AddmmBackward0&gt;)\ntensor([[-0.3097, -0.1970, -0.7683],\n        [-0.0129,  0.6332,  0.0520],\n        [-1.2995, -0.4907, -0.0705],\n        [-0.0834,  0.6312,  0.1858]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\nfrom functools import partial\n\n\ndef functional_last_layer_neg_log_prior(last_layer_weights):\n    log_prior = torch.distributions.Normal(0, 1).log_prob(last_layer_weights).sum()\n    return -log_prior\n\n\ndef functional_neg_log_likelihood(state_dict, model, x, y, log_noise_std):\n    out = torch.func.functional_call(model, state_dict, x)\n    mu_hat = out.squeeze()\n    assert mu_hat.shape == y.shape\n\n    noise_std = torch.exp(log_noise_std).expand_as(mu_hat)\n    dist = torch.distributions.Normal(mu_hat, noise_std)\n    return -dist.log_prob(y).sum()\n\n\ndef functional_last_layer_neg_log_joint(\n    last_layer_weights, state_dict, model, x, y, log_noise_std\n):\n    state_dict[\"fc3.weight\"] = last_layer_weights\n    return functional_neg_log_likelihood(\n        state_dict, model, x, y, log_noise_std\n    ) + functional_last_layer_neg_log_prior(last_layer_weights)\n\n\nstate_dict = model_7.state_dict()\nlast_layer_weights = state_dict[\"fc3.weight\"]\n\npartial_func = partial(\n    functional_last_layer_neg_log_joint,\n    state_dict=state_dict,\n    model=model_7,\n    x=x_lin[:, None],\n    y=y,\n    log_noise_std=fixed_log_noise_std,\n)\n\nH = torch.func.hessian(partial_func)(last_layer_weights)\nprint(H.shape)\n\nH = H[0, :, 0, :]\nprint(H.shape)\n\ntorch.Size([1, 10, 1, 10])\ntorch.Size([10, 10])\n\n\n\nplt.imshow(H.cpu().numpy())\nplt.colorbar()\n\n&lt;matplotlib.colorbar.Colorbar at 0x7fae349d7be0&gt;\n\n\n\n\n\n\nimport seaborn as sns\nsns.heatmap(H.cpu().numpy())\nplt.gca().set_aspect(\"equal\")\n\n\n\n\n\ncov = torch.inverse(H)\nlaplace_posterior = torch.distributions.MultivariateNormal(\n    last_layer_weights.ravel(), cov\n)\nlast_layer_weights_samples = laplace_posterior.sample((101,))[..., None]\nlast_layer_weights_samples.shape\n\ntorch.Size([101, 10, 1])\n\n\n\nstate_dict[\"fc3.weight\"].shape, last_layer_weights_samples[0].shape, x_lin[\n    :, None\n].shape\n\n(torch.Size([1, 10]), torch.Size([10, 1]), torch.Size([100, 1]))\n\n\n\ndef forward_pass(last_layer_weight):\n    state_dict = model_7.state_dict()\n    state_dict[\"fc3.weight\"] = last_layer_weight.reshape(1, -1)\n    return torch.func.functional_call(model_7, state_dict, x_lin[:, None]).squeeze()\n\n\nforward_pass(last_layer_weights_samples[0]).shape\n\ntorch.Size([100])\n\n\n\nmc_outputs = torch.vmap(forward_pass)(last_layer_weights_samples)\nprint(mc_outputs.shape)\n\ntorch.Size([101, 100])\n\n\n\nmean_mc_outputs = mc_outputs.mean(0)\nstd_mc_outputs = mc_outputs.std(0)\nmean_mc_outputs.shape, std_mc_outputs.shape\n\n(torch.Size([100]), torch.Size([100]))\n\n\n\nwith torch.no_grad():\n    plt.scatter(x_lin.cpu(), y.cpu(), s=10, color=\"C0\", label=\"Data\")\n    plt.plot(x_lin.cpu(), f(x_lin.cpu()), color=\"C1\", label=\"True function\")\n    plt.plot(\n        x_lin.cpu(), mean_mc_outputs.cpu(), color=\"C2\", label=\"Laplace approximation\"\n    )\n    # Plot the +- 2 sigma region (where sigma is fixed to 0.2)\n    plt.fill_between(\n        x_lin.cpu(),\n        mean_mc_outputs.cpu() - 2 * std_mc_outputs.cpu(),\n        mean_mc_outputs.cpu() + 2 * std_mc_outputs.cpu(),\n        alpha=0.2,\n        color=\"C2\",\n    )\n    plt.xlabel(\"$x$\")\n    plt.ylabel(\"$y$\")\n    plt.legend()\n\n\n\n\n\n### Case 3: Both aleatoric and epistemic uncertainty"
  },
  {
    "objectID": "notebooks/bayes-librarian.html",
    "href": "notebooks/bayes-librarian.html",
    "title": "Bayes Librarian",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\n\np_l = 1.0/21.0\np_f = 1- p_l\nprior = torch.distributions.Categorical(probs=torch.tensor([p_l, p_f]))\nprior\n\nCategorical(probs: torch.Size([2]))\n\n\n\npd.Series(index=[\"Librarian\", \"Farmer\"], data=prior.probs).plot(kind='bar', rot=0)\n# Write the values on top of the bar\nfor i, v in enumerate(prior.probs):\n    plt.text(i - 0.1, v + 0.01, f\"{v.item():0.3f}\")\nplt.title(\"Prior\")\n\nText(0.5, 1.0, 'Prior')\n\n\n\n\n\n\n# Get 210 samples from prior given a seed\n\ntorch.manual_seed(3)\nsamples = prior.sample(torch.Size([210, ]))\nprint(samples)\nprint(samples.sum())\n\ntensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\ntensor(199)\n\n\n\np_d_l = torch.tensor(0.4)\np_d_f = torch.tensor(0.1)\n\n\np_l_d = (p_d_l * p_l) / (p_d_l * p_l + p_d_f * p_f)\np_l_d\n\ntensor(0.1667)\n\n\n\np_d_l *p_l\n\ntensor(0.0190)\n\n\n\np_d_f*p_f\n\ntensor(0.0952)\n\n\n\n## Electricity consumption\n\n\nprior_theta_0 = torch.distributions.Uniform(80, 100)\nprior_theta_1 = torch.distributions.Uniform(1, 2)\n\n\n# Plottting PDFs\n\neps = 1e-3\ntheta_0_lin = torch.linspace(80+eps, 100.0-eps, 100)\nplt.plot(theta_0_lin, torch.exp(prior_theta_0.log_prob(theta_0_lin)))\nplt.xlabel(r'$\\theta_0$')\nplt.ylabel(r'$p(\\theta_0)$')\nplt.title(r'Prior PDF for $\\theta_0$')\n# Fill between \nplt.fill_between(theta_0_lin, torch.exp(prior_theta_0.log_prob(theta_0_lin)), color='C0', alpha=0.2)\n\n&lt;matplotlib.collections.PolyCollection at 0x7f67945e25b0&gt;\n\n\n\n\n\n\ntheta_1_lin = torch.linspace(1+eps, 2.0-eps, 100)\nplt.plot(theta_1_lin, torch.exp(prior_theta_1.log_prob(theta_1_lin)))\nplt.xlabel(r'$\\theta_1$')\nplt.ylabel(r'$p(\\theta_1)$')\nplt.title(r'Prior PDF for $\\theta_1$')\n# Fill between \nplt.fill_between(theta_1_lin, torch.exp(prior_theta_1.log_prob(theta_1_lin)), color='C0', alpha=0.2)\n\n&lt;matplotlib.collections.PolyCollection at 0x7f67923c04c0&gt;\n\n\n\n\n\n\n# plotting samples from prior\n\nn_samples = 1000\ntheta_0s = prior_theta_0.sample(torch.Size([n_samples,]))\ntheta_1s = prior_theta_1.sample(torch.Size([n_samples,]))\n\n\n# Plot the samples\nfig, ax = plt.subplots(figsize=(8, 2), ncols=2)\nax[0].plot(theta_0_lin, torch.exp(prior_theta_0.log_prob(theta_0_lin)), label='prior')\n\n# plot theta_0 samples\nax[0].scatter(theta_0s, torch.zeros_like(theta_0s),  label='samples',c='r', marker='x')\n\nax[1].plot(theta_1_lin, torch.exp(prior_theta_1.log_prob(theta_1_lin)), label='prior')\nax[1].scatter(theta_1s, torch.zeros_like(theta_1s), label='samples',c='r', marker='x')\n\nax[1].set_title(r'$\\theta_1$')\nax[0].set_title(r'$\\theta_0$')\n\nText(0.5, 1.0, '$\\\\theta_0$')\n\n\n\n\n\n\n# Generating draws from prior\n\nxs = torch.linspace(1, 1000, 100)\ndef forward(x, theta_0, theta_1):\n    return theta_0 + theta_1 * x\n\npreds = []\n\nfor theta_0, theta_1 in zip(theta_0s, theta_1s):\n    print(theta_0.item(), theta_1.item())\n    preds.append(forward(xs, theta_0, theta_1))\n    plt.plot(xs, preds[-1], color='C1', alpha=0.5)\n\nplt.xlabel('Number of Occupants')\nplt.ylabel('Electricity Consumption')\n\n87.69513702392578 1.0412778854370117\n95.6269760131836 1.1212730407714844\n86.8476333618164 1.2125056982040405\n90.94928741455078 1.1671473979949951\n96.58927154541016 1.3385244607925415\n87.47602844238281 1.558532476425171\n88.01754760742188 1.1025707721710205\n90.33709716796875 1.9295198917388916\n88.80725860595703 1.9910452365875244\n98.52078247070312 1.986382007598877\n84.97046661376953 1.1545259952545166\n89.32050323486328 1.5447709560394287\n96.62725067138672 1.250910997390747\n81.65365600585938 1.5428019762039185\n89.25396728515625 1.5592520236968994\n85.2579574584961 1.0442684888839722\n94.31983947753906 1.6553218364715576\n89.50067138671875 1.9270992279052734\n95.697998046875 1.299673318862915\n98.55661010742188 1.488832950592041\n85.66251373291016 1.7670807838439941\n84.5622787475586 1.9460041522979736\n89.70915222167969 1.7484333515167236\n83.01439666748047 1.6918725967407227\n94.00970458984375 1.6238453388214111\n80.7804946899414 1.3233580589294434\n91.94137573242188 1.953773021697998\n97.56145477294922 1.169715166091919\n92.48450469970703 1.114927887916565\n83.50093078613281 1.102515459060669\n82.62401580810547 1.5394084453582764\n82.85623931884766 1.4271032810211182\n86.58822631835938 1.0481557846069336\n94.35069274902344 1.1144349575042725\n91.43531799316406 1.2319121360778809\n90.06417083740234 1.374377727508545\n99.1529541015625 1.220672369003296\n92.55977630615234 1.6680750846862793\n94.7859115600586 1.1695594787597656\n81.30776977539062 1.3169939517974854\n90.8858413696289 1.424668550491333\n94.4291000366211 1.144008755683899\n86.97049713134766 1.876807689666748\n98.18754577636719 1.5776394605636597\n92.92131042480469 1.9428175687789917\n80.43154907226562 1.4329195022583008\n85.76531219482422 1.875524878501892\n87.48646545410156 1.3303887844085693\n87.47976684570312 1.428504228591919\n86.83807373046875 1.197716474533081\n86.94332885742188 1.5565887689590454\n94.37505340576172 1.4149739742279053\n93.06510162353516 1.067138671875\n96.40441131591797 1.289334774017334\n86.10169982910156 1.5684490203857422\n90.42550659179688 1.2710241079330444\n91.0185775756836 1.0674898624420166\n93.92642211914062 1.3713047504425049\n86.58151245117188 1.1188983917236328\n85.77822875976562 1.5820157527923584\n92.26363372802734 1.3258450031280518\n99.87332916259766 1.6146454811096191\n84.66149139404297 1.1545571088790894\n82.93559265136719 1.6960713863372803\n89.84662628173828 1.5519955158233643\n96.96514892578125 1.7468791007995605\n95.19371795654297 1.976472020149231\n94.27191925048828 1.6630446910858154\n82.7536392211914 1.9746594429016113\n90.74134826660156 1.8186657428741455\n99.88572692871094 1.354374647140503\n93.4601821899414 1.410269856452942\n90.59257507324219 1.2586917877197266\n87.16841888427734 1.2511789798736572\n97.9290542602539 1.339185357093811\n95.5967788696289 1.7606523036956787\n92.15022277832031 1.3517167568206787\n84.46255493164062 1.1570004224777222\n90.3648681640625 1.0489882230758667\n86.80931091308594 1.6851460933685303\n96.78753662109375 1.3440935611724854\n93.14493560791016 1.7365984916687012\n98.11967468261719 1.602406620979309\n80.8687744140625 1.038487195968628\n94.12189483642578 1.223759651184082\n91.72600555419922 1.6831367015838623\n89.2578353881836 1.2158634662628174\n80.0279769897461 1.6264379024505615\n99.61504364013672 1.084007978439331\n83.5066909790039 1.5729857683181763\n80.53050994873047 1.8047398328781128\n89.59612274169922 1.953641653060913\n92.02345275878906 1.1013636589050293\n95.53692626953125 1.4922316074371338\n91.98695373535156 1.2603998184204102\n90.43048095703125 1.0833650827407837\n94.84186553955078 1.9851264953613281\n87.72840118408203 1.065014123916626\n81.46113586425781 1.6820820569992065\n97.96985626220703 1.234586238861084\n82.44818878173828 1.011096477508545\n96.11310577392578 1.4245643615722656\n87.82499694824219 1.6904051303863525\n84.83235931396484 1.1519567966461182\n88.76789093017578 1.13896644115448\n84.79768371582031 1.312787413597107\n80.25230407714844 1.1010459661483765\n95.03224182128906 1.7609448432922363\n81.25543212890625 1.04115629196167\n89.00857543945312 1.1022148132324219\n92.93892669677734 1.6987924575805664\n93.81294250488281 1.2739479541778564\n91.67818450927734 1.8880064487457275\n82.468994140625 1.276659607887268\n94.61796569824219 1.2439587116241455\n80.1130142211914 1.0376932621002197\n97.12281036376953 1.730310082435608\n92.46382141113281 1.7947566509246826\n90.61416625976562 1.8840017318725586\n82.46800231933594 1.7793362140655518\n96.13663482666016 1.5259487628936768\n96.89361572265625 1.305039405822754\n89.8781509399414 1.0746833086013794\n91.61681365966797 1.7047715187072754\n82.93636322021484 1.238344430923462\n82.81407928466797 1.0890676975250244\n96.61282348632812 1.5943901538848877\n99.01074981689453 1.4189800024032593\n81.95106506347656 1.5043325424194336\n96.00111389160156 1.1587986946105957\n93.47216796875 1.2459975481033325\n98.85737609863281 1.9700062274932861\n86.61084747314453 1.6007869243621826\n95.08634948730469 1.1321947574615479\n84.5699234008789 1.842087745666504\n90.51310729980469 1.68861985206604\n87.24632263183594 1.941713571548462\n82.32901000976562 1.665165901184082\n95.54249572753906 1.6798239946365356\n84.75167846679688 1.8800559043884277\n89.34307098388672 1.231102466583252\n86.72317504882812 1.9233851432800293\n89.29837036132812 1.8016605377197266\n88.3612060546875 1.37612783908844\n98.79486083984375 1.523589849472046\n98.48333740234375 1.2476298809051514\n97.07954406738281 1.0195120573043823\n89.96569061279297 1.0555812120437622\n80.99148559570312 1.376490592956543\n93.362060546875 1.7271687984466553\n81.7196044921875 1.5611259937286377\n90.10861206054688 1.8441978693008423\n89.50457000732422 1.5818358659744263\n84.30509948730469 1.2079615592956543\n83.50444793701172 1.7497899532318115\n91.68502044677734 1.3454217910766602\n80.13153839111328 1.0732593536376953\n99.80758666992188 1.6212793588638306\n91.77196502685547 1.2740830183029175\n94.59294128417969 1.097072720527649\n88.84345245361328 1.4506481885910034\n86.78339385986328 1.0639290809631348\n88.83187866210938 1.8970956802368164\n94.25908660888672 1.587091088294983\n88.65489196777344 1.1026930809020996\n82.4282455444336 1.9630863666534424\n97.47400665283203 1.292677402496338\n96.66935729980469 1.2211899757385254\n81.74362182617188 1.0532464981079102\n89.22825622558594 1.754408597946167\n82.65867614746094 1.0193803310394287\n88.86253356933594 1.1457579135894775\n80.92573547363281 1.0950336456298828\n94.04350280761719 1.990125060081482\n96.60394287109375 1.987403154373169\n97.53553771972656 1.4419186115264893\n89.57406616210938 1.6335155963897705\n91.17781066894531 1.4893226623535156\n86.52110290527344 1.6093614101409912\n86.1540756225586 1.9536876678466797\n87.56365203857422 1.566344976425171\n90.33023071289062 1.8433496952056885\n82.40996551513672 1.0289032459259033\n85.71919250488281 1.919156551361084\n99.86544036865234 1.782569408416748\n94.81980895996094 1.4061675071716309\n82.9118881225586 1.4045815467834473\n99.01624298095703 1.5185314416885376\n97.09477233886719 1.645193338394165\n89.12952423095703 1.5449402332305908\n88.90231323242188 1.1528569459915161\n87.5825424194336 1.441185712814331\n87.4133529663086 1.800107479095459\n92.37303161621094 1.5086145401000977\n99.30390167236328 1.9642198085784912\n99.74899291992188 1.4058808088302612\n95.60200500488281 1.5114103555679321\n90.23295593261719 1.444563627243042\n92.5718002319336 1.5712342262268066\n81.01299285888672 1.8754684925079346\n89.2287368774414 1.7894538640975952\n92.5677719116211 1.8815683126449585\n94.48453521728516 1.0842392444610596\n93.99372100830078 1.7238528728485107\n91.33424377441406 1.1637542247772217\n88.50811767578125 1.3193455934524536\n85.53105926513672 1.6868886947631836\n87.34080505371094 1.8577947616577148\n92.7117919921875 1.5039350986480713\n89.24696350097656 1.3948745727539062\n81.40795135498047 1.086325764656067\n96.5345458984375 1.7923710346221924\n98.70243835449219 1.9552621841430664\n92.06018829345703 1.6399625539779663\n87.05399322509766 1.1945961713790894\n81.01774597167969 1.5529268980026245\n98.87288665771484 1.5917402505874634\n89.5815658569336 1.7662642002105713\n82.79290008544922 1.4998633861541748\n80.24394989013672 1.1864984035491943\n95.508056640625 1.1963965892791748\n86.65911865234375 1.5199213027954102\n84.08351135253906 1.8646318912506104\n92.0901870727539 1.1967954635620117\n88.18511199951172 1.326673150062561\n94.06657409667969 1.9710323810577393\n85.5030288696289 1.3380656242370605\n90.45893096923828 1.2146241664886475\n81.58501434326172 1.865027666091919\n96.42964935302734 1.1058900356292725\n98.82797241210938 1.1475470066070557\n98.73994445800781 1.4258317947387695\n94.16300201416016 1.33109450340271\n98.30198669433594 1.634709358215332\n93.94081115722656 1.828623652458191\n86.51219177246094 1.265526294708252\n96.52320861816406 1.8763457536697388\n98.6265869140625 1.1988688707351685\n91.43827819824219 1.8440756797790527\n93.32282257080078 1.0980292558670044\n81.95563507080078 1.354668140411377\n96.27238464355469 1.215299129486084\n84.34428405761719 1.8884146213531494\n81.55213165283203 1.5654256343841553\n89.6497802734375 1.356525182723999\n99.29093933105469 1.6838717460632324\n95.04837799072266 1.3560669422149658\n81.63878631591797 1.2418384552001953\n89.12718963623047 1.3357279300689697\n87.46515655517578 1.4678325653076172\n92.83613586425781 1.7892435789108276\n91.69847106933594 1.2009649276733398\n98.4359130859375 1.905756950378418\n98.43559265136719 1.0245305299758911\n87.91726684570312 1.7803137302398682\n93.9311752319336 1.4504296779632568\n97.53208923339844 1.7488648891448975\n85.20597839355469 1.3056707382202148\n99.80611419677734 1.404740333557129\n88.9251480102539 1.0782647132873535\n85.16576385498047 1.3475453853607178\n85.73490905761719 1.4886318445205688\n91.07793426513672 1.8859808444976807\n81.94158935546875 1.5911576747894287\n87.99101257324219 1.6303831338882446\n98.05056762695312 1.0774091482162476\n95.1716079711914 1.0707284212112427\n96.35324096679688 1.3814113140106201\n85.59410095214844 1.7233291864395142\n98.8589859008789 1.6704661846160889\n82.66046905517578 1.0010724067687988\n82.14085388183594 1.3173435926437378\n96.20088195800781 1.5716652870178223\n97.9405746459961 1.7548600435256958\n90.15998840332031 1.0804041624069214\n94.58312225341797 1.360841155052185\n95.50540161132812 1.0990573167800903\n92.21611022949219 1.5698652267456055\n99.58377075195312 1.6420044898986816\n85.63987731933594 1.9295915365219116\n96.26022338867188 1.0003501176834106\n80.15142822265625 1.0048238039016724\n89.23384094238281 1.9587490558624268\n82.99113464355469 1.1053338050842285\n97.3426284790039 1.5909531116485596\n98.71217346191406 1.1090104579925537\n87.20555114746094 1.298750400543213\n89.35206604003906 1.1972639560699463\n83.49285888671875 1.1133019924163818\n85.85491180419922 1.869433879852295\n88.74373626708984 1.1152124404907227\n84.66169738769531 1.032759189605713\n93.00992584228516 1.9837510585784912\n86.14134979248047 1.6887331008911133\n98.59461975097656 1.080071210861206\n94.73643493652344 1.6711037158966064\n87.08386993408203 1.4622790813446045\n80.96151733398438 1.0804364681243896\n93.2742691040039 1.9616378545761108\n97.86018371582031 1.6221661567687988\n90.74947357177734 1.4679657220840454\n89.17950439453125 1.6991548538208008\n82.93622589111328 1.1061826944351196\n83.94174194335938 1.1872522830963135\n81.08946990966797 1.8086113929748535\n87.80683898925781 1.305240511894226\n85.30784606933594 1.4780648946762085\n80.12765502929688 1.897214412689209\n92.23592376708984 1.33310866355896\n80.84759521484375 1.6072919368743896\n91.5848159790039 1.220404863357544\n96.11167907714844 1.2652465105056763\n92.53817749023438 1.931967854499817\n96.32613372802734 1.3347055912017822\n80.47168731689453 1.7610195875167847\n89.83350372314453 1.9038317203521729\n82.05730438232422 1.3888483047485352\n97.04126739501953 1.524307370185852\n99.83863830566406 1.1387238502502441\n96.62197875976562 1.5320913791656494\n86.57617950439453 1.496248722076416\n90.88282775878906 1.7337374687194824\n84.61872863769531 1.6061371564865112\n80.97779083251953 1.2528002262115479\n80.44483184814453 1.516909122467041\n95.21382904052734 1.9045822620391846\n80.2861328125 1.79441237449646\n94.36021423339844 1.8566420078277588\n92.70948791503906 1.445806622505188\n89.04227447509766 1.3563913106918335\n80.06767272949219 1.36397123336792\n82.87981414794922 1.0417895317077637\n85.39154052734375 1.8766453266143799\n97.44548034667969 1.7993073463439941\n90.1553726196289 1.5245330333709717\n83.11884307861328 1.346864104270935\n88.65609741210938 1.6343283653259277\n81.6729507446289 1.8613231182098389\n83.9632797241211 1.8341238498687744\n88.94541931152344 1.6620174646377563\n96.30646514892578 1.333033800125122\n95.3998031616211 1.1219444274902344\n92.66915893554688 1.8882604837417603\n89.97352600097656 1.9781239032745361\n82.77247619628906 1.8393640518188477\n98.02163696289062 1.6135040521621704\n80.08682250976562 1.1931250095367432\n82.48628234863281 1.7100921869277954\n89.97698211669922 1.8217391967773438\n88.32479095458984 1.6070287227630615\n97.6349868774414 1.8012388944625854\n93.0042495727539 1.4907748699188232\n98.78262329101562 1.9023869037628174\n89.17003631591797 1.1349231004714966\n88.15408325195312 1.9487724304199219\n84.39936828613281 1.0591232776641846\n89.317138671875 1.2374107837677002\n88.28820037841797 1.4968205690383911\n85.02552032470703 1.890254259109497\n84.24485778808594 1.065805435180664\n82.170166015625 1.8334324359893799\n86.24152374267578 1.5876519680023193\n97.01067352294922 1.1534284353256226\n81.58153533935547 1.861690640449524\n83.04762268066406 1.3336613178253174\n93.97317504882812 1.5183649063110352\n90.00640869140625 1.5225598812103271\n90.9233169555664 1.1471093893051147\n97.70230865478516 1.851118564605713\n93.2962875366211 1.4258244037628174\n99.33335876464844 1.4624136686325073\n86.70885467529297 1.0074397325515747\n88.84932708740234 1.8021918535232544\n80.50030517578125 1.0101171731948853\n82.77212524414062 1.1637043952941895\n90.78730010986328 1.0748193264007568\n94.13116455078125 1.8970420360565186\n95.94595336914062 1.6449190378189087\n91.89263916015625 1.7218899726867676\n85.97459411621094 1.4877347946166992\n98.50749206542969 1.1829235553741455\n88.4510498046875 1.6825315952301025\n85.79403686523438 1.259835124015808\n88.90570068359375 1.6126017570495605\n93.93443298339844 1.398136854171753\n81.72805786132812 1.9015271663665771\n96.02115631103516 1.0172184705734253\n98.46682739257812 1.3596460819244385\n84.52902221679688 1.6072989702224731\n88.45693969726562 1.2749097347259521\n94.19126892089844 1.0986212491989136\n99.05484008789062 1.971077799797058\n94.07135772705078 1.2463831901550293\n80.08610534667969 1.6148242950439453\n95.32771301269531 1.4674646854400635\n84.63024139404297 1.4844646453857422\n80.96675109863281 1.1639809608459473\n81.53005981445312 1.3308959007263184\n96.05962371826172 1.8497109413146973\n93.70967864990234 1.4482638835906982\n92.92405700683594 1.2706226110458374\n92.90196990966797 1.797845721244812\n83.52706909179688 1.4338971376419067\n94.42025756835938 1.4818401336669922\n95.91780090332031 1.9951822757720947\n97.80067443847656 1.0893971920013428\n89.46163940429688 1.5250924825668335\n92.21309661865234 1.4673281908035278\n81.63961029052734 1.460693120956421\n96.56671142578125 1.8651611804962158\n81.10565948486328 1.668044090270996\n81.5894775390625 1.271575689315796\n88.22618103027344 1.6871442794799805\n98.44947052001953 1.4937515258789062\n90.52273559570312 1.1085081100463867\n80.44692993164062 1.5279107093811035\n87.55543518066406 1.324415683746338\n90.79936218261719 1.1178081035614014\n81.06486511230469 1.736134648323059\n98.48744201660156 1.4704158306121826\n96.25715637207031 1.3769464492797852\n90.72998809814453 1.8688011169433594\n89.17372131347656 1.819145917892456\n82.09491729736328 1.1790657043457031\n85.36598205566406 1.8264503479003906\n95.80296325683594 1.4930779933929443\n95.19683074951172 1.1025617122650146\n91.56018829345703 1.8875255584716797\n95.2552490234375 1.632291555404663\n91.86444091796875 1.569888710975647\n93.09357452392578 1.1392353773117065\n80.02865600585938 1.7590293884277344\n89.14838409423828 1.4920017719268799\n90.33744812011719 1.127612590789795\n97.77242279052734 1.306746006011963\n95.96150207519531 1.6678380966186523\n90.35309600830078 1.7874670028686523\n86.41181945800781 1.7617707252502441\n85.44058227539062 1.6017524003982544\n99.6016616821289 1.7393227815628052\n96.60693359375 1.774422526359558\n99.51984405517578 1.2080415487289429\n94.75322723388672 1.5050280094146729\n97.84530639648438 1.422541856765747\n96.12996673583984 1.2059016227722168\n96.96800231933594 1.0802865028381348\n99.29610443115234 1.3785951137542725\n99.7637939453125 1.155839443206787\n81.2962646484375 1.4671499729156494\n82.70000457763672 1.8565661907196045\n86.53301239013672 1.3617717027664185\n80.9884033203125 1.0914902687072754\n84.16556549072266 1.1358442306518555\n98.1244125366211 1.3115699291229248\n86.32878875732422 1.7048910856246948\n90.72171020507812 1.3039581775665283\n99.68870544433594 1.5448617935180664\n91.55982208251953 1.6279070377349854\n95.13211822509766 1.8350934982299805\n92.51482391357422 1.5801448822021484\n98.24629974365234 1.1688917875289917\n83.68853759765625 1.5509145259857178\n83.41927337646484 1.1064735651016235\n95.55207824707031 1.4728004932403564\n82.78467559814453 1.4583171606063843\n93.66595458984375 1.4533686637878418\n98.6113052368164 1.9356716871261597\n83.80675506591797 1.8612539768218994\n80.73794555664062 1.8096702098846436\n97.62300109863281 1.3299038410186768\n92.72089385986328 1.5748240947723389\n85.024658203125 1.7261515855789185\n93.6810531616211 1.8122947216033936\n80.28606414794922 1.7277923822402954\n82.3918685913086 1.8946515321731567\n98.45598602294922 1.746030569076538\n86.21611022949219 1.358589768409729\n94.80453491210938 1.5360863208770752\n92.11781311035156 1.71842360496521\n86.97250366210938 1.4501370191574097\n90.20829772949219 1.9510061740875244\n95.7842025756836 1.5418438911437988\n91.82008361816406 1.6570210456848145\n87.27477264404297 1.7911310195922852\n81.82063293457031 1.5614025592803955\n92.24595642089844 1.3466110229492188\n99.67198181152344 1.0122146606445312\n86.72737121582031 1.06300950050354\n87.62284851074219 1.8864786624908447\n93.38764953613281 1.3682794570922852\n81.69622039794922 1.9802289009094238\n90.4000473022461 1.8631947040557861\n98.49722290039062 1.4564552307128906\n88.75487518310547 1.655290961265564\n87.91423797607422 1.1321805715560913\n87.37792205810547 1.6390581130981445\n97.37086486816406 1.4559935331344604\n90.47662353515625 1.4960585832595825\n99.66328430175781 1.7768841981887817\n84.1020736694336 1.6165105104446411\n99.58877563476562 1.5559637546539307\n89.61442565917969 1.0251836776733398\n93.586181640625 1.507324457168579\n98.76620483398438 1.0353035926818848\n81.74576568603516 1.7205651998519897\n98.36883544921875 1.42416512966156\n95.49655151367188 1.8263685703277588\n98.14613342285156 1.0789601802825928\n98.68721008300781 1.3878099918365479\n99.5651626586914 1.1787943840026855\n87.18000030517578 1.6864721775054932\n86.595703125 1.9095823764801025\n95.84112548828125 1.06962251663208\n84.1778793334961 1.7237639427185059\n83.77605438232422 1.8962490558624268\n94.51152038574219 1.7772753238677979\n90.28147888183594 1.5205633640289307\n90.34710693359375 1.8482685089111328\n81.21249389648438 1.7342454195022583\n84.42314910888672 1.6690165996551514\n98.13052368164062 1.8424246311187744\n96.65554809570312 1.8497288227081299\n93.6399917602539 1.477353572845459\n83.35796356201172 1.0414211750030518\n91.53684997558594 1.6548962593078613\n96.76288604736328 1.1879498958587646\n89.65204620361328 1.6149208545684814\n80.90271759033203 1.8829424381256104\n85.37530517578125 1.702674150466919\n80.3994140625 1.4674813747406006\n84.2823486328125 1.4763164520263672\n96.10466766357422 1.0428643226623535\n91.8232650756836 1.146437406539917\n87.81658935546875 1.8529260158538818\n94.59446716308594 1.7889952659606934\n88.49081420898438 1.5795292854309082\n97.00790405273438 1.4620342254638672\n91.68185424804688 1.3564547300338745\n97.39276885986328 1.0309299230575562\n80.75239562988281 1.302687644958496\n87.96551513671875 1.2410178184509277\n91.6697006225586 1.8507330417633057\n95.40574645996094 1.3018969297409058\n96.36846160888672 1.2443230152130127\n80.5966567993164 1.650611400604248\n88.48704528808594 1.0398454666137695\n99.28273010253906 1.5687675476074219\n89.9427490234375 1.6616110801696777\n83.8802490234375 1.0650686025619507\n80.98805236816406 1.3144359588623047\n90.46640014648438 1.4533863067626953\n87.94342041015625 1.4379196166992188\n96.86217498779297 1.722522497177124\n97.1824951171875 1.7648491859436035\n94.94293975830078 1.7476022243499756\n88.57774353027344 1.9936397075653076\n81.87714385986328 1.3116981983184814\n84.58016967773438 1.3853662014007568\n90.22624969482422 1.8877133131027222\n93.26171875 1.3057349920272827\n85.26964569091797 1.7732207775115967\n88.49036407470703 1.4678995609283447\n83.45116424560547 1.398559808731079\n81.73490142822266 1.3599162101745605\n80.60186004638672 1.607072114944458\n92.12307739257812 1.3804558515548706\n82.58917236328125 1.9838064908981323\n96.34735107421875 1.0393333435058594\n92.86759948730469 1.4368032217025757\n84.02291107177734 1.7780158519744873\n80.03865814208984 1.6370843648910522\n85.43888854980469 1.7002469301223755\n95.39344787597656 1.953841209411621\n82.55075073242188 1.362874984741211\n89.55502319335938 1.2146159410476685\n91.37075805664062 1.6585370302200317\n93.4520034790039 1.9682751893997192\n89.2584228515625 1.5960586071014404\n96.02014923095703 1.2836705446243286\n87.00834655761719 1.1300926208496094\n91.74502563476562 1.676926612854004\n85.42420959472656 1.4379678964614868\n91.68942260742188 1.5796723365783691\n92.3317642211914 1.992002010345459\n93.67393493652344 1.5307087898254395\n82.57008361816406 1.2691798210144043\n90.5518569946289 1.7374107837677002\n96.5008316040039 1.7301230430603027\n95.30280303955078 1.5667967796325684\n96.31549072265625 1.2386984825134277\n86.32937622070312 1.1683495044708252\n93.25740814208984 1.2452566623687744\n88.98576354980469 1.1448354721069336\n91.8854751586914 1.7744994163513184\n85.98224639892578 1.1230238676071167\n93.2282485961914 1.1721405982971191\n95.08592987060547 1.9898439645767212\n87.02610778808594 1.5295970439910889\n87.21879577636719 1.9920413494110107\n89.9481430053711 1.9008828401565552\n93.08697509765625 1.6155953407287598\n88.41838073730469 1.7965482473373413\n89.20204162597656 1.735062599182129\n91.37265014648438 1.138883113861084\n94.8079605102539 1.4398438930511475\n99.36428833007812 1.8113322257995605\n88.27420043945312 1.9618552923202515\n83.03826904296875 1.1955100297927856\n96.51607513427734 1.683363676071167\n83.83062744140625 1.3281757831573486\n96.16497802734375 1.5947082042694092\n90.38520050048828 1.2728993892669678\n88.38111877441406 1.6290942430496216\n80.53921508789062 1.7611379623413086\n81.95087432861328 1.1960370540618896\n81.45306396484375 1.673715591430664\n91.85885620117188 1.1945915222167969\n81.97167205810547 1.0205057859420776\n81.46564483642578 1.524099588394165\n81.84530639648438 1.8652347326278687\n91.78020477294922 1.750521183013916\n92.25891876220703 1.0031507015228271\n86.03592681884766 1.903341293334961\n84.34727478027344 1.448254108428955\n91.22293853759766 1.0013437271118164\n85.41646575927734 1.6531617641448975\n88.41971588134766 1.967858910560608\n97.15705108642578 1.1303396224975586\n88.0723648071289 1.7646214962005615\n95.79047393798828 1.6941215991973877\n84.21778106689453 1.778570532798767\n84.01835632324219 1.653878927230835\n93.9107437133789 1.1549937725067139\n84.52613830566406 1.7052514553070068\n84.23381042480469 1.253612995147705\n98.05361938476562 1.1652004718780518\n87.08734130859375 1.5861719846725464\n84.90568542480469 1.189119577407837\n92.3504638671875 1.9187812805175781\n87.37548828125 1.3960113525390625\n84.04778289794922 1.3164339065551758\n92.84234619140625 1.783273696899414\n90.77714538574219 1.665143370628357\n97.57646942138672 1.3605051040649414\n81.65654754638672 1.3727788925170898\n81.46382141113281 1.6269168853759766\n92.97415161132812 1.4338579177856445\n92.45384216308594 1.4932732582092285\n98.71630859375 1.9220644235610962\n99.18801879882812 1.5020290613174438\n94.50535583496094 1.6706345081329346\n93.87782287597656 1.8368029594421387\n80.73177337646484 1.9445865154266357\n90.4026870727539 1.7702679634094238\n96.26884460449219 1.0647261142730713\n96.21143341064453 1.7131527662277222\n83.15489196777344 1.9959337711334229\n92.94298553466797 1.4255867004394531\n86.27571868896484 1.2139116525650024\n89.59510803222656 1.83095383644104\n98.43739318847656 1.6682121753692627\n81.6409912109375 1.2444205284118652\n90.85409545898438 1.7423512935638428\n93.0773696899414 1.0354089736938477\n84.70011138916016 1.230469822883606\n85.11830139160156 1.4981557130813599\n87.32134246826172 1.5977025032043457\n90.03490447998047 1.7946093082427979\n90.2147216796875 1.104181170463562\n89.17416381835938 1.6704487800598145\n83.1048355102539 1.7645598649978638\n82.71820831298828 1.4501047134399414\n94.20553588867188 1.3868703842163086\n99.91905212402344 1.7349451780319214\n94.77164459228516 1.1228876113891602\n81.1861343383789 1.5759849548339844\n90.24392700195312 1.784378170967102\n88.24333190917969 1.5038079023361206\n92.0295181274414 1.355574607849121\n92.56514739990234 1.2769356966018677\n85.94490814208984 1.9950015544891357\n86.05892181396484 1.0340492725372314\n91.34903717041016 1.6086660623550415\n81.53462982177734 1.8889412879943848\n82.08914184570312 1.0108835697174072\n82.99040985107422 1.5281376838684082\n94.89295959472656 1.6591260433197021\n88.80055236816406 1.74747896194458\n86.06137084960938 1.4740259647369385\n86.14717102050781 1.9505882263183594\n82.40860748291016 1.3710678815841675\n94.55815124511719 1.6016926765441895\n83.80721282958984 1.3749752044677734\n80.2197265625 1.3133662939071655\n82.26643371582031 1.3465780019760132\n84.53607940673828 1.9912424087524414\n83.35774230957031 1.839600920677185\n84.59407043457031 1.4957480430603027\n93.56404113769531 1.4851701259613037\n92.06135559082031 1.1837477684020996\n81.80078887939453 1.003249168395996\n86.82144927978516 1.3300600051879883\n88.28636932373047 1.1090788841247559\n91.64513397216797 1.5997440814971924\n99.58380889892578 1.9814856052398682\n80.24290466308594 1.8041205406188965\n98.6710205078125 1.2375580072402954\n84.29248046875 1.5288810729980469\n80.7286605834961 1.0446298122406006\n97.26927947998047 1.9321799278259277\n82.84217834472656 1.2857847213745117\n91.19561004638672 1.8906525373458862\n89.48432159423828 1.4106606245040894\n81.46939086914062 1.2941572666168213\n83.37738800048828 1.130700707435608\n88.78155517578125 1.6292293071746826\n85.92105102539062 1.5038135051727295\n98.54142761230469 1.201120138168335\n86.03743743896484 1.6141196489334106\n96.57463073730469 1.8828784227371216\n82.45696258544922 1.755530595779419\n94.83319091796875 1.4144964218139648\n87.38792419433594 1.395566701889038\n86.35507202148438 1.953106164932251\n94.02960968017578 1.4930729866027832\n98.34146881103516 1.3978049755096436\n94.55321502685547 1.219794511795044\n86.0677490234375 1.9958608150482178\n93.33192443847656 1.391430377960205\n84.49732971191406 1.7394044399261475\n99.59793090820312 1.265183448791504\n82.78313446044922 1.2995491027832031\n82.94720458984375 1.5027931928634644\n90.03015899658203 1.6642940044403076\n85.33958435058594 1.2336509227752686\n82.81128692626953 1.5233395099639893\n83.21725463867188 1.9979586601257324\n94.02526092529297 1.9433088302612305\n96.185546875 1.39166259765625\n94.62015533447266 1.422394037246704\n85.43770599365234 1.3459270000457764\n87.85009002685547 1.6309995651245117\n90.31221008300781 1.1308749914169312\n85.93450927734375 1.2944839000701904\n95.16260528564453 1.6092753410339355\n88.80064392089844 1.8153597116470337\n94.45771026611328 1.3654019832611084\n91.87638092041016 1.607332468032837\n92.98275756835938 1.7597867250442505\n82.830810546875 1.2815625667572021\n99.4336929321289 1.0294337272644043\n96.1424331665039 1.63151216506958\n91.74088287353516 1.8203094005584717\n94.86433410644531 1.6463167667388916\n85.03170776367188 1.8908147811889648\n81.66612243652344 1.9308836460113525\n97.11016845703125 1.944331407546997\n93.381103515625 1.7135376930236816\n88.73222351074219 1.8859648704528809\n94.90327453613281 1.683983325958252\n80.44949340820312 1.901411533355713\n97.26458740234375 1.1535134315490723\n88.45130920410156 1.220460057258606\n96.82727813720703 1.510488510131836\n84.75334167480469 1.760849952697754\n98.310791015625 1.583519697189331\n97.90435791015625 1.668187141418457\n89.99076843261719 1.2872053384780884\n94.72821044921875 1.4270225763320923\n93.0601577758789 1.5296818017959595\n94.30963897705078 1.1160366535186768\n93.94793701171875 1.6426379680633545\n86.19263458251953 1.08041512966156\n80.5118408203125 1.8654968738555908\n84.01466369628906 1.3280844688415527\n96.20509338378906 1.6256213188171387\n93.00784301757812 1.2826988697052002\n90.70428466796875 1.8038830757141113\n85.2476806640625 1.7395219802856445\n82.3729476928711 1.6036834716796875\n89.78605651855469 1.6776859760284424\n80.54756927490234 1.8309955596923828\n99.66976928710938 1.2932779788970947\n99.87591552734375 1.1250203847885132\n98.35208129882812 1.9846861362457275\n92.65155029296875 1.107607126235962\n89.141357421875 1.3425897359848022\n86.205078125 1.5620756149291992\n98.31776428222656 1.5817723274230957\n83.6128921508789 1.104830265045166\n82.37863159179688 1.130692720413208\n84.2057113647461 1.9208526611328125\n84.14820861816406 1.1117706298828125\n95.58302307128906 1.798877477645874\n99.08193969726562 1.109937310218811\n96.45347595214844 1.0028170347213745\n90.14459991455078 1.5874903202056885\n95.96649169921875 1.7088102102279663\n91.61677551269531 1.8787145614624023\n90.53985595703125 1.1856184005737305\n86.97960662841797 1.5280441045761108\n80.56758117675781 1.1526362895965576\n85.12653350830078 1.779416561126709\n80.04230499267578 1.4652273654937744\n91.02215576171875 1.9656496047973633\n88.33577728271484 1.4623045921325684\n98.91292572021484 1.866753101348877\n94.00769805908203 1.5757315158843994\n92.6316909790039 1.8505401611328125\n84.2348403930664 1.1799921989440918\n95.00141906738281 1.9301543235778809\n90.13406372070312 1.0038076639175415\n88.1715316772461 1.2157790660858154\n93.29283142089844 1.9890989065170288\n89.0271224975586 1.7377375364303589\n91.8408203125 1.7466797828674316\n99.33059692382812 1.4390294551849365\n98.48324584960938 1.758401870727539\n83.00625610351562 1.3527941703796387\n98.83973693847656 1.079725742340088\n92.71626281738281 1.8784775733947754\n99.38002014160156 1.965032696723938\n95.19747161865234 1.8720885515213013\n80.86019897460938 1.009787917137146\n87.3360366821289 1.2022541761398315\n81.04074096679688 1.8308234214782715\n98.82062530517578 1.1745812892913818\n90.1404800415039 1.9698154926300049\n80.95049285888672 1.7338175773620605\n91.59555053710938 1.0537080764770508\n92.85134887695312 1.5899709463119507\n89.0922622680664 1.7719515562057495\n87.76759338378906 1.7769479751586914\n94.38178253173828 1.0224690437316895\n98.71640014648438 1.6177469491958618\n89.71712493896484 1.4378740787506104\n83.92728424072266 1.8954205513000488\n83.64366912841797 1.7923521995544434\n98.96711730957031 1.6632966995239258\n96.44139862060547 1.7354342937469482\n81.7240219116211 1.1570839881896973\n86.23362731933594 1.757424235343933\n86.37788391113281 1.4433300495147705\n80.74813842773438 1.4932761192321777\n97.84660339355469 1.9760985374450684\n93.23468780517578 1.0188976526260376\n84.22872924804688 1.3363087177276611\n96.82887268066406 1.221339225769043\n81.30179595947266 1.9991141557693481\n84.09915161132812 1.252927303314209\n90.10759735107422 1.9463887214660645\n93.94319915771484 1.526930809020996\n94.18348693847656 1.9327185153961182\n84.27458190917969 1.7738151550292969\n93.13091278076172 1.7500479221343994\n80.47447967529297 1.9000530242919922\n98.79817199707031 1.4553866386413574\n86.2924575805664 1.3707048892974854\n88.50386047363281 1.667838215827942\n88.3800048828125 1.0530340671539307\n91.60956573486328 1.302704095840454\n86.78005981445312 1.4603569507598877\n90.30412292480469 1.517063856124878\n84.20452880859375 1.890026330947876\n89.8901596069336 1.0054442882537842\n93.95210266113281 1.5633063316345215\n99.6352310180664 1.0056164264678955\n98.12481689453125 1.6828978061676025\n87.24083709716797 1.657820224761963\n88.11785888671875 1.1198830604553223\n94.37049865722656 1.6802012920379639\n82.21324157714844 1.0426301956176758\n90.2756576538086 1.7751855850219727\n97.32678985595703 1.5809354782104492\n80.03449249267578 1.877395510673523\n90.29203796386719 1.1785944700241089\n94.00375366210938 1.9306368827819824\n87.07160186767578 1.0483334064483643\n93.4297866821289 1.231441855430603\n93.20204162597656 1.9675328731536865\n87.07601165771484 1.1881678104400635\n82.18881225585938 1.1132152080535889\n95.82549285888672 1.9563894271850586\n89.05828857421875 1.929946780204773\n80.41268157958984 1.9739079475402832\n88.15396118164062 1.0157376527786255\n94.43635559082031 1.439072847366333\n80.95018005371094 1.2114360332489014\n97.67599487304688 1.9241340160369873\n97.97877502441406 1.38897705078125\n82.70357513427734 1.4984066486358643\n80.9278335571289 1.8461503982543945\n82.1534194946289 1.3695964813232422\n96.12203979492188 1.37688410282135\n84.20735168457031 1.0404396057128906\n89.20557403564453 1.8401185274124146\n85.48104858398438 1.667121410369873\n96.72209167480469 1.595235824584961\n83.95387268066406 1.9338353872299194\n89.39398956298828 1.9475802183151245\n87.39876556396484 1.826859474182129\n84.99310302734375 1.478555679321289\n95.20046997070312 1.8166275024414062\n84.63009643554688 1.2446922063827515\n90.1427001953125 1.2517023086547852\n92.98526763916016 1.2036794424057007\n94.19744873046875 1.0349105596542358\n95.95453643798828 1.812288522720337\n82.15237426757812 1.9743807315826416\n92.40218353271484 1.7967054843902588\n89.28179931640625 1.2706964015960693\n94.41395568847656 1.0045077800750732\n93.30809020996094 1.5501983165740967\n93.10982513427734 1.1014915704727173\n85.00166320800781 1.9952356815338135\n94.99205780029297 1.593123197555542\n92.78434753417969 1.5873770713806152\n85.2752685546875 1.8876330852508545\n96.14407348632812 1.6973540782928467\n96.59182739257812 1.777432918548584\n85.21195983886719 1.247289776802063\n89.11259460449219 1.9683427810668945\n84.28923797607422 1.442199945449829\n95.35619354248047 1.0686843395233154\n81.9161376953125 1.6271142959594727\n98.88188171386719 1.9269213676452637\n94.53003692626953 1.381432294845581\n80.15739440917969 1.875699758529663\n99.72911071777344 1.9995348453521729\n88.91797637939453 1.2367712259292603\n94.02236938476562 1.9013264179229736\n92.25674438476562 1.1762142181396484\n87.81084442138672 1.028839349746704\n93.73471069335938 1.3626747131347656\n85.55268096923828 1.481982707977295\n88.40141296386719 1.7987523078918457\n81.53286743164062 1.2459611892700195\n95.06607055664062 1.9266705513000488\n98.91923522949219 1.6137280464172363\n86.81133270263672 1.1890655755996704\n85.51641082763672 1.8769354820251465\n95.20496368408203 1.021319031715393\n91.9203872680664 1.3852016925811768\n90.42877197265625 1.4692349433898926\n93.15274810791016 1.7049617767333984\n92.43025207519531 1.815345048904419\n90.53237915039062 1.2616076469421387\n97.911376953125 1.6995875835418701\n99.65914916992188 1.1845998764038086\n91.41571807861328 1.7571431398391724\n86.17327117919922 1.4590321779251099\n82.06706237792969 1.583986520767212\n81.95830535888672 1.8748775720596313\n89.40499114990234 1.8530055284500122\n87.00450134277344 1.7186863422393799\n90.86737060546875 1.1591532230377197\n87.36505889892578 1.1975839138031006\n86.66249084472656 1.802229881286621\n91.9198989868164 1.3777878284454346\n80.39051818847656 1.9387800693511963\n87.5408935546875 1.2266218662261963\n92.21771240234375 1.302396297454834\n89.4444580078125 1.4973241090774536\n82.5759048461914 1.4075777530670166\n93.01850128173828 1.1784536838531494\n87.36629486083984 1.3914499282836914\n93.0128173828125 1.9193010330200195\n99.21200561523438 1.8135334253311157\n82.42012023925781 1.9386579990386963\n91.3363037109375 1.080588698387146\n84.02668762207031 1.849124550819397\n85.27171325683594 1.414588451385498\n84.32006072998047 1.6265616416931152\n90.93821716308594 1.0528053045272827\n81.02252960205078 1.7647711038589478\n83.12752532958984 1.9332249164581299\n80.77437591552734 1.6287462711334229\n81.12611389160156 1.7884021997451782\n96.69781494140625 1.6648938655853271\n82.15121459960938 1.7358323335647583\n85.9214859008789 1.6570940017700195\n84.7430648803711 1.4608850479125977\n95.41434478759766 1.7912076711654663\n82.72222900390625 1.8343868255615234\n87.48568725585938 1.8611987829208374\n86.147705078125 1.5697367191314697\n81.70919799804688 1.4862655401229858\n87.0477066040039 1.3368690013885498\n98.66348266601562 1.7423510551452637\n96.37520599365234 1.3208246231079102\n92.814453125 1.8332717418670654\n80.84992218017578 1.9198791980743408\n87.80859375 1.0776509046554565\n84.58948516845703 1.8839328289031982\n89.78652954101562 1.70749831199646\n81.93563842773438 1.118016242980957\n84.79975891113281 1.4021191596984863\n90.56642150878906 1.2989983558654785\n82.07353210449219 1.1830602884292603\n82.82074737548828 1.949235200881958\n\n\nText(0, 0.5, 'Electricity Consumption')\n\n\n\n\n\n\ntorch.distributions.Uniform(0, 1).support\n\nInterval(lower_bound=0.0, upper_bound=1.0)\n\n\n\ntorch.distributions.LogNormal(0, 1).support\n\nGreaterThan(lower_bound=0.0)\n\n\n\nforward(xs, theta_0, theta_1)\n\ntensor([83.7934])\n\n\n\ntorch.stack(preds).max(axis=0).values\n\ntensor([ 100.2101,  118.9044,  137.5986,  156.2929,  175.8384,  195.5405,\n         215.2426,  234.9448,  254.6469,  274.3490,  294.0511,  313.7533,\n         333.4554,  353.1575,  372.8596,  392.5617,  412.2639,  431.9660,\n         451.6681,  471.3702,  491.0724,  510.7745,  530.4766,  550.1788,\n         569.8809,  589.5830,  609.2852,  628.9872,  648.6894,  668.3915,\n         688.0936,  707.7958,  727.4979,  747.2000,  766.9022,  786.6043,\n         806.3064,  826.0085,  845.7106,  865.4127,  885.1149,  904.8170,\n         924.5192,  944.2213,  963.9234,  983.6255, 1003.3276, 1023.0298,\n        1042.7319, 1062.4340, 1082.1361, 1101.8383, 1121.5404, 1141.2424,\n        1160.9446, 1180.6467, 1200.3489, 1220.0510, 1239.7531, 1259.4552,\n        1279.1573, 1298.8595, 1318.5615, 1338.2638, 1357.9658, 1377.6680,\n        1397.3701, 1417.0723, 1436.7743, 1456.4764, 1476.1786, 1495.8806,\n        1515.5829, 1535.2849, 1554.9871, 1574.6892, 1594.3914, 1614.0935,\n        1633.7955, 1653.4977, 1673.1998, 1692.9020, 1712.6040, 1732.3063,\n        1752.0083, 1771.7104, 1791.4126, 1811.1147, 1830.8168, 1850.5189,\n        1870.2211, 1889.9231, 1909.6254, 1929.3274, 1949.0295, 1968.7317,\n        1988.4338, 2008.1359, 2027.8381, 2047.5402])"
  },
  {
    "objectID": "notebooks/mle_log_reg.html",
    "href": "notebooks/mle_log_reg.html",
    "title": "Probabilistic Machine Learning",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Retina display\n%config InlineBackend.figure_format = 'retina'\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\n\nnp.random.seed(42)\nnum_samples = 100\nX1 = np.random.rand(num_samples, 2) + np.array([1, 1])\nX2 = np.random.rand(num_samples, 2) + np.array([-1, -1])\nX = np.vstack((X1, X2))\ny = np.hstack((np.zeros(num_samples), np.ones(num_samples)))\n\nX_tensor = torch.tensor(X, dtype=torch.float32)\ny_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n\n\nclass LogisticRegressionModel(nn.Module):\n    def __init__(self):\n        super(LogisticRegressionModel, self).__init__()\n        self.linear = nn.Linear(2, 1)\n\n    def forward(self, x):\n        return torch.sigmoid(self.linear(x))\n\n\nmodel = LogisticRegressionModel()\ncriterion = nn.BCELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n\nnum_epochs = 1000\nfor epoch in range(num_epochs):\n    optimizer.zero_grad()\n    outputs = model(X_tensor)\n    loss = criterion(outputs, y_tensor)\n    loss.backward()\n    optimizer.step()\n    if (epoch+1) % 50 == 0:\n        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch +\n              1, num_epochs, loss.item()))\n\nEpoch [50/1000], Loss: 0.1417\nEpoch [100/1000], Loss: 0.0846\nEpoch [150/1000], Loss: 0.0610\nEpoch [200/1000], Loss: 0.0478\nEpoch [250/1000], Loss: 0.0395\nEpoch [300/1000], Loss: 0.0336\nEpoch [350/1000], Loss: 0.0293\nEpoch [400/1000], Loss: 0.0260\nEpoch [450/1000], Loss: 0.0234\nEpoch [500/1000], Loss: 0.0213\nEpoch [550/1000], Loss: 0.0195\nEpoch [600/1000], Loss: 0.0181\nEpoch [650/1000], Loss: 0.0168\nEpoch [700/1000], Loss: 0.0157\nEpoch [750/1000], Loss: 0.0147\nEpoch [800/1000], Loss: 0.0139\nEpoch [850/1000], Loss: 0.0131\nEpoch [900/1000], Loss: 0.0125\nEpoch [950/1000], Loss: 0.0119\nEpoch [1000/1000], Loss: 0.0113\n\n\n\nplt.scatter(X1[:, 0], X1[:, 1], color='blue', label='Class 0')\nplt.scatter(X2[:, 0], X2[:, 1], color='red', label='Class 1')\n\nwith torch.no_grad():\n    w = model.linear.weight.numpy()\n    b = model.linear.bias.numpy()\n    x_boundary = np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 100)\n    y_boundary = -(w[0][0] * x_boundary + b) / w[0][1]\n    plt.plot(x_boundary, y_boundary, color='green',\n             linewidth=2, label='Decision Boundary')\n\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('Logistic Regression Decision Boundary')\nplt.legend()\nplt.savefig('figures/mle/log_reg.pdf')\n\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed"
  },
  {
    "objectID": "notebooks/distributions.html",
    "href": "notebooks/distributions.html",
    "title": "Discrete distributions",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\n\nBernoulli distribution\nThe PDF of the Bernoulli distribution is given by\n\\[\nf(x) = p^x (1-p)^{1-x}\n\\]\nwhere \\(x \\in \\{0, 1\\}\\) and \\(p \\in [0, 1]\\).\n\nbernoulli = torch.distributions.Bernoulli(probs=0.3)\nprint(bernoulli.probs)\n\neq_cat = torch.distributions.Categorical(probs = torch.tensor([0.7, 0.3]))\neq_cat.probs\n\ntensor(0.3000)\n\n\ntensor([0.7000, 0.3000])\n\n\n\n# Plot PDF\np_1 = bernoulli.probs.item()\np_0 = 1 - p_1\n\nplt.bar([0, 1], [p_0, p_1], color='C0', edgecolor='k')\nplt.ylim(0, 1)\nplt.xticks([0, 1], ['0', '1'])\n\n([&lt;matplotlib.axis.XTick at 0x7f31863dbca0&gt;,\n  &lt;matplotlib.axis.XTick at 0x7f31863dbc70&gt;],\n [Text(0, 0, '0'), Text(1, 0, '1')])\n\n\n\n\n\n\n### Careful!\nbernoulli = torch.distributions.Bernoulli(logits=-20.0)\nbernoulli.probs\n\ntensor(2.0612e-09)\n\n\nLogits?!\nProbs range from 0 to 1, logits range from -inf to inf. Logits are the inverse of the sigmoid function.\nThe sigmoid function is defined as:\n\\[\\sigma(x) = \\frac{1}{1 + e^{-x}}\\]\nThe inverse of the sigmoid function is defined as:\n\\[\\sigma^{-1}(x) = \\log \\frac{x}{1 - x}\\]\n\n### Sampling\nbernoulli.sample()\n\ntensor(0.)\n\n\n\nbernoulli.sample((2,))\n\ntensor([0., 0.])\n\n\n\ndata = bernoulli.sample((1000,))\ndata\n\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n\n\n\n### Count number of 1s\ndata.sum()\n\ntensor(0.)\n\n\n\n### IID sampling\nsize = 1000\ndata = torch.empty(size)\nfor s_num in range(size):\n    dist = torch.distributions.Bernoulli(probs=0.3) # Each sample uses the same distribution (Identical)\n    data[s_num] = dist.sample() # Each sample is independent (Independent)\n\n\n### Dependent sampling\nsize = 1000\n\n### If previous sample was 1, next sample is 1 with probability 0.9\n### If previous sample was 1, next sample is 0 with probability 0.1\n### If previous sample was 0, next sample is 0 with probability 0.8\n### If previous sample was 0, next sample is 1 with probability 0.2\n\n\n### Categorical distribution\n\np1 = -0.2\np2 = 0.3\np3 = 0.6\n\n#categorical = torch.distributions.Categorical(probs=torch.tensor([p1, p2, p3]))\n#categorical.probs\n\ncat2 = torch.distributions.Categorical(logits=torch.tensor([p1, p2, p3]))\ncat2.probs\n\ntensor([0.2052, 0.3383, 0.4566])\n\n\n\n# Plot PDF\n\nplt.bar([0, 1, 2], [p1, p2, p3], color='C0', edgecolor='k')\nplt.ylim(0, 1)\nplt.xticks([0, 1, 2], ['0', '1', '2'])\n\n([&lt;matplotlib.axis.XTick at 0x7f318427d0d0&gt;,\n  &lt;matplotlib.axis.XTick at 0x7f318427d0a0&gt;,\n  &lt;matplotlib.axis.XTick at 0x7f318428ef40&gt;],\n [Text(0, 0, '0'), Text(1, 0, '1'), Text(2, 0, '2')])\n\n\n\n\n\n\n### Uniform distribution\n\nuniform = torch.distributions.Uniform(low=0, high=1)\n\n\nuniform.sample()\n\ntensor(0.4386)\n\n\n\nuniform.support\n\nInterval(lower_bound=0.0, upper_bound=1.0)\n\n\n\n### Plot PDF\nxs = torch.linspace(0.0, 0.99999, 500)\nys = uniform.log_prob(xs).exp()\n\nplt.plot(xs, ys, color='C0')\n# Filled area\nplt.fill_between(xs, ys, color='C0', alpha=0.2)\n\n&lt;matplotlib.collections.PolyCollection at 0x7f3184213e50&gt;\n\n\n\n\n\n\n### Why log_prob? and not prob?\n\n\n### Normal distribution\n\nnormal = torch.distributions.Normal(loc=0, scale=1)\n\n\nnormal.support\n\nReal()\n\n\n\n### Plot PDF\nxs = torch.linspace(-5, 5, 500)\nys = normal.log_prob(xs).exp()\nplt.plot(xs, ys, color='C0')\n# Filled area\nplt.fill_between(xs, ys, color='C0', alpha=0.2)\n\n&lt;matplotlib.collections.PolyCollection at 0x7f318418df10&gt;\n\n\n\n\n\n\nxs = torch.linspace(-50, 50, 500)\nprobs = normal.log_prob(xs).exp()\nplt.plot(xs, probs, color='C0')\n# Filled area\n#plt.fill_between(xs, probs, color='C0', alpha=0.2)\n\n\n\n\n\nnormal.log_prob(torch.tensor(-20)), normal.log_prob(torch.tensor(-40))\n\nnormal.log_prob(torch.tensor(-20)).exp(), normal.log_prob(torch.tensor(-40)).exp()\n\n(tensor(0.), tensor(0.))\n\n\n\nxs = torch.linspace(-50, 50, 500)\nlogprobs = normal.log_prob(xs)\nplt.plot(xs, logprobs, color='C0')\n\n\n\n\n\ndef plot_normal(mu, sigma):\n    mu = torch.tensor(mu)\n    sigma = torch.tensor(sigma)\n    xs = torch.linspace(-40, 40, 1000)\n    dist = torch.distributions.Normal(mu, sigma)\n    \n    logprobs = dist.log_prob(xs)\n    probs = torch.exp(logprobs)\n    fig, ax = plt.subplots(nrows=2)\n    ax[0].plot(xs, probs)\n    ax[0].set_title(\"Probability\")\n    ax[1].plot(xs, logprobs)\n    ax[1].set_title(\"Log Probability\")\n\n\nplot_normal(0, 1)\n\n\n\n\n\n# Interactive slider for plot_normal function\n\nfrom ipywidgets import interact, FloatSlider\n\ninteract(plot_normal, mu=FloatSlider(min=-2, max=2, step=0.1, value=0), sigma=FloatSlider(min=0.1, max=2, step=0.1, value=1))\n\n\n\n\n&lt;function __main__.plot_normal(mu, sigma)&gt;\n\n\n\nsamples = normal.sample((1000,))\nsamples[:20]\n\ntensor([-0.0520, -1.0746,  0.1424, -0.1028, -1.0832,  0.2766,  1.3047, -2.3132,\n        -0.7942,  0.0828, -0.9418, -1.4644,  0.6963,  0.5597,  0.2721, -1.8722,\n        -1.0237,  1.4067, -0.0434, -1.5735])\n\n\n\n_ = plt.hist(samples.numpy(), bins=50, density=True, edgecolor='k')\n\n\n\n\n\n_ = plt.hist(samples.numpy(), bins=30, density=True, edgecolor='k')\n\n\n\n\n\nimport seaborn as sns\nsns.kdeplot(samples.numpy(), bw_adjust=2.1, shade=True)\n\n&lt;AxesSubplot:ylabel='Density'&gt;\n\n\n\n\n\n\n### IID sampling\n\nn_samples = 1000\nsamples = []\nfor i in range(n_samples):\n    dist = torch.distributions.Normal(0, 1) # Using identical distribution over all samples\n    samples.append(dist.sample()) # sample is independent of previous samples\n\nsamples = torch.stack(samples)\n\nfig, ax = plt.subplots(nrows=2)\nsns.kdeplot(samples.numpy(), bw_adjust=2.0, shade=True, ax=ax[0])\nax[0].set_title(\"KDE of IID samples\")\n\nax[1].plot(samples.numpy())\nax[1].set_title(\"IID samples\")\n\nText(0.5, 1.0, 'IID samples')\n\n\n\n\n\n\n### Non-IID sampling (non-identical distribution)\n\nn_samples = 100\nsamples = []\nfor i in range(n_samples):\n    # Non-indentical distribution\n    if i%2:\n        dist = torch.distributions.Normal(torch.tensor([2.0]), torch.tensor([0.5]))\n    else:\n        dist = torch.distributions.Normal(torch.tensor([-2.0]), torch.tensor([0.5]))\n    samples.append(dist.sample())\n\nsamples = torch.stack(samples)\n\nfig, ax = plt.subplots(nrows=2)\nsns.kdeplot(samples.numpy().flatten(), bw_adjust=1.0, shade=True, ax=ax[0])\nax[0].set_title(\"KDE of non-identical samples\")\n\nax[1].plot(samples.numpy().flatten())\nax[1].set_title(\"Samples over time\")\n\nText(0.5, 1.0, 'Samples over time')\n\n\n\n\n\n\n### Non-IID sampling (dependent sampling)\n\nn_samples = 1000\nprev_sample = torch.tensor([10.0])\nsamples = []\nfor i in range(n_samples):\n    dist = torch.distributions.Normal(prev_sample, 1)\n    sample = dist.sample()\n    samples.append(sample)\n    prev_sample = sample\n\nsamples = torch.stack(samples)\nfig, ax = plt.subplots(nrows=2)\nsns.kdeplot(samples.numpy().flatten(), bw_adjust=2.0, shade=True, ax=ax[0])\nax[0].set_title(\"KDE of samples\")\n\nax[1].plot(samples.numpy().flatten())\nax[1].set_title(\"IID samples\")    \n\nText(0.5, 1.0, 'IID samples')\n\n\n\n\n\n\n### Laplace distribution v/s Normal distribution\n\nlaplace = torch.distributions.Laplace(loc=0, scale=1)\nnormal = torch.distributions.Normal(loc=0, scale=1)\nstudent_t_1 = torch.distributions.StudentT(df=1)\nstudent_t_2 = torch.distributions.StudentT(df=2)\n\n\nxs = torch.linspace(-6, 6, 500)\nys_laplace = laplace.log_prob(xs).exp()\nplt.plot(xs, ys_laplace, color='C0', label='Laplace')\n\nys_normal = normal.log_prob(xs).exp()\nplt.plot(xs, ys_normal, color='C1', label='Normal')\n\nys_student_t_1 = student_t_1.log_prob(xs).exp()\nplt.plot(xs, ys_student_t_1, color='C2', label='Student T (df=1)')\n\nys_student_t_2 = student_t_2.log_prob(xs).exp()\nplt.plot(xs, ys_student_t_2, color='C3', label='Student T (df=2)')\n\nplt.legend()\n\nzoom  = False\nif zoom:\n    plt.xlim(5, 6)\n    plt.ylim(-0.002, 0.02)\n\n\n\n\n\n### Beta distribution\n\nbeta = torch.distributions.Beta(concentration1=2, concentration0=2)\nbeta.support\n\nInterval(lower_bound=0.0, upper_bound=1.0)\n\n\n\n# PDF\nxs = torch.linspace(0, 1, 500)\nys = beta.log_prob(xs).exp()\nplt.plot(xs, ys, color='C0')\n# Filled area\nplt.fill_between(xs, ys, color='C0', alpha=0.2)\n\n&lt;matplotlib.collections.PolyCollection at 0x7f1912fb6910&gt;\n\n\n\n\n\n\ns = beta.sample()\ns\n\ntensor(0.2485)\n\n\n\n# Add widget to play with parameters\nfrom ipywidgets import interact\n\ndef plot_beta(a, b):\n    beta = torch.distributions.Beta(concentration1=a, concentration0=b)\n    xs = torch.linspace(0, 1, 500)\n    ys = beta.log_prob(xs).exp()\n    plt.plot(xs, ys, color='C0')\n    # Filled area\n    plt.fill_between(xs, ys, color='C0', alpha=0.2)\n\ninteract(plot_beta,a=(0.1, 20, 0.1), b=(0.1, 20, 0.1))\n\n\n\n\n&lt;function __main__.plot_beta(a, b)&gt;\n\n\n\n### Dirichlet distribution\n\ndirichlet = torch.distributions.Dirichlet(concentration=torch.tensor([2.0, 2.0, 2.0]))\ndirichlet.support\n\nSimplex()\n\n\n\ns = dirichlet.sample()\nprint(s, s.sum())\n\ntensor([0.0994, 0.7448, 0.1558]) tensor(1.)\n\n\n\ns = dirichlet.sample()\nprint(s, s.sum())\n\ntensor([0.4175, 0.3628, 0.2197]) tensor(1.)\n\n\n\ndirichlet2 = torch.distributions.Dirichlet(concentration=torch.tensor([0.8, 0.1, 0.1]))\ns = dirichlet2.sample()\nprint(s, s.sum())\n\ntensor([9.7325e-01, 7.5464e-10, 2.6754e-02]) tensor(1.)"
  },
  {
    "objectID": "notebooks/active-learning.html",
    "href": "notebooks/active-learning.html",
    "title": "Active Learning",
    "section": "",
    "text": "try:\n    from astra.torch.models import ResNetClassifier\nexcept:\n    %pip install git+https://github.com/sustainability-lab/ASTRA\n\n\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport os\nimport numpy as np\nimport pandas as pd\n%matplotlib inline\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n\n# Confusion matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nimport torchsummary\nfrom tqdm import tqdm\n\nimport umap\n\n# ASTRA\nfrom astra.torch.data import load_cifar_10\nfrom astra.torch.utils import train_fn\nfrom astra.torch.models import ResNetClassifier\n\n# Netron, ONNX for model visualization\ntry:\n    import netron\nexcept ModuleNotFoundError:\n    %pip install netron\n    import netron\n\ntry:\n    import onnx\nexcept ModuleNotFoundError:\n    %pip install onnx\n    import onnx\n\nimport copy\n\n/home/patel_zeel/ASTRA/astra/torch/data.py:12: UserWarning: TORCH_HOME not set, setting it to /home/patel_zeel/.cache/torch\n  warnings.warn(f\"TORCH_HOME not set, setting it to {os.environ['TORCH_HOME']}\")\n\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice\n\ndevice(type='cuda')"
  },
  {
    "objectID": "notebooks/active-learning.html#imports",
    "href": "notebooks/active-learning.html#imports",
    "title": "Active Learning",
    "section": "",
    "text": "try:\n    from astra.torch.models import ResNetClassifier\nexcept:\n    %pip install git+https://github.com/sustainability-lab/ASTRA\n\n\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport os\nimport numpy as np\nimport pandas as pd\n%matplotlib inline\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n\n# Confusion matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nimport torchsummary\nfrom tqdm import tqdm\n\nimport umap\n\n# ASTRA\nfrom astra.torch.data import load_cifar_10\nfrom astra.torch.utils import train_fn\nfrom astra.torch.models import ResNetClassifier\n\n# Netron, ONNX for model visualization\ntry:\n    import netron\nexcept ModuleNotFoundError:\n    %pip install netron\n    import netron\n\ntry:\n    import onnx\nexcept ModuleNotFoundError:\n    %pip install onnx\n    import onnx\n\nimport copy\n\n/home/patel_zeel/ASTRA/astra/torch/data.py:12: UserWarning: TORCH_HOME not set, setting it to /home/patel_zeel/.cache/torch\n  warnings.warn(f\"TORCH_HOME not set, setting it to {os.environ['TORCH_HOME']}\")\n\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice\n\ndevice(type='cuda')"
  },
  {
    "objectID": "notebooks/active-learning.html#dataset",
    "href": "notebooks/active-learning.html#dataset",
    "title": "Active Learning",
    "section": "Dataset",
    "text": "Dataset\n\ndataset = load_cifar_10()\ndataset\n\nFiles already downloaded and verified\nFiles already downloaded and verified\n\n\n\nCIFAR-10 Dataset\nlength of dataset: 60000\nshape of images: torch.Size([3, 32, 32])\nlen of classes: 10\nclasses: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\ndtype of images: torch.float32\ndtype of labels: torch.int64\n            \n\n\n\n# Plot some images\nplt.figure(figsize=(6, 6))\nfor i in range(25):\n    plt.subplot(5, 5, i+1)\n    plt.imshow(torch.einsum(\"chw-&gt;hwc\", dataset.data[i].cpu()))\n    plt.axis('off')\n    plt.title(dataset.classes[dataset.targets[i]])\nplt.tight_layout()\n\n\n\n\n\nData splitting\n\nn_train = 1000\nn_test = 20000\n\nX = dataset.data\ny = dataset.targets\n\nprint(X.shape)\nprint(X.shape, X.dtype)\nprint(X.min(), X.max())\nprint(y.shape, y.dtype)\n\ntorch.Size([60000, 3, 32, 32])\ntorch.Size([60000, 3, 32, 32]) torch.float32\ntensor(0.) tensor(1.)\ntorch.Size([60000]) torch.int64\n\n\n\ntorch.manual_seed(0)\nidx = torch.randperm(len(X))\ntrain_idx = idx[:n_train]\npool_idx = idx[n_train:-n_test]\ntest_idx = idx[-n_test:]\nprint(f\"Length of train set: {len(train_idx)}\")\nprint(f\"Length of pool set: {len(pool_idx)}\")\nprint(f\"Length of test set: {len(test_idx)}\")\n\nLength of train set: 1000\nLength of pool set: 39000\nLength of test set: 20000\n\n\n\nresnet = ResNetClassifier(models.resnet18, models.ResNet18_Weights.DEFAULT, n_classes=10).to(device)\n\n/home/patel_zeel/miniconda3/envs/torch_gpu_py311/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n  warnings.warn(\"Can't initialize NVML\")\n\n\n\ntorchsummary.summary(resnet, (3, 32, 32))\n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 64, 16, 16]           9,408\n       BatchNorm2d-2           [-1, 64, 16, 16]             128\n              ReLU-3           [-1, 64, 16, 16]               0\n         MaxPool2d-4             [-1, 64, 8, 8]               0\n            Conv2d-5             [-1, 64, 8, 8]          36,864\n       BatchNorm2d-6             [-1, 64, 8, 8]             128\n              ReLU-7             [-1, 64, 8, 8]               0\n            Conv2d-8             [-1, 64, 8, 8]          36,864\n       BatchNorm2d-9             [-1, 64, 8, 8]             128\n             ReLU-10             [-1, 64, 8, 8]               0\n       BasicBlock-11             [-1, 64, 8, 8]               0\n           Conv2d-12             [-1, 64, 8, 8]          36,864\n      BatchNorm2d-13             [-1, 64, 8, 8]             128\n             ReLU-14             [-1, 64, 8, 8]               0\n           Conv2d-15             [-1, 64, 8, 8]          36,864\n      BatchNorm2d-16             [-1, 64, 8, 8]             128\n             ReLU-17             [-1, 64, 8, 8]               0\n       BasicBlock-18             [-1, 64, 8, 8]               0\n           Conv2d-19            [-1, 128, 4, 4]          73,728\n      BatchNorm2d-20            [-1, 128, 4, 4]             256\n             ReLU-21            [-1, 128, 4, 4]               0\n           Conv2d-22            [-1, 128, 4, 4]         147,456\n      BatchNorm2d-23            [-1, 128, 4, 4]             256\n           Conv2d-24            [-1, 128, 4, 4]           8,192\n      BatchNorm2d-25            [-1, 128, 4, 4]             256\n             ReLU-26            [-1, 128, 4, 4]               0\n       BasicBlock-27            [-1, 128, 4, 4]               0\n           Conv2d-28            [-1, 128, 4, 4]         147,456\n      BatchNorm2d-29            [-1, 128, 4, 4]             256\n             ReLU-30            [-1, 128, 4, 4]               0\n           Conv2d-31            [-1, 128, 4, 4]         147,456\n      BatchNorm2d-32            [-1, 128, 4, 4]             256\n             ReLU-33            [-1, 128, 4, 4]               0\n       BasicBlock-34            [-1, 128, 4, 4]               0\n           Conv2d-35            [-1, 256, 2, 2]         294,912\n      BatchNorm2d-36            [-1, 256, 2, 2]             512\n             ReLU-37            [-1, 256, 2, 2]               0\n           Conv2d-38            [-1, 256, 2, 2]         589,824\n      BatchNorm2d-39            [-1, 256, 2, 2]             512\n           Conv2d-40            [-1, 256, 2, 2]          32,768\n      BatchNorm2d-41            [-1, 256, 2, 2]             512\n             ReLU-42            [-1, 256, 2, 2]               0\n       BasicBlock-43            [-1, 256, 2, 2]               0\n           Conv2d-44            [-1, 256, 2, 2]         589,824\n      BatchNorm2d-45            [-1, 256, 2, 2]             512\n             ReLU-46            [-1, 256, 2, 2]               0\n           Conv2d-47            [-1, 256, 2, 2]         589,824\n      BatchNorm2d-48            [-1, 256, 2, 2]             512\n             ReLU-49            [-1, 256, 2, 2]               0\n       BasicBlock-50            [-1, 256, 2, 2]               0\n           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n             ReLU-53            [-1, 512, 1, 1]               0\n           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n           Conv2d-56            [-1, 512, 1, 1]         131,072\n      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n             ReLU-58            [-1, 512, 1, 1]               0\n       BasicBlock-59            [-1, 512, 1, 1]               0\n           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n             ReLU-62            [-1, 512, 1, 1]               0\n           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n             ReLU-65            [-1, 512, 1, 1]               0\n       BasicBlock-66            [-1, 512, 1, 1]               0\nAdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n         Identity-68                  [-1, 512]               0\n           ResNet-69                  [-1, 512]               0\n          Flatten-70                  [-1, 512]               0\n           ResNet-71                  [-1, 512]               0\n           Linear-72                  [-1, 512]         262,656\n             ReLU-73                  [-1, 512]               0\n          Dropout-74                  [-1, 512]               0\n              MLP-75                  [-1, 512]               0\n           Linear-76                   [-1, 10]           5,130\n    MLPClassifier-77                   [-1, 10]               0\n================================================================\nTotal params: 11,444,298\nTrainable params: 11,444,298\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.01\nForward/backward pass size (MB): 1.32\nParams size (MB): 43.66\nEstimated Total Size (MB): 44.98\n----------------------------------------------------------------\n\n\n\ndef get_accuracy(net, X, y):\n    # Set the net to evaluation mode\n    net.eval()\n    with torch.no_grad():\n        logits_pred = net(X)\n        y_pred = logits_pred.argmax(dim=1)\n        acc = (y_pred == y).float().mean()\n    return y_pred, acc\n\ndef predict(net, classes, plot_confusion_matrix=False):\n    for i, (name, idx) in enumerate(zip((\"train\", \"pool\", \"test\"), [train_idx, pool_idx, test_idx])):\n        X_dataset = X[idx].to(device)\n        y_dataset = y[idx].to(device)\n        y_pred, acc = get_accuracy(net, X_dataset, y_dataset)\n        print(f'{name} set accuracy: {acc*100:.2f}%')\n        if plot_confusion_matrix:\n            cm = confusion_matrix(y_dataset.cpu(), y_pred.cpu())\n            cm_display = ConfusionMatrixDisplay(cm, display_labels=classes).plot(values_format='d'\n                                                                                , cmap='Blues')\n            # Rotate the labels on x-axis to make them readable\n            _ = plt.xticks(rotation=90)\n            plt.show()\n\n\npredict(resnet, dataset.classes, plot_confusion_matrix=True)\n\ntrain set accuracy: 7.70%\npool set accuracy: 7.57%\ntest set accuracy: 7.58%\n\n\n\n\n\n\n\n\n\n\n\n\ndef viz_embeddings(net, X, y, device):\n    reducer = umap.UMAP()\n    with torch.no_grad():\n        emb = net.featurizer(X.to(device))\n    emb = emb.cpu().numpy()\n    emb = reducer.fit_transform(emb)\n    plt.figure(figsize=(4, 4))\n    plt.scatter(emb[:, 0], emb[:, 1], c=y.cpu().numpy(), cmap='tab10')\n    # Add a colorbar legend to mark color to class mapping\n    cb = plt.colorbar(boundaries=np.arange(11)-0.5)\n    cb.set_ticks(np.arange(10))\n    cb.set_ticklabels(dataset.classes)\n    plt.title(\"UMAP embeddings\")\n    plt.tight_layout()\n\nviz_embeddings(resnet, X[train_idx], y[train_idx], device)\n\n\n\n\n\nTrain the model on train set\n\nmodel_only_train = ResNetClassifier(models.resnet18, None, n_classes=10, activation=nn.GELU(), dropout=0.1).to(device)\niter_losses, epoch_losses = train_fn(model_only_train, X[train_idx], y[train_idx], nn.CrossEntropyLoss(), lr=3e-4, \n                                     batch_size=128, epochs=30, verbose=False)\n\n\nplt.plot(iter_losses)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Training loss\")\n\nText(0, 0.5, 'Training loss')\n\n\n\n\n\n\npredict(model_only_train, dataset.classes, plot_confusion_matrix=True)\n\ntrain set accuracy: 100.00%\npool set accuracy: 35.53%\ntest set accuracy: 35.33%\n\n\n\n\n\n\n\n\n\n\n\n\nviz_embeddings(model_only_train, X[train_idx], y[train_idx], device)\n\n\n\n\n\nviz_embeddings(model_only_train, X[test_idx[:1000]], y[test_idx[:1000]], device)\n\n\n\n\n\n\n\nTrain on train + pool\n\ntrain_plus_pool_idx = torch.cat([train_idx, pool_idx])\n\nmodel_train_plus_pool = ResNetClassifier(models.resnet18, None, n_classes=10, activation=nn.GELU(), dropout=0.1).to(device)\n\niter_losses, epoch_losses = train_fn(model_train_plus_pool, X[train_plus_pool_idx], y[train_plus_pool_idx], loss_fn=nn.CrossEntropyLoss(),\n                                     lr=3e-4,\n                                        batch_size=1024, epochs=30)\n\nLoss: 1.318003: 100%|██████████| 40/40 [00:01&lt;00:00, 33.50it/s]\nLoss: 1.130332: 100%|██████████| 40/40 [00:01&lt;00:00, 35.81it/s]\nLoss: 0.821397: 100%|██████████| 40/40 [00:01&lt;00:00, 36.25it/s]\nLoss: 0.779279: 100%|██████████| 40/40 [00:01&lt;00:00, 35.97it/s]\nLoss: 0.862528: 100%|██████████| 40/40 [00:01&lt;00:00, 35.12it/s]\nLoss: 0.524238: 100%|██████████| 40/40 [00:01&lt;00:00, 36.35it/s]\nLoss: 0.510165: 100%|██████████| 40/40 [00:01&lt;00:00, 36.51it/s]\nLoss: 0.423117: 100%|██████████| 40/40 [00:01&lt;00:00, 36.69it/s]\nLoss: 0.458616: 100%|██████████| 40/40 [00:01&lt;00:00, 36.86it/s]\nLoss: 0.165772: 100%|██████████| 40/40 [00:01&lt;00:00, 35.59it/s]\nLoss: 0.087809: 100%|██████████| 40/40 [00:01&lt;00:00, 35.90it/s]\nLoss: 0.296134: 100%|██████████| 40/40 [00:01&lt;00:00, 35.39it/s]\nLoss: 0.319740: 100%|██████████| 40/40 [00:01&lt;00:00, 36.15it/s]\nLoss: 0.301341: 100%|██████████| 40/40 [00:01&lt;00:00, 37.49it/s]\nLoss: 0.072943: 100%|██████████| 40/40 [00:01&lt;00:00, 36.23it/s]\nLoss: 0.047622: 100%|██████████| 40/40 [00:01&lt;00:00, 36.49it/s]\nLoss: 0.031719: 100%|██████████| 40/40 [00:01&lt;00:00, 37.51it/s]\nLoss: 0.031865: 100%|██████████| 40/40 [00:01&lt;00:00, 35.63it/s]\nLoss: 0.046578: 100%|██████████| 40/40 [00:01&lt;00:00, 37.41it/s]\nLoss: 0.066532: 100%|██████████| 40/40 [00:01&lt;00:00, 37.07it/s]\nLoss: 0.128377: 100%|██████████| 40/40 [00:01&lt;00:00, 35.39it/s]\nLoss: 0.075391: 100%|██████████| 40/40 [00:01&lt;00:00, 35.56it/s]\nLoss: 0.180659: 100%|██████████| 40/40 [00:01&lt;00:00, 33.25it/s]\nLoss: 0.122659: 100%|██████████| 40/40 [00:01&lt;00:00, 36.06it/s]\nLoss: 0.116485: 100%|██████████| 40/40 [00:01&lt;00:00, 36.39it/s]\nLoss: 0.216088: 100%|██████████| 40/40 [00:01&lt;00:00, 36.43it/s]\nLoss: 0.063434: 100%|██████████| 40/40 [00:01&lt;00:00, 36.54it/s]\nLoss: 0.111266: 100%|██████████| 40/40 [00:01&lt;00:00, 35.95it/s]\nLoss: 0.039604: 100%|██████████| 40/40 [00:01&lt;00:00, 35.99it/s]\nLoss: 0.111527: 100%|██████████| 40/40 [00:01&lt;00:00, 36.67it/s]\n\n\nEpoch 1: 1.636701644897461\nEpoch 2: 1.2044568725585938\nEpoch 3: 0.9858628723144531\nEpoch 4: 0.787159913635254\nEpoch 5: 0.6295903686523437\nEpoch 6: 0.5099980773925781\nEpoch 7: 0.3717664451599121\nEpoch 8: 0.3031633331298828\nEpoch 9: 0.29022122650146487\nEpoch 10: 0.26079437942504885\nEpoch 11: 0.13630542945861818\nEpoch 12: 0.1002807499885559\nEpoch 13: 0.18608614768981935\nEpoch 14: 0.16239934244155885\nEpoch 15: 0.14895454578399658\nEpoch 16: 0.06240539264678955\nEpoch 17: 0.02642523465156555\nEpoch 18: 0.026696961951255798\nEpoch 19: 0.03432212963104248\nEpoch 20: 0.03469213643074036\nEpoch 21: 0.07525548481941223\nEpoch 22: 0.13279609174728393\nEpoch 23: 0.07557179298400879\nEpoch 24: 0.08241450214385987\nEpoch 25: 0.07914679989814759\nEpoch 26: 0.08806239323616027\nEpoch 27: 0.11902619647979737\nEpoch 28: 0.05027928895950318\nEpoch 29: 0.05692305927276611\nEpoch 30: 0.03280420844554901\n\n\n\nplt.plot(iter_losses)   \nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Training loss\")\n\nText(0, 0.5, 'Training loss')\n\n\n\n\n\n\nviz_embeddings(model_train_plus_pool, X[train_idx], y[train_idx], device)\n\n\n\n\n\nviz_embeddings(model_train_plus_pool, X[test_idx[:1000]], y[test_idx[:1000]], device)\n\n\n\n\n\npredict(model_train_plus_pool, dataset.classes, plot_confusion_matrix=True)\n\ntrain set accuracy: 97.80%\npool set accuracy: 98.03%\ntest set accuracy: 61.14%\n\n\n\n\n\n\n\n\n\n\n\n\naccuracy_only_train = get_accuracy(model_only_train, X[test_idx].to(device), y[test_idx].to(device))[1]\naccuracy_train_plus_pool = get_accuracy(model_train_plus_pool, X[test_idx].to(device), y[test_idx].to(device))[1]\n\nplt.axhline(accuracy_only_train.cpu(), color='r', label='Only train')\nplt.axhline(accuracy_train_plus_pool.cpu(), color='g', label='Train + pool')\nplt.legend()\n\nplt.ylabel(\"Accuracy\")\n\nText(0, 0.5, 'Accuracy')\n\n\n\n\n\n\n\nActive learning loop\n\ndef setdiff1d(a, b):\n    mask = ~a.unsqueeze(1).eq(b).any(dim=1)\n    return torch.masked_select(a, mask)\n\na = torch.tensor([1, 2, 3, 4, 5])\nb = torch.tensor([1, 3, 5])\n\nprint(setdiff1d(a, b))\n\na = torch.tensor([1, 2, 3, 4, 5])\nb = torch.tensor([1, 2, 3, 4, 5])\n\nprint(setdiff1d(a, b))\n\ntensor([2, 4])\ntensor([], dtype=torch.int64)\n\n\n\ndef al_loop(model, query_strategy, num_al_iterations, num_epochs_finetune,\n            train_idx, pool_idx, test_idx, query_size, \n            X, y, device, random_seed=0, verbose=False):\n    \"\"\"\n    model: PyTorch model trained on train_idx\n    query_strategy: function that takes in model and pool_idx, \n                train_idx and returns indices to query\n    num_al_iterations: number of active learning iterations\n    num_epochs_finetune: number of epochs to train on queried data + train_idx\n    train_idx: indices of data used for training\n    pool_idx: indices of data used for querying\n    test_idx: indices of data used for testing\n    query_size: number of data points to query at each iteration\n    X: data\n    y: labels\n    device: torch device\n    random_seed: random seed\n    verbose: print statements\n    \"\"\"\n    tr_idx = train_idx.clone()\n    p_idx = pool_idx.clone()\n    torch.manual_seed(random_seed)\n    \n    print(f\"Initial train size: {train_idx.shape}\")\n    print(f\"Initial pool size: {pool_idx.shape}\")\n    \n    # Initial model test accuracy\n    init_accuracy = get_accuracy(model, X[test_idx].to(device), y[test_idx].to(device))[1].item()\n    print(f\"Test accuracy before AL: {init_accuracy:0.4f}\")\n    \n    # Test accuracies\n    test_accuracies = {0: init_accuracy}\n    \n    for iteration in range(num_al_iterations):\n        \n        # Query\n        model.eval()\n        query_idx = query_strategy(model, p_idx, tr_idx, random_seed, query_size, X, y, device)\n        # Add queried data to train_idx\n        tr_idx = torch.cat([tr_idx, query_idx])\n        # Remove queried data from pool_idx\n        p_idx = setdiff1d(p_idx, query_idx)\n        # Retrain model on pooled data\n        iter_losses, epoch_losses = train_fn(model, X[tr_idx], y[tr_idx], loss_fn=nn.CrossEntropyLoss(),\n                                             lr=3e-4, batch_size=1024,\n                                             epochs=num_epochs_finetune, \n                                             verbose=False)\n        test_accuracies[iteration+1] = get_accuracy(model, X[test_idx].to(device), y[test_idx].to(device))[1].item()\n        if verbose:\n            print(f\"Active learning iteration {iteration+1}/{num_al_iterations}\")\n            print(f\"Train set size: {len(tr_idx)}, Pool set size: {len(p_idx)}\")\n            print(f\"Test accuracy: {test_accuracies[iteration]:0.4f}\")\n            print()\n    return model, tr_idx, p_idx, test_accuracies      \n    \n\n\ndef random_sampling(model, pool_idx, train_idx, random_seed, query_size, X, y, device):\n    torch.manual_seed(random_seed)\n    query_idx = pool_idx[torch.randperm(len(pool_idx))[:query_size]]\n    return query_idx\n\n\nquery_size = 20\nnum_al_iterations = 50\n\n\n\nimport copy\n\nmodel_r = copy.deepcopy(model_only_train)\n\nmodel, t_idx, p_idx, test_acc_random = al_loop(model_r, random_sampling, num_al_iterations, 20, train_idx, pool_idx, test_idx, query_size, X, y,device=device,verbose=True)\n\nInitial train size: torch.Size([1000])\nInitial pool size: torch.Size([39000])\nTest accuracy before AL: 0.3532\nActive learning iteration 1/50\nTrain set size: 1020, Pool set size: 38980\nTest accuracy: 0.3532\n\nActive learning iteration 2/50\nTrain set size: 1040, Pool set size: 38960\nTest accuracy: 0.3646\n\nActive learning iteration 3/50\nTrain set size: 1060, Pool set size: 38940\nTest accuracy: 0.2930\n\nActive learning iteration 4/50\nTrain set size: 1080, Pool set size: 38920\nTest accuracy: 0.3288\n\nActive learning iteration 5/50\nTrain set size: 1100, Pool set size: 38900\nTest accuracy: 0.3429\n\nActive learning iteration 6/50\nTrain set size: 1120, Pool set size: 38880\nTest accuracy: 0.3391\n\nActive learning iteration 7/50\nTrain set size: 1140, Pool set size: 38860\nTest accuracy: 0.3533\n\nActive learning iteration 8/50\nTrain set size: 1160, Pool set size: 38840\nTest accuracy: 0.3580\n\nActive learning iteration 9/50\nTrain set size: 1180, Pool set size: 38820\nTest accuracy: 0.3529\n\nActive learning iteration 10/50\nTrain set size: 1200, Pool set size: 38800\nTest accuracy: 0.3370\n\nActive learning iteration 11/50\nTrain set size: 1220, Pool set size: 38780\nTest accuracy: 0.3599\n\nActive learning iteration 12/50\nTrain set size: 1240, Pool set size: 38760\nTest accuracy: 0.3711\n\nActive learning iteration 13/50\nTrain set size: 1260, Pool set size: 38740\nTest accuracy: 0.3647\n\nActive learning iteration 14/50\nTrain set size: 1280, Pool set size: 38720\nTest accuracy: 0.3762\n\nActive learning iteration 15/50\nTrain set size: 1300, Pool set size: 38700\nTest accuracy: 0.3592\n\nActive learning iteration 16/50\nTrain set size: 1320, Pool set size: 38680\nTest accuracy: 0.3703\n\nActive learning iteration 17/50\nTrain set size: 1340, Pool set size: 38660\nTest accuracy: 0.3832\n\nActive learning iteration 18/50\nTrain set size: 1360, Pool set size: 38640\nTest accuracy: 0.3783\n\nActive learning iteration 19/50\nTrain set size: 1380, Pool set size: 38620\nTest accuracy: 0.3989\n\nActive learning iteration 20/50\nTrain set size: 1400, Pool set size: 38600\nTest accuracy: 0.3907\n\nActive learning iteration 21/50\nTrain set size: 1420, Pool set size: 38580\nTest accuracy: 0.3886\n\nActive learning iteration 22/50\nTrain set size: 1440, Pool set size: 38560\nTest accuracy: 0.3909\n\nActive learning iteration 23/50\nTrain set size: 1460, Pool set size: 38540\nTest accuracy: 0.3914\n\nActive learning iteration 24/50\nTrain set size: 1480, Pool set size: 38520\nTest accuracy: 0.3984\n\nActive learning iteration 25/50\nTrain set size: 1500, Pool set size: 38500\nTest accuracy: 0.4037\n\nActive learning iteration 26/50\nTrain set size: 1520, Pool set size: 38480\nTest accuracy: 0.4018\n\nActive learning iteration 27/50\nTrain set size: 1540, Pool set size: 38460\nTest accuracy: 0.4005\n\nActive learning iteration 28/50\nTrain set size: 1560, Pool set size: 38440\nTest accuracy: 0.3951\n\nActive learning iteration 29/50\nTrain set size: 1580, Pool set size: 38420\nTest accuracy: 0.4089\n\nActive learning iteration 30/50\nTrain set size: 1600, Pool set size: 38400\nTest accuracy: 0.4150\n\nActive learning iteration 31/50\nTrain set size: 1620, Pool set size: 38380\nTest accuracy: 0.3971\n\nActive learning iteration 32/50\nTrain set size: 1640, Pool set size: 38360\nTest accuracy: 0.4115\n\nActive learning iteration 33/50\nTrain set size: 1660, Pool set size: 38340\nTest accuracy: 0.4040\n\nActive learning iteration 34/50\nTrain set size: 1680, Pool set size: 38320\nTest accuracy: 0.4166\n\nActive learning iteration 35/50\nTrain set size: 1700, Pool set size: 38300\nTest accuracy: 0.4111\n\nActive learning iteration 36/50\nTrain set size: 1720, Pool set size: 38280\nTest accuracy: 0.4158\n\nActive learning iteration 37/50\nTrain set size: 1740, Pool set size: 38260\nTest accuracy: 0.4119\n\nActive learning iteration 38/50\nTrain set size: 1760, Pool set size: 38240\nTest accuracy: 0.4155\n\nActive learning iteration 39/50\nTrain set size: 1780, Pool set size: 38220\nTest accuracy: 0.4022\n\nActive learning iteration 40/50\nTrain set size: 1800, Pool set size: 38200\nTest accuracy: 0.4122\n\nActive learning iteration 41/50\nTrain set size: 1820, Pool set size: 38180\nTest accuracy: 0.4129\n\nActive learning iteration 42/50\nTrain set size: 1840, Pool set size: 38160\nTest accuracy: 0.4187\n\nActive learning iteration 43/50\nTrain set size: 1860, Pool set size: 38140\nTest accuracy: 0.4152\n\nActive learning iteration 44/50\nTrain set size: 1880, Pool set size: 38120\nTest accuracy: 0.4195\n\nActive learning iteration 45/50\nTrain set size: 1900, Pool set size: 38100\nTest accuracy: 0.4227\n\nActive learning iteration 46/50\nTrain set size: 1920, Pool set size: 38080\nTest accuracy: 0.4348\n\nActive learning iteration 47/50\nTrain set size: 1940, Pool set size: 38060\nTest accuracy: 0.4250\n\nActive learning iteration 48/50\nTrain set size: 1960, Pool set size: 38040\nTest accuracy: 0.4243\n\nActive learning iteration 49/50\nTrain set size: 1980, Pool set size: 38020\nTest accuracy: 0.4245\n\nActive learning iteration 50/50\nTrain set size: 2000, Pool set size: 38000\nTest accuracy: 0.4189\n\n\n\n\npd.Series(test_acc_random).plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n### Now, running across multiple random seeds\n\nquery_size = 20\nnum_al_iterations = 50\nms = {}\nt_idxs = {}\np_idxs = {}\ntest_acc_random = {}\nfor rs in range(5):\n    print(f\"Random seed: {rs}\")\n    model = copy.deepcopy(model_only_train)\n    ms[rs], t_idxs[rs], p_idxs[rs], test_acc_random[rs] = al_loop(model, random_sampling, num_al_iterations, 20, train_idx, pool_idx, test_idx, query_size, X, y,device=device,verbose=False, random_seed=rs)\n\nRandom seed: 0\nInitial train size: torch.Size([1000])\nInitial pool size: torch.Size([39000])\nTest accuracy before AL: 0.3532\nRandom seed: 1\nInitial train size: torch.Size([1000])\nInitial pool size: torch.Size([39000])\nTest accuracy before AL: 0.3532\nRandom seed: 2\nInitial train size: torch.Size([1000])\nInitial pool size: torch.Size([39000])\nTest accuracy before AL: 0.3532\nRandom seed: 3\nInitial train size: torch.Size([1000])\nInitial pool size: torch.Size([39000])\nTest accuracy before AL: 0.3532\nRandom seed: 4\nInitial train size: torch.Size([1000])\nInitial pool size: torch.Size([39000])\nTest accuracy before AL: 0.3532\n\n\n\ntest_acc_random_df = pd.DataFrame(test_acc_random)\nmean_acc = test_acc_random_df.mean(axis=1)\nstd_acc = test_acc_random_df.std(axis=1)\n\n\nplt.plot(mean_acc, label=\"Random sampling (mean)\")\nplt.fill_between(mean_acc.index, mean_acc-std_acc,\n                 mean_acc+std_acc, alpha=0.2, label=\"Random sampling (std)\")\n\n# Accuracy of model trained on train_idx\nplt.axhline(accuracy_only_train.cpu(), color='r', label='Only train')\n\n# Accuracy of model trained on train_idx + pool_idx\nplt.axhline(accuracy_train_plus_pool.cpu(), color='g', label='Train + pool')\nplt.xlabel(\"Active learning iteration\")\nplt.ylabel(\"Test accuracy\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fe75c755150&gt;\n\n\n\n\n\n\ndef entropy_sampling(model, pool_idx, train_idx, random_seed, query_size, X, y, device):\n    torch.manual_seed(random_seed)\n    with torch.no_grad():\n        logits = model(X[pool_idx].to(device)) # (len(pool_idx), n_classes)\n        probs = F.softmax(logits, dim=1)\n        entropy = torch.sum(-probs * torch.log(probs), dim=1)\n        entropy_sorted = entropy.sort(descending=True)\n        query_idx = pool_idx.to(device)[entropy_sorted.indices[:query_size]]\n    return query_idx.cpu()\n\n\nentropy_sampling(model_only_train, pool_idx, train_idx, 0, 5, X, y, device)\n\ntensor([ 2946, 45792, 47575, 31607, 56040])\n\n\n\nmodel_e = copy.deepcopy(model_only_train)\n# AL loop\n\nm, p_idx, t_idx, test_acc_entropy  = al_loop(model_e, entropy_sampling, num_al_iterations, 20, train_idx, pool_idx, test_idx, query_size, \n                                             X, y,device=device,verbose=True, random_seed=0)\n\nInitial train size: torch.Size([1000])\nInitial pool size: torch.Size([39000])\nTest accuracy before AL: 0.3532\nActive learning iteration 1/50\nTrain set size: 1020, Pool set size: 38980\nTest accuracy: 0.3532\n\nActive learning iteration 2/50\nTrain set size: 1040, Pool set size: 38960\nTest accuracy: 0.3591\n\nActive learning iteration 3/50\nTrain set size: 1060, Pool set size: 38940\nTest accuracy: 0.3174\n\nActive learning iteration 4/50\nTrain set size: 1080, Pool set size: 38920\nTest accuracy: 0.3345\n\nActive learning iteration 5/50\nTrain set size: 1100, Pool set size: 38900\nTest accuracy: 0.3158\n\nActive learning iteration 6/50\nTrain set size: 1120, Pool set size: 38880\nTest accuracy: 0.3662\n\nActive learning iteration 7/50\nTrain set size: 1140, Pool set size: 38860\nTest accuracy: 0.3424\n\nActive learning iteration 8/50\nTrain set size: 1160, Pool set size: 38840\nTest accuracy: 0.3477\n\nActive learning iteration 9/50\nTrain set size: 1180, Pool set size: 38820\nTest accuracy: 0.3511\n\nActive learning iteration 10/50\nTrain set size: 1200, Pool set size: 38800\nTest accuracy: 0.3558\n\nActive learning iteration 11/50\nTrain set size: 1220, Pool set size: 38780\nTest accuracy: 0.3467\n\nActive learning iteration 12/50\nTrain set size: 1240, Pool set size: 38760\nTest accuracy: 0.3605\n\nActive learning iteration 13/50\nTrain set size: 1260, Pool set size: 38740\nTest accuracy: 0.3667\n\nActive learning iteration 14/50\nTrain set size: 1280, Pool set size: 38720\nTest accuracy: 0.3646\n\nActive learning iteration 15/50\nTrain set size: 1300, Pool set size: 38700\nTest accuracy: 0.3633\n\nActive learning iteration 16/50\nTrain set size: 1320, Pool set size: 38680\nTest accuracy: 0.3802\n\nActive learning iteration 17/50\nTrain set size: 1340, Pool set size: 38660\nTest accuracy: 0.3669\n\nActive learning iteration 18/50\nTrain set size: 1360, Pool set size: 38640\nTest accuracy: 0.3780\n\nActive learning iteration 19/50\nTrain set size: 1380, Pool set size: 38620\nTest accuracy: 0.3739\n\nActive learning iteration 20/50\nTrain set size: 1400, Pool set size: 38600\nTest accuracy: 0.3843\n\nActive learning iteration 21/50\nTrain set size: 1420, Pool set size: 38580\nTest accuracy: 0.3860\n\nActive learning iteration 22/50\nTrain set size: 1440, Pool set size: 38560\nTest accuracy: 0.3837\n\nActive learning iteration 23/50\nTrain set size: 1460, Pool set size: 38540\nTest accuracy: 0.3799\n\nActive learning iteration 24/50\nTrain set size: 1480, Pool set size: 38520\nTest accuracy: 0.3591\n\nActive learning iteration 25/50\nTrain set size: 1500, Pool set size: 38500\nTest accuracy: 0.3926\n\nActive learning iteration 26/50\nTrain set size: 1520, Pool set size: 38480\nTest accuracy: 0.3903\n\nActive learning iteration 27/50\nTrain set size: 1540, Pool set size: 38460\nTest accuracy: 0.3960\n\nActive learning iteration 28/50\nTrain set size: 1560, Pool set size: 38440\nTest accuracy: 0.3824\n\nActive learning iteration 29/50\nTrain set size: 1580, Pool set size: 38420\nTest accuracy: 0.3941\n\nActive learning iteration 30/50\nTrain set size: 1600, Pool set size: 38400\nTest accuracy: 0.3977\n\nActive learning iteration 31/50\nTrain set size: 1620, Pool set size: 38380\nTest accuracy: 0.3928\n\nActive learning iteration 32/50\nTrain set size: 1640, Pool set size: 38360\nTest accuracy: 0.3870\n\nActive learning iteration 33/50\nTrain set size: 1660, Pool set size: 38340\nTest accuracy: 0.3995\n\nActive learning iteration 34/50\nTrain set size: 1680, Pool set size: 38320\nTest accuracy: 0.4075\n\nActive learning iteration 35/50\nTrain set size: 1700, Pool set size: 38300\nTest accuracy: 0.4020\n\nActive learning iteration 36/50\nTrain set size: 1720, Pool set size: 38280\nTest accuracy: 0.3941\n\nActive learning iteration 37/50\nTrain set size: 1740, Pool set size: 38260\nTest accuracy: 0.4062\n\nActive learning iteration 38/50\nTrain set size: 1760, Pool set size: 38240\nTest accuracy: 0.4101\n\nActive learning iteration 39/50\nTrain set size: 1780, Pool set size: 38220\nTest accuracy: 0.3931\n\nActive learning iteration 40/50\nTrain set size: 1800, Pool set size: 38200\nTest accuracy: 0.3932\n\nActive learning iteration 41/50\nTrain set size: 1820, Pool set size: 38180\nTest accuracy: 0.4135\n\nActive learning iteration 42/50\nTrain set size: 1840, Pool set size: 38160\nTest accuracy: 0.4065\n\nActive learning iteration 43/50\nTrain set size: 1860, Pool set size: 38140\nTest accuracy: 0.4134\n\nActive learning iteration 44/50\nTrain set size: 1880, Pool set size: 38120\nTest accuracy: 0.4129\n\nActive learning iteration 45/50\nTrain set size: 1900, Pool set size: 38100\nTest accuracy: 0.4169\n\nActive learning iteration 46/50\nTrain set size: 1920, Pool set size: 38080\nTest accuracy: 0.4140\n\nActive learning iteration 47/50\nTrain set size: 1940, Pool set size: 38060\nTest accuracy: 0.4044\n\nActive learning iteration 48/50\nTrain set size: 1960, Pool set size: 38040\nTest accuracy: 0.4225\n\nActive learning iteration 49/50\nTrain set size: 1980, Pool set size: 38020\nTest accuracy: 0.4159\n\nActive learning iteration 50/50\nTrain set size: 2000, Pool set size: 38000\nTest accuracy: 0.4221\n\n\n\n\nfig, ax = plt.subplots(1, 1)\nax.plot(mean_acc, label=\"Random sampling (mean)\")\nax.fill_between(mean_acc.index, mean_acc-std_acc,\n                 mean_acc+std_acc, alpha=0.2, label=\"Random sampling (std)\")\nax.axhline(accuracy_only_train.cpu(), color='r', label='Only train')\nax.axhline(accuracy_train_plus_pool.cpu(), color='g', label='Train + pool')\n\npd.Series(test_acc_entropy).plot(ax=ax, label=\"Entropy sampling\", color='C2')\n\nax.set_xlabel(\"Active learning iteration\")\nax.set_ylabel(\"Test accuracy\")\nax.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fe70c114990&gt;\n\n\n\n\n\n\ndef margin_sampling(model, pool_idx, train_idx, random_seed, query_size, X, y, device):\n    torch.manual_seed(random_seed)\n    with torch.no_grad():\n        logits = model(X[pool_idx].to(device))    \n        probs = F.softmax(logits, dim=1)\n        margin = torch.topk(probs, 2, dim=1).values\n        margin = margin[:, 0] - margin[:, 1]\n        margin_sorted = margin.sort(descending=True)\n        query_idx = pool_idx.to(device)[margin_sorted.indices[:query_size]]\n    return query_idx.cpu()\n\n\nmargin_sampling(model_only_train, pool_idx, train_idx, 0, 5, X, y, device)\n\ntensor([36785, 51553,  1668, 53790, 28164])\n\n\n\ndef diversity_sampling(model, pool_idx, train_idx, random_seed, query_size, X, y, device):\n    torch.manual_seed(random_seed)\n    with torch.no_grad():\n        emb_pool = model.featurizer(X[pool_idx].to(device))\n        emb_train = model.featurizer(X[train_idx].to(device))\n        # Find the distance between each pool point and each train point\n        dist = torch.cdist(emb_pool, emb_train)\n        print(dist.shape)\n        # Find the minimum distance for each pool point\n        min_dist = dist.min(dim=1).values\n        print(min_dist)\n        # Sort the pool points by minimum distance\n        min_dist_sorted = min_dist.sort(descending=True)\n        query_idx = pool_idx.to(device)[min_dist_sorted.indices[:query_size]]\n    return query_idx.cpu()\n        \n\n\ndiversity_sampling(model_only_train, pool_idx, train_idx, 0, 5, X, y, device)\n\ntorch.Size([39000, 1000])\ntensor([20.9841, 14.6172, 21.3995,  ..., 22.9982, 18.0180, 20.0437],\n       device='cuda:0')\n\n\ntensor([40154, 26018, 15418, 18260, 43270])\n\n\n\n# BALD sample dataset to illustrate the idea\n\npred_A = torch.tensor([0.5]*10).reshape(-1, 1, 1).repeat(1, 1, 2)\nprint(pred_A.shape)\nprint(pred_A)\n\ntorch.Size([10, 1, 2])\ntensor([[[0.5000, 0.5000]],\n\n        [[0.5000, 0.5000]],\n\n        [[0.5000, 0.5000]],\n\n        [[0.5000, 0.5000]],\n\n        [[0.5000, 0.5000]],\n\n        [[0.5000, 0.5000]],\n\n        [[0.5000, 0.5000]],\n\n        [[0.5000, 0.5000]],\n\n        [[0.5000, 0.5000]],\n\n        [[0.5000, 0.5000]]])\n\n\n\npred_B = torch.tensor([[0.0, 1.0], [1.0, 0.0]]).repeat(5, 1).reshape(10, 1, 2)\nprint(pred_B.shape)\nprint(pred_B)\n\ntorch.Size([10, 1, 2])\ntensor([[[0., 1.]],\n\n        [[1., 0.]],\n\n        [[0., 1.]],\n\n        [[1., 0.]],\n\n        [[0., 1.]],\n\n        [[1., 0.]],\n\n        [[0., 1.]],\n\n        [[1., 0.]],\n\n        [[0., 1.]],\n\n        [[1., 0.]]])\n\n\n\ndef BALD_score(logits):\n    \"\"\"\n    logits: (n_MC_passes, n_samples, n_classes)\n    \"\"\"\n    probs = F.softmax(logits, dim=2)\n    expected_probs = probs.mean(dim=0) # Expectation over MC passes\n    if bald_verbose:\n        print(probs.shape)\n        print(expected_probs.shape)\n    entropy_expected_probs = torch.sum(-expected_probs * torch.log(expected_probs), dim=1)\n    if bald_verbose:\n        print(entropy_expected_probs.shape)\n    \n    entropy_probs = torch.sum(-probs * torch.log(probs), dim=2)\n    if bald_verbose:\n        print(entropy_probs.shape)\n    expected_entropy_probs = entropy_probs.mean(dim=0)\n    if bald_verbose:\n        print(expected_entropy_probs.shape)\n    \n    bald_score = entropy_expected_probs - expected_entropy_probs\n    if bald_verbose:\n        print(bald_score.shape)\n    return bald_score\n\n\nbald_verbose = True\ns = BALD_score(torch.rand(100, 5, 3))\n\ntorch.Size([100, 5, 3])\ntorch.Size([5, 3])\ntorch.Size([5])\ntorch.Size([100, 5])\ntorch.Size([5])\ntorch.Size([5])\n\n\n\nbald_verbose = False\n\n# Entropy\nentropy_A = torch.sum(-pred_A.mean(dim=0) * torch.log(pred_A.mean(dim=0)), dim=1)\nentropy_B = torch.sum(-pred_B.mean(dim=0) * torch.log(pred_B.mean(dim=0)), dim=1)\n\nprint(entropy_A)\nprint(entropy_B)\n\ntensor([0.6931])\ntensor([0.6931])\n\n\n\nprint(BALD_score(pred_A))\nprint(BALD_score(pred_B))\n\ntensor([0.])\ntensor([0.1109])\n\n\n\ndef BALD_sampling(model, pool_idx, train_idx, random_seed, query_size, X, y, device):\n    \"\"\"\n    model: MC dropout model\n    \"\"\"\n    \n    # Evaluate the logits on the pool set for each MC pass\n    n_MC_passes = 8\n    logits = []\n    model.train()\n    with torch.no_grad():\n        for mc_pass in range(n_MC_passes):\n            # Set mode of model for MC dropout\n            logits.append(model(X[pool_idx].to(device)))\n    logits = torch.stack(logits)\n    # print(logits.shape)\n    bald_score = BALD_score(logits)\n    bald_score_sorted = bald_score.sort(descending=True)\n    query_idx = pool_idx.to(device)[bald_score_sorted.indices[:query_size]]\n    return query_idx.cpu()\n\n\nz = BALD_sampling(model_only_train, pool_idx, train_idx, 0, 5, X, y, device)\n\ntorch.Size([8, 39000, 10])\ntorch.Size([39000, 10])\ntorch.Size([39000])\ntorch.Size([8, 39000])\ntorch.Size([39000])\ntorch.Size([39000])\n\n\n\nmodel_e = copy.deepcopy(model_only_train)\n# AL loop\n\nm, p_idx, t_idx, test_acc_margin  = al_loop(model_e, margin_sampling, num_al_iterations, 20, train_idx, pool_idx, test_idx, query_size, X, y,device=device,verbose=True, random_seed=0)\n\nInitial train size: torch.Size([1000])\nInitial pool size: torch.Size([39000])\nTest accuracy before AL: 0.3521\nActive learning iteration 1/50\nTrain set size: 1020, Pool set size: 38980\nTest accuracy: 0.3521\n\nActive learning iteration 2/50\nTrain set size: 1040, Pool set size: 38960\nTest accuracy: 0.3510\n\nActive learning iteration 3/50\nTrain set size: 1060, Pool set size: 38940\nTest accuracy: 0.2814\n\nActive learning iteration 4/50\nTrain set size: 1080, Pool set size: 38920\nTest accuracy: 0.3350\n\nActive learning iteration 5/50\nTrain set size: 1100, Pool set size: 38900\nTest accuracy: 0.3204\n\nActive learning iteration 6/50\nTrain set size: 1120, Pool set size: 38880\nTest accuracy: 0.3264\n\nActive learning iteration 7/50\nTrain set size: 1140, Pool set size: 38860\nTest accuracy: 0.3409\n\nActive learning iteration 8/50\nTrain set size: 1160, Pool set size: 38840\nTest accuracy: 0.3454\n\nActive learning iteration 9/50\nTrain set size: 1180, Pool set size: 38820\nTest accuracy: 0.3625\n\nActive learning iteration 10/50\nTrain set size: 1200, Pool set size: 38800\nTest accuracy: 0.3419\n\nActive learning iteration 11/50\nTrain set size: 1220, Pool set size: 38780\nTest accuracy: 0.3574\n\nActive learning iteration 12/50\nTrain set size: 1240, Pool set size: 38760\nTest accuracy: 0.3632\n\nActive learning iteration 13/50\nTrain set size: 1260, Pool set size: 38740\nTest accuracy: 0.3593\n\nActive learning iteration 14/50\nTrain set size: 1280, Pool set size: 38720\nTest accuracy: 0.3571\n\nActive learning iteration 15/50\nTrain set size: 1300, Pool set size: 38700\nTest accuracy: 0.3683\n\nActive learning iteration 16/50\nTrain set size: 1320, Pool set size: 38680\nTest accuracy: 0.3810\n\nActive learning iteration 17/50\nTrain set size: 1340, Pool set size: 38660\nTest accuracy: 0.3817\n\nActive learning iteration 18/50\nTrain set size: 1360, Pool set size: 38640\nTest accuracy: 0.3795\n\nActive learning iteration 19/50\nTrain set size: 1380, Pool set size: 38620\nTest accuracy: 0.3812\n\nActive learning iteration 20/50\nTrain set size: 1400, Pool set size: 38600\nTest accuracy: 0.3898\n\nActive learning iteration 21/50\nTrain set size: 1420, Pool set size: 38580\nTest accuracy: 0.3847\n\nActive learning iteration 22/50\nTrain set size: 1440, Pool set size: 38560\nTest accuracy: 0.3881\n\nActive learning iteration 23/50\nTrain set size: 1460, Pool set size: 38540\nTest accuracy: 0.3826\n\nActive learning iteration 24/50\nTrain set size: 1480, Pool set size: 38520\nTest accuracy: 0.3846\n\nActive learning iteration 25/50\nTrain set size: 1500, Pool set size: 38500\nTest accuracy: 0.3821\n\nActive learning iteration 26/50\nTrain set size: 1520, Pool set size: 38480\nTest accuracy: 0.3795\n\nActive learning iteration 27/50\nTrain set size: 1540, Pool set size: 38460\nTest accuracy: 0.3668\n\nActive learning iteration 28/50\nTrain set size: 1560, Pool set size: 38440\nTest accuracy: 0.3878\n\nActive learning iteration 29/50\nTrain set size: 1580, Pool set size: 38420\nTest accuracy: 0.3817\n\nActive learning iteration 30/50\nTrain set size: 1600, Pool set size: 38400\nTest accuracy: 0.3943\n\nActive learning iteration 31/50\nTrain set size: 1620, Pool set size: 38380\nTest accuracy: 0.3880\n\nActive learning iteration 32/50\nTrain set size: 1640, Pool set size: 38360\nTest accuracy: 0.3844\n\nActive learning iteration 33/50\nTrain set size: 1660, Pool set size: 38340\nTest accuracy: 0.3962\n\nActive learning iteration 34/50\nTrain set size: 1680, Pool set size: 38320\nTest accuracy: 0.3863\n\nActive learning iteration 35/50\nTrain set size: 1700, Pool set size: 38300\nTest accuracy: 0.3951\n\nActive learning iteration 36/50\nTrain set size: 1720, Pool set size: 38280\nTest accuracy: 0.4009\n\nActive learning iteration 37/50\nTrain set size: 1740, Pool set size: 38260\nTest accuracy: 0.4013\n\nActive learning iteration 38/50\nTrain set size: 1760, Pool set size: 38240\nTest accuracy: 0.4113\n\nActive learning iteration 39/50\nTrain set size: 1780, Pool set size: 38220\nTest accuracy: 0.4067\n\nActive learning iteration 40/50\nTrain set size: 1800, Pool set size: 38200\nTest accuracy: 0.4026\n\nActive learning iteration 41/50\nTrain set size: 1820, Pool set size: 38180\nTest accuracy: 0.4134\n\nActive learning iteration 42/50\nTrain set size: 1840, Pool set size: 38160\nTest accuracy: 0.4095\n\nActive learning iteration 43/50\nTrain set size: 1860, Pool set size: 38140\nTest accuracy: 0.4150\n\nActive learning iteration 44/50\nTrain set size: 1880, Pool set size: 38120\nTest accuracy: 0.4081\n\nActive learning iteration 45/50\nTrain set size: 1900, Pool set size: 38100\nTest accuracy: 0.4110\n\nActive learning iteration 46/50\nTrain set size: 1920, Pool set size: 38080\nTest accuracy: 0.4203\n\nActive learning iteration 47/50\nTrain set size: 1940, Pool set size: 38060\nTest accuracy: 0.4192\n\nActive learning iteration 48/50\nTrain set size: 1960, Pool set size: 38040\nTest accuracy: 0.4165\n\nActive learning iteration 49/50\nTrain set size: 1980, Pool set size: 38020\nTest accuracy: 0.4135\n\nActive learning iteration 50/50\nTrain set size: 2000, Pool set size: 38000\nTest accuracy: 0.4199\n\n\n\n\nfig, ax = plt.subplots(1, 1)\nax.plot(mean_acc, label=\"Random sampling (mean)\")\nax.fill_between(mean_acc.index, mean_acc-std_acc,\n                 mean_acc+std_acc, alpha=0.2, label=\"Random sampling (std)\")\nax.axhline(accuracy_only_train.cpu(), color='r', label='Only train')\nax.axhline(accuracy_train_plus_pool.cpu(), color='g', label='Train + pool')\n\npd.Series(test_acc_entropy).plot(ax=ax, label=\"Entropy sampling\", color='C2')\npd.Series(test_acc_margin).plot(ax=ax, label=\"Margin sampling\", color='C4')\n\nax.set_xlabel(\"Active learning iteration\")\nax.set_ylabel(\"Test accuracy\")\nax.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fe752c68c10&gt;\n\n\n\n\n\n\nmodel_e = copy.deepcopy(model_only_train)\n# AL loop\n\nm, p_idx, t_idx, test_acc_diversity  = al_loop(model_e, diversity_sampling, num_al_iterations, 20, train_idx, pool_idx, test_idx, query_size, X, y,device=device,verbose=True, random_seed=0)\n\nInitial train size: torch.Size([1000])\nInitial pool size: torch.Size([39000])\nTest accuracy before AL: 0.3542\ntorch.Size([39000, 1000])\ntensor([23.6742, 16.5322, 23.8433,  ..., 25.6275, 19.5198, 21.9679],\n       device='cuda:0')\nActive learning iteration 1/50\nTrain set size: 1020, Pool set size: 38980\nTest accuracy: 0.3542\n\ntorch.Size([38980, 1020])\ntensor([22.4518, 17.6675, 18.1893,  ..., 20.2362, 19.4250, 20.4936],\n       device='cuda:0')\nActive learning iteration 2/50\nTrain set size: 1040, Pool set size: 38960\nTest accuracy: 0.3567\n\ntorch.Size([38960, 1040])\ntensor([18.6880, 14.2518, 21.2189,  ..., 23.0635, 16.1121, 19.2636],\n       device='cuda:0')\nActive learning iteration 3/50\nTrain set size: 1060, Pool set size: 38940\nTest accuracy: 0.2866\n\ntorch.Size([38940, 1060])\ntensor([19.6873, 18.1151, 18.7516,  ..., 22.0924, 15.2531, 17.8292],\n       device='cuda:0')\nActive learning iteration 4/50\nTrain set size: 1080, Pool set size: 38920\nTest accuracy: 0.3207\n\ntorch.Size([38920, 1080])\ntensor([18.8964, 17.3016, 17.7136,  ..., 21.9477, 16.0015, 18.8336],\n       device='cuda:0')\nActive learning iteration 5/50\nTrain set size: 1100, Pool set size: 38900\nTest accuracy: 0.3307\n\ntorch.Size([38900, 1100])\ntensor([17.6785, 15.6355, 21.0368,  ..., 24.9875, 18.3958, 19.8655],\n       device='cuda:0')\nActive learning iteration 6/50\nTrain set size: 1120, Pool set size: 38880\nTest accuracy: 0.3469\n\ntorch.Size([38880, 1120])\ntensor([17.3388, 19.3429, 19.3024,  ..., 22.3471, 19.8595, 17.8355],\n       device='cuda:0')\nActive learning iteration 7/50\nTrain set size: 1140, Pool set size: 38860\nTest accuracy: 0.3280\n\ntorch.Size([38860, 1140])\ntensor([20.5951, 14.4556, 17.5015,  ..., 23.5888, 17.3285, 17.5650],\n       device='cuda:0')\nActive learning iteration 8/50\nTrain set size: 1160, Pool set size: 38840\nTest accuracy: 0.3269\n\ntorch.Size([38840, 1160])\ntensor([19.8505, 16.7460, 16.7813,  ..., 21.0331, 14.3331, 18.0884],\n       device='cuda:0')\nActive learning iteration 9/50\nTrain set size: 1180, Pool set size: 38820\nTest accuracy: 0.3533\n\ntorch.Size([38820, 1180])\ntensor([17.8798, 16.4285, 19.6184,  ..., 22.6378, 16.6225, 16.5178],\n       device='cuda:0')\nActive learning iteration 10/50\nTrain set size: 1200, Pool set size: 38800\nTest accuracy: 0.3647\n\ntorch.Size([38800, 1200])\ntensor([14.8099, 20.4215, 17.6085,  ..., 17.5421, 17.3908, 15.8993],\n       device='cuda:0')\nActive learning iteration 11/50\nTrain set size: 1220, Pool set size: 38780\nTest accuracy: 0.3538\n\ntorch.Size([38780, 1220])\ntensor([12.3447, 16.7038, 19.9586,  ..., 16.9770, 13.5527, 16.1936],\n       device='cuda:0')\nActive learning iteration 12/50\nTrain set size: 1240, Pool set size: 38760\nTest accuracy: 0.3765\n\ntorch.Size([38760, 1240])\ntensor([10.8885, 15.6467, 17.1195,  ..., 17.9652, 14.7832, 14.1509],\n       device='cuda:0')\nActive learning iteration 13/50\nTrain set size: 1260, Pool set size: 38740\nTest accuracy: 0.3505\n\ntorch.Size([38740, 1260])\ntensor([10.9213, 16.9594, 14.1012,  ..., 16.9323, 18.8507, 20.4573],\n       device='cuda:0')\nActive learning iteration 14/50\nTrain set size: 1280, Pool set size: 38720\nTest accuracy: 0.3765\n\ntorch.Size([38720, 1280])\ntensor([15.6936, 12.4250, 13.8343,  ..., 20.8527, 11.3354, 17.0484],\n       device='cuda:0')\nActive learning iteration 15/50\nTrain set size: 1300, Pool set size: 38700\nTest accuracy: 0.3702\n\ntorch.Size([38700, 1300])\ntensor([11.7571, 11.6948, 15.0714,  ..., 22.1285, 11.6764, 18.9696],\n       device='cuda:0')\nActive learning iteration 16/50\nTrain set size: 1320, Pool set size: 38680\nTest accuracy: 0.3780\n\ntorch.Size([38680, 1320])\ntensor([11.6128, 13.6831, 19.4089,  ..., 19.8163, 13.5096, 17.7417],\n       device='cuda:0')\nActive learning iteration 17/50\nTrain set size: 1340, Pool set size: 38660\nTest accuracy: 0.3753\n\ntorch.Size([38660, 1340])\ntensor([12.7533, 12.5090, 13.7721,  ..., 21.9642, 11.7916, 16.0330],\n       device='cuda:0')\nActive learning iteration 18/50\nTrain set size: 1360, Pool set size: 38640\nTest accuracy: 0.3789\n\ntorch.Size([38640, 1360])\ntensor([13.6340, 18.0564, 16.5101,  ..., 17.5498, 13.4201, 14.1262],\n       device='cuda:0')\nActive learning iteration 19/50\nTrain set size: 1380, Pool set size: 38620\nTest accuracy: 0.3869\n\ntorch.Size([38620, 1380])\ntensor([14.3605, 13.9379, 13.9138,  ..., 16.9807, 11.0110, 17.9584],\n       device='cuda:0')\nActive learning iteration 20/50\nTrain set size: 1400, Pool set size: 38600\nTest accuracy: 0.3820\n\ntorch.Size([38600, 1400])\ntensor([ 9.3540, 15.1157, 15.3845,  ..., 20.5848, 13.5188, 20.3413],\n       device='cuda:0')\nActive learning iteration 21/50\nTrain set size: 1420, Pool set size: 38580\nTest accuracy: 0.3829\n\ntorch.Size([38580, 1420])\ntensor([10.9461, 13.1866, 12.9142,  ..., 19.3511, 18.5600, 15.6003],\n       device='cuda:0')\nActive learning iteration 22/50\nTrain set size: 1440, Pool set size: 38560\nTest accuracy: 0.3918\n\ntorch.Size([38560, 1440])\ntensor([15.2146, 13.9401,  9.8038,  ..., 18.6441, 13.0452, 14.2449],\n       device='cuda:0')\nActive learning iteration 23/50\nTrain set size: 1460, Pool set size: 38540\nTest accuracy: 0.3908\n\ntorch.Size([38540, 1460])\ntensor([14.3856, 16.2845, 14.8060,  ..., 18.1766, 11.5805, 15.7493],\n       device='cuda:0')\nActive learning iteration 24/50\nTrain set size: 1480, Pool set size: 38520\nTest accuracy: 0.3958\n\ntorch.Size([38520, 1480])\ntensor([16.1329, 12.1462, 12.3500,  ..., 20.0847, 14.8019, 18.0795],\n       device='cuda:0')\nActive learning iteration 25/50\nTrain set size: 1500, Pool set size: 38500\nTest accuracy: 0.3991\n\ntorch.Size([38500, 1500])\ntensor([14.5194, 17.3563, 11.2866,  ..., 17.4368, 17.8245, 15.1265],\n       device='cuda:0')\nActive learning iteration 26/50\nTrain set size: 1520, Pool set size: 38480\nTest accuracy: 0.4013\n\ntorch.Size([38480, 1520])\ntensor([ 8.8625, 14.8794, 16.4875,  ..., 15.7839, 19.6722, 18.8112],\n       device='cuda:0')\nActive learning iteration 27/50\nTrain set size: 1540, Pool set size: 38460\nTest accuracy: 0.3947\n\ntorch.Size([38460, 1540])\ntensor([15.0159, 16.4058, 11.9871,  ..., 12.9960, 13.1737, 16.7721],\n       device='cuda:0')\nActive learning iteration 28/50\nTrain set size: 1560, Pool set size: 38440\nTest accuracy: 0.4047\n\ntorch.Size([38440, 1560])\ntensor([13.5995, 16.1296, 13.3624,  ..., 17.0889, 16.3813, 17.3411],\n       device='cuda:0')\nActive learning iteration 29/50\nTrain set size: 1580, Pool set size: 38420\nTest accuracy: 0.4072\n\ntorch.Size([38420, 1580])\ntensor([12.3904, 15.0362, 17.8841,  ..., 17.0775, 18.0504, 11.0309],\n       device='cuda:0')\nActive learning iteration 30/50\nTrain set size: 1600, Pool set size: 38400\nTest accuracy: 0.4116\n\ntorch.Size([38400, 1600])\ntensor([12.9135,  9.6945, 15.8095,  ..., 17.2932, 15.5810, 16.3118],\n       device='cuda:0')\nActive learning iteration 31/50\nTrain set size: 1620, Pool set size: 38380\nTest accuracy: 0.4063\n\ntorch.Size([38380, 1620])\ntensor([15.4356, 10.6304, 16.7421,  ..., 17.1605, 11.3902, 15.2919],\n       device='cuda:0')\nActive learning iteration 32/50\nTrain set size: 1640, Pool set size: 38360\nTest accuracy: 0.4078\n\ntorch.Size([38360, 1640])\ntensor([16.1068,  9.7305, 13.4376,  ..., 17.2062, 15.9482, 16.0848],\n       device='cuda:0')\nActive learning iteration 33/50\nTrain set size: 1660, Pool set size: 38340\nTest accuracy: 0.4086\n\ntorch.Size([38340, 1660])\ntensor([15.3394, 13.6819, 14.6290,  ..., 18.6518, 14.8610, 17.4382],\n       device='cuda:0')\nActive learning iteration 34/50\nTrain set size: 1680, Pool set size: 38320\nTest accuracy: 0.4124\n\ntorch.Size([38320, 1680])\ntensor([14.2327,  9.8067, 15.7820,  ..., 16.4721, 14.0880, 16.4032],\n       device='cuda:0')\nActive learning iteration 35/50\nTrain set size: 1700, Pool set size: 38300\nTest accuracy: 0.4086\n\ntorch.Size([38300, 1700])\ntensor([14.0780,  9.1898, 14.8344,  ..., 14.7846, 16.5506, 12.5081],\n       device='cuda:0')\nActive learning iteration 36/50\nTrain set size: 1720, Pool set size: 38280\nTest accuracy: 0.4203\n\ntorch.Size([38280, 1720])\ntensor([13.1480, 14.5886, 18.0845,  ..., 12.8378, 17.5011, 10.9341],\n       device='cuda:0')\nActive learning iteration 37/50\nTrain set size: 1740, Pool set size: 38260\nTest accuracy: 0.4083\n\ntorch.Size([38260, 1740])\ntensor([14.2355, 13.3441, 16.4646,  ..., 15.7044, 17.7319, 10.2174],\n       device='cuda:0')\nActive learning iteration 38/50\nTrain set size: 1760, Pool set size: 38240\nTest accuracy: 0.4027\n\ntorch.Size([38240, 1760])\ntensor([16.9951, 10.9682, 16.2470,  ..., 15.9360, 17.6638, 13.6467],\n       device='cuda:0')\nActive learning iteration 39/50\nTrain set size: 1780, Pool set size: 38220\nTest accuracy: 0.4056\n\ntorch.Size([38220, 1780])\ntensor([16.6178, 15.3217, 19.2981,  ..., 12.2111, 17.7510, 15.2010],\n       device='cuda:0')\nActive learning iteration 40/50\nTrain set size: 1800, Pool set size: 38200\nTest accuracy: 0.4103\n\ntorch.Size([38200, 1800])\ntensor([10.2955, 12.8777, 16.5146,  ..., 14.0876, 11.8172, 14.9162],\n       device='cuda:0')\nActive learning iteration 41/50\nTrain set size: 1820, Pool set size: 38180\nTest accuracy: 0.4151\n\ntorch.Size([38180, 1820])\ntensor([11.0803, 11.2387, 15.0846,  ..., 15.4536, 13.5256, 13.8069],\n       device='cuda:0')\nActive learning iteration 42/50\nTrain set size: 1840, Pool set size: 38160\nTest accuracy: 0.4102\n\ntorch.Size([38160, 1840])\ntensor([12.8535,  9.4402, 12.2937,  ..., 16.5952,  8.8925, 15.5577],\n       device='cuda:0')\nActive learning iteration 43/50\nTrain set size: 1860, Pool set size: 38140\nTest accuracy: 0.4118\n\ntorch.Size([38140, 1860])\ntensor([13.1878, 10.5807, 10.3227,  ..., 14.9606,  8.5745, 12.8881],\n       device='cuda:0')\nActive learning iteration 44/50\nTrain set size: 1880, Pool set size: 38120\nTest accuracy: 0.4146\n\ntorch.Size([38120, 1880])\ntensor([12.5138,  9.2340, 12.3855,  ..., 15.4842,  9.8531, 14.1602],\n       device='cuda:0')\nActive learning iteration 45/50\nTrain set size: 1900, Pool set size: 38100\nTest accuracy: 0.4131\n\ntorch.Size([38100, 1900])\ntensor([ 9.8062, 12.2065, 13.1162,  ..., 15.4177,  9.6292, 12.2413],\n       device='cuda:0')\nActive learning iteration 46/50\nTrain set size: 1920, Pool set size: 38080\nTest accuracy: 0.4139\n\ntorch.Size([38080, 1920])\ntensor([11.2360, 11.6494, 14.7855,  ..., 13.6113,  9.9965, 12.6070],\n       device='cuda:0')\nActive learning iteration 47/50\nTrain set size: 1940, Pool set size: 38060\nTest accuracy: 0.4215\n\ntorch.Size([38060, 1940])\ntensor([ 9.1881, 12.8201, 10.3720,  ..., 19.3521,  9.2557, 13.7367],\n       device='cuda:0')\nActive learning iteration 48/50\nTrain set size: 1960, Pool set size: 38040\nTest accuracy: 0.4202\n\ntorch.Size([38040, 1960])\ntensor([ 9.4228,  9.1350, 12.4068,  ..., 16.4824,  8.6921, 14.4326],\n       device='cuda:0')\nActive learning iteration 49/50\nTrain set size: 1980, Pool set size: 38020\nTest accuracy: 0.4300\n\ntorch.Size([38020, 1980])\ntensor([ 8.9530, 10.3205, 14.8197,  ..., 16.3522,  8.9211, 11.4559],\n       device='cuda:0')\nActive learning iteration 50/50\nTrain set size: 2000, Pool set size: 38000\nTest accuracy: 0.4309\n\n\n\n\nfig, ax = plt.subplots(1, 1)\nax.plot(mean_acc, label=\"Random sampling (mean)\")\nax.fill_between(mean_acc.index, mean_acc-std_acc,\n                 mean_acc+std_acc, alpha=0.2, label=\"Random sampling (std)\")\nax.axhline(accuracy_only_train.cpu(), color='r', label='Only train')\nax.axhline(accuracy_train_plus_pool.cpu(), color='g', label='Train + pool')\n\npd.Series(test_acc_entropy).plot(ax=ax, label=\"Entropy sampling\", color='C2')\npd.Series(test_acc_margin).plot(ax=ax, label=\"Margin sampling\", color='C4')\npd.Series(test_acc_diversity).plot(ax=ax, label=\"Diversity sampling\", color='C5')\n\nax.set_xlabel(\"Active learning iteration\")\nax.set_ylabel(\"Test accuracy\")\nax.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fe752d57850&gt;\n\n\n\n\n\n\nbald_verbose = False\nmodel_e = copy.deepcopy(model_only_train)\n# AL loop\n\nm, p_idx, t_idx, test_acc_bald  = al_loop(model_e, BALD_sampling, num_al_iterations, 20, train_idx, pool_idx, test_idx, query_size, X, y,device=device,verbose=True, random_seed=0)\n\nInitial train size: torch.Size([1000])\nInitial pool size: torch.Size([39000])\nTest accuracy before AL: 0.3521\nActive learning iteration 1/50\nTrain set size: 1020, Pool set size: 38980\nTest accuracy: 0.3521\n\nActive learning iteration 2/50\nTrain set size: 1040, Pool set size: 38960\nTest accuracy: 0.3523\n\nActive learning iteration 3/50\nTrain set size: 1060, Pool set size: 38940\nTest accuracy: 0.3150\n\nActive learning iteration 4/50\nTrain set size: 1080, Pool set size: 38920\nTest accuracy: 0.3273\n\nActive learning iteration 5/50\nTrain set size: 1100, Pool set size: 38900\nTest accuracy: 0.3360\n\nActive learning iteration 6/50\nTrain set size: 1120, Pool set size: 38880\nTest accuracy: 0.3395\n\nActive learning iteration 7/50\nTrain set size: 1140, Pool set size: 38860\nTest accuracy: 0.3402\n\nActive learning iteration 8/50\nTrain set size: 1160, Pool set size: 38840\nTest accuracy: 0.3465\n\nActive learning iteration 9/50\nTrain set size: 1180, Pool set size: 38820\nTest accuracy: 0.3492\n\nActive learning iteration 10/50\nTrain set size: 1200, Pool set size: 38800\nTest accuracy: 0.3594\n\nActive learning iteration 11/50\nTrain set size: 1220, Pool set size: 38780\nTest accuracy: 0.3620\n\nActive learning iteration 12/50\nTrain set size: 1240, Pool set size: 38760\nTest accuracy: 0.3763\n\nActive learning iteration 13/50\nTrain set size: 1260, Pool set size: 38740\nTest accuracy: 0.3712\n\nActive learning iteration 14/50\nTrain set size: 1280, Pool set size: 38720\nTest accuracy: 0.3748\n\nActive learning iteration 15/50\nTrain set size: 1300, Pool set size: 38700\nTest accuracy: 0.3703\n\nActive learning iteration 16/50\nTrain set size: 1320, Pool set size: 38680\nTest accuracy: 0.3769\n\nActive learning iteration 17/50\nTrain set size: 1340, Pool set size: 38660\nTest accuracy: 0.3837\n\nActive learning iteration 18/50\nTrain set size: 1360, Pool set size: 38640\nTest accuracy: 0.3850\n\nActive learning iteration 19/50\nTrain set size: 1380, Pool set size: 38620\nTest accuracy: 0.3856\n\nActive learning iteration 20/50\nTrain set size: 1400, Pool set size: 38600\nTest accuracy: 0.3864\n\nActive learning iteration 21/50\nTrain set size: 1420, Pool set size: 38580\nTest accuracy: 0.3983\n\nActive learning iteration 22/50\nTrain set size: 1440, Pool set size: 38560\nTest accuracy: 0.3892\n\nActive learning iteration 23/50\nTrain set size: 1460, Pool set size: 38540\nTest accuracy: 0.3960\n\nActive learning iteration 24/50\nTrain set size: 1480, Pool set size: 38520\nTest accuracy: 0.3946\n\nActive learning iteration 25/50\nTrain set size: 1500, Pool set size: 38500\nTest accuracy: 0.4018\n\nActive learning iteration 26/50\nTrain set size: 1520, Pool set size: 38480\nTest accuracy: 0.4090\n\nActive learning iteration 27/50\nTrain set size: 1540, Pool set size: 38460\nTest accuracy: 0.4011\n\nActive learning iteration 28/50\nTrain set size: 1560, Pool set size: 38440\nTest accuracy: 0.3966\n\nActive learning iteration 29/50\nTrain set size: 1580, Pool set size: 38420\nTest accuracy: 0.3931\n\nActive learning iteration 30/50\nTrain set size: 1600, Pool set size: 38400\nTest accuracy: 0.4018\n\nActive learning iteration 31/50\nTrain set size: 1620, Pool set size: 38380\nTest accuracy: 0.4026\n\nActive learning iteration 32/50\nTrain set size: 1640, Pool set size: 38360\nTest accuracy: 0.4106\n\nActive learning iteration 33/50\nTrain set size: 1660, Pool set size: 38340\nTest accuracy: 0.4172\n\nActive learning iteration 34/50\nTrain set size: 1680, Pool set size: 38320\nTest accuracy: 0.4182\n\nActive learning iteration 35/50\nTrain set size: 1700, Pool set size: 38300\nTest accuracy: 0.4261\n\nActive learning iteration 36/50\nTrain set size: 1720, Pool set size: 38280\nTest accuracy: 0.4175\n\nActive learning iteration 37/50\nTrain set size: 1740, Pool set size: 38260\nTest accuracy: 0.4292\n\nActive learning iteration 38/50\nTrain set size: 1760, Pool set size: 38240\nTest accuracy: 0.4201\n\nActive learning iteration 39/50\nTrain set size: 1780, Pool set size: 38220\nTest accuracy: 0.4175\n\nActive learning iteration 40/50\nTrain set size: 1800, Pool set size: 38200\nTest accuracy: 0.4160\n\nActive learning iteration 41/50\nTrain set size: 1820, Pool set size: 38180\nTest accuracy: 0.4215\n\nActive learning iteration 42/50\nTrain set size: 1840, Pool set size: 38160\nTest accuracy: 0.4152\n\nActive learning iteration 43/50\nTrain set size: 1860, Pool set size: 38140\nTest accuracy: 0.4373\n\nActive learning iteration 44/50\nTrain set size: 1880, Pool set size: 38120\nTest accuracy: 0.4386\n\nActive learning iteration 45/50\nTrain set size: 1900, Pool set size: 38100\nTest accuracy: 0.4318\n\nActive learning iteration 46/50\nTrain set size: 1920, Pool set size: 38080\nTest accuracy: 0.4395\n\nActive learning iteration 47/50\nTrain set size: 1940, Pool set size: 38060\nTest accuracy: 0.4304\n\nActive learning iteration 48/50\nTrain set size: 1960, Pool set size: 38040\nTest accuracy: 0.4372\n\nActive learning iteration 49/50\nTrain set size: 1980, Pool set size: 38020\nTest accuracy: 0.4413\n\nActive learning iteration 50/50\nTrain set size: 2000, Pool set size: 38000\nTest accuracy: 0.4262\n\n\n\n\nfig, ax = plt.subplots(1, 1)\nax.plot(mean_acc, label=\"Random sampling (mean)\")\nax.fill_between(mean_acc.index, mean_acc-std_acc,\n                 mean_acc+std_acc, alpha=0.2, label=\"Random sampling (std)\")\nax.axhline(accuracy_only_train.cpu(), color='r', label='Only train')\nax.axhline(accuracy_train_plus_pool.cpu(), color='g', label='Train + pool')\n\npd.Series(test_acc_entropy).plot(ax=ax, label=\"Entropy sampling\", color='C2')\npd.Series(test_acc_margin).plot(ax=ax, label=\"Margin sampling\", color='C4')\npd.Series(test_acc_diversity).plot(ax=ax, label=\"Diversity sampling\", color='C5')\npd.Series(test_acc_bald).plot(ax=ax, label=\"BALD sampling\", color='C6')\n\nax.set_xlabel(\"Active learning iteration\")\nax.set_ylabel(\"Test accuracy\")\nax.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fe752f24ed0&gt;"
  },
  {
    "objectID": "notebooks/monte-carlo-dropout.html",
    "href": "notebooks/monte-carlo-dropout.html",
    "title": "Monte Carlo Dropout and Deep Ensemble for Bayesian Deep Learning",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "notebooks/monte-carlo-dropout.html#imports",
    "href": "notebooks/monte-carlo-dropout.html#imports",
    "title": "Monte Carlo Dropout and Deep Ensemble for Bayesian Deep Learning",
    "section": "Imports",
    "text": "Imports\n\n### Regular Dropout\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport pandas as pd\n\nimport numpy as np \nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n# Use GPU\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\ncuda:0\n\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\nInspired from: https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/Bayesian_Neural_Networks/dl2_bnn_tut2_student_with_answers.html\n\ndef get_simple_data_train():\n    x = torch.linspace(-2, 4, 300)\n    x = torch.hstack([x, torch.linspace(6, 8, 300)])\n    f = lambda x: torch.sin(x)\n    eps = torch.randn_like(x) * 0.1\n    f_x = f(x) \n    y_train = f_x + eps\n    x_train = x[:, None]\n    return x_train, y_train, f\n\n\ndef plot_generic(add_to_plot=None):\n    fig, ax = plt.subplots()\n\n    plt.xlabel(\"X\",)\n    plt.ylabel(\"Y\",)\n\n    x_train, y_train, true_func = get_simple_data_train()\n    \n    x_all = torch.linspace(-2, 8, 1000)\n\n    ax.plot(x_train, y_train, 'C0', marker='o', ms= 4, linestyle='none', alpha=0.2, label='Observations')\n    ax.plot(x_all, true_func(x_all), 'C1', linewidth=1, label=\"true function\")\n    if add_to_plot is not None:\n        add_to_plot(ax)\n\n    plt.legend(loc=4, frameon=False)\n    plt.show()\n\n\nplot_generic()\n\n\n\n\n\n# Define a simple MLP without using nn.Linear\n\ninput_dim = 1\n\nhidden_l1_dim = 100\nhidden_l2_dim = 50\n\n\ndef init_params():\n    W1 = nn.Parameter(torch.randn(input_dim, hidden_l1_dim, requires_grad=True).to(device))\n    b1 = nn.Parameter(torch.zeros(hidden_l1_dim, requires_grad=True).to(device))\n    W2 = nn.Parameter(torch.randn(hidden_l1_dim, hidden_l2_dim, requires_grad=True).to(device))\n    b2 = nn.Parameter(torch.zeros(hidden_l2_dim, requires_grad=True).to(device))\n    W3 = nn.Parameter(torch.randn(hidden_l2_dim, 1, requires_grad=True).to(device))\n    b3 = nn.Parameter(torch.zeros(1, requires_grad=True).to(device))\n    return [W1, b1, W2, b2, W3, b3]\n\n\ndef mlp(x, params, p=0.0):\n    W1, b1, W2, b2, W3, b3 = params\n    h1 = torch.tanh(x @ W1 + b1)\n    h2 = torch.tanh(h1 @ W2 + b2)\n    return (h2 @ W3 + b3).ravel()\n\n\ntrain_x, train_y, true_func = get_simple_data_train()\ntrain_x = train_x.to(device)\ntrain_y = train_y.to(device)\n\n\ntrain_x.shape\n\ntorch.Size([600, 1])\n\n\n\ntest_x = torch.linspace(-2, 8, 1000)[:, None]\ntest_x = test_x.to(device)\n\n\nparameters = init_params()\n\n\nwith torch.no_grad():\n    y_hat_untrained = mlp(test_x, parameters).ravel()\n# Detach and convert to numpy\ny_hat_untrained = y_hat_untrained.cpu().detach().numpy()\n\n\ndef plot_predictions(x_test, y_preds):\n    def add_predictions(ax):\n        ax.plot(x_test, y_preds, 'C2', label='neural net prediction')\n\n    plot_generic(add_predictions)\n\n\n# Plot the untrained model\nplot_predictions(test_x.cpu(), y_hat_untrained)"
  },
  {
    "objectID": "notebooks/monte-carlo-dropout.html#simple-mlp",
    "href": "notebooks/monte-carlo-dropout.html#simple-mlp",
    "title": "Monte Carlo Dropout and Deep Ensemble for Bayesian Deep Learning",
    "section": "Simple MLP",
    "text": "Simple MLP\n\n# Modify the train function to pass the dropout flag and dropout probability\ndef train(params, opt, fwd_func, x_train, y_train, epochs=1000, dropout=False, p=0.0):\n    for i in range(epochs):\n        y_hat = fwd_func(x_train, params, p)  # Pass the dropout flag and probability to fwd_func\n        loss = F.mse_loss(y_hat, y_train)\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n        \n        if i % 300 == 0:\n            print(f\"Epoch {i}, loss {loss.item():.3f}\")\n    return params\n\n\nparameters = init_params()\n\n# Continue with optimizer\noptimizer = torch.optim.Adam(parameters, lr=3e-4)\n\ntrained_params = train(parameters, optimizer, mlp, train_x, train_y, epochs=6000, dropout=False, p=0.0)\n\nEpoch 0, loss 20.761\nEpoch 300, loss 0.040\nEpoch 600, loss 0.015\nEpoch 900, loss 0.013\nEpoch 1200, loss 0.012\nEpoch 1500, loss 0.012\nEpoch 1800, loss 0.012\nEpoch 2100, loss 0.011\nEpoch 2400, loss 0.011\nEpoch 2700, loss 0.011\nEpoch 3000, loss 0.011\nEpoch 3300, loss 0.011\nEpoch 3600, loss 0.011\nEpoch 3900, loss 0.011\nEpoch 4200, loss 0.010\nEpoch 4500, loss 0.010\nEpoch 4800, loss 0.010\nEpoch 5100, loss 0.010\nEpoch 5400, loss 0.010\nEpoch 5700, loss 0.010\n\n\n\n# Plot the trained model\nwith torch.no_grad():\n    y_hat_trained = mlp(test_x, trained_params).ravel()\n# Detach and convert to numpy\ny_hat_trained = y_hat_trained.cpu().detach().numpy()\n\nplot_predictions(test_x.cpu(), y_hat_trained)"
  },
  {
    "objectID": "notebooks/monte-carlo-dropout.html#deep-ensemble",
    "href": "notebooks/monte-carlo-dropout.html#deep-ensemble",
    "title": "Monte Carlo Dropout and Deep Ensemble for Bayesian Deep Learning",
    "section": "Deep ensemble",
    "text": "Deep ensemble\n\n# Define a simple MLP without using nn.Linear\n\ninput_dim = 1\n\nhidden_l1_dim = 100\nhidden_l2_dim = 50\n\n\ndef init_params():\n    W1 = nn.Parameter(torch.randn(input_dim, hidden_l1_dim, requires_grad=True).to(device))\n    b1 = nn.Parameter(torch.zeros(hidden_l1_dim, requires_grad=True).to(device))\n    W2 = nn.Parameter(torch.randn(hidden_l1_dim, hidden_l2_dim, requires_grad=True).to(device))\n    b2 = nn.Parameter(torch.zeros(hidden_l2_dim, requires_grad=True).to(device))\n    W3 = nn.Parameter(torch.randn(hidden_l2_dim, 1, requires_grad=True).to(device))\n    b3 = nn.Parameter(torch.zeros(1, requires_grad=True).to(device))\n    return [W1, b1, W2, b2, W3, b3]\n\n\ndef mlp(x, params, p=0.0):\n    W1, b1, W2, b2, W3, b3 = params\n    h1 = torch.tanh(x @ W1 + b1)\n    h2 = torch.tanh(h1 @ W2 + b2)\n    return (h2 @ W3 + b3).ravel()\n\n\nn_ensembles = 5\n\nensemble_params_list = []\nfor i in range(n_ensembles):\n    print(f\"Training ensemble member {i}\")\n    parameters = init_params()\n    optimizer = torch.optim.Adam(parameters, lr=3e-4)\n    trained_params = train(parameters, optimizer, mlp, train_x, train_y, epochs=6000, dropout=False, p=0.0)\n    ensemble_params_list.append(trained_params)\n\nTraining ensemble member 0\nEpoch 0, loss 3.468\nEpoch 300, loss 0.038\nEpoch 600, loss 0.020\nEpoch 900, loss 0.014\nEpoch 1200, loss 0.012\nEpoch 1500, loss 0.011\nEpoch 1800, loss 0.011\nEpoch 2100, loss 0.010\nEpoch 2400, loss 0.010\nEpoch 2700, loss 0.010\nEpoch 3000, loss 0.010\nEpoch 3300, loss 0.010\nEpoch 3600, loss 0.010\nEpoch 3900, loss 0.010\nEpoch 4200, loss 0.010\nEpoch 4500, loss 0.010\nEpoch 4800, loss 0.010\nEpoch 5100, loss 0.010\nEpoch 5400, loss 0.009\nEpoch 5700, loss 0.009\nTraining ensemble member 1\nEpoch 0, loss 34.524\nEpoch 300, loss 0.042\nEpoch 600, loss 0.022\nEpoch 900, loss 0.015\nEpoch 1200, loss 0.014\nEpoch 1500, loss 0.013\nEpoch 1800, loss 0.013\nEpoch 2100, loss 0.013\nEpoch 2400, loss 0.013\nEpoch 2700, loss 0.012\nEpoch 3000, loss 0.012\nEpoch 3300, loss 0.012\nEpoch 3600, loss 0.012\nEpoch 3900, loss 0.012\nEpoch 4200, loss 0.012\nEpoch 4500, loss 0.012\nEpoch 4800, loss 0.012\nEpoch 5100, loss 0.011\nEpoch 5400, loss 0.011\nEpoch 5700, loss 0.011\nTraining ensemble member 2\nEpoch 0, loss 8.979\nEpoch 300, loss 0.019\nEpoch 600, loss 0.013\nEpoch 900, loss 0.011\nEpoch 1200, loss 0.011\nEpoch 1500, loss 0.010\nEpoch 1800, loss 0.010\nEpoch 2100, loss 0.010\nEpoch 2400, loss 0.010\nEpoch 2700, loss 0.010\nEpoch 3000, loss 0.010\nEpoch 3300, loss 0.010\nEpoch 3600, loss 0.010\nEpoch 3900, loss 0.010\nEpoch 4200, loss 0.010\nEpoch 4500, loss 0.010\nEpoch 4800, loss 0.010\nEpoch 5100, loss 0.010\nEpoch 5400, loss 0.010\nEpoch 5700, loss 0.010\nTraining ensemble member 3\nEpoch 0, loss 51.558\nEpoch 300, loss 0.151\nEpoch 600, loss 0.070\nEpoch 900, loss 0.019\nEpoch 1200, loss 0.013\nEpoch 1500, loss 0.012\nEpoch 1800, loss 0.011\nEpoch 2100, loss 0.011\nEpoch 2400, loss 0.010\nEpoch 2700, loss 0.010\nEpoch 3000, loss 0.010\nEpoch 3300, loss 0.010\nEpoch 3600, loss 0.010\nEpoch 3900, loss 0.010\nEpoch 4200, loss 0.010\nEpoch 4500, loss 0.010\nEpoch 4800, loss 0.010\nEpoch 5100, loss 0.009\nEpoch 5400, loss 0.009\nEpoch 5700, loss 0.009\nTraining ensemble member 4\nEpoch 0, loss 12.525\nEpoch 300, loss 0.100\nEpoch 600, loss 0.062\nEpoch 900, loss 0.021\nEpoch 1200, loss 0.015\nEpoch 1500, loss 0.013\nEpoch 1800, loss 0.012\nEpoch 2100, loss 0.011\nEpoch 2400, loss 0.010\nEpoch 2700, loss 0.010\nEpoch 3000, loss 0.010\nEpoch 3300, loss 0.010\nEpoch 3600, loss 0.010\nEpoch 3900, loss 0.010\nEpoch 4200, loss 0.010\nEpoch 4500, loss 0.010\nEpoch 4800, loss 0.010\nEpoch 5100, loss 0.010\nEpoch 5400, loss 0.010\nEpoch 5700, loss 0.010\n\n\n\n# Plot the trained model\ndef plot_deep_ensemble(x_test, y_preds_list):\n    def add_predictions(ax):\n        for i, y_preds in enumerate(y_preds_list, 1):\n            ax.plot(x_test, y_preds, f'C{i}', label=f'neural net prediction {i}')\n\n    plot_generic(add_predictions)\n\ny_preds_list = []\nfor trained_params in ensemble_params_list:\n    with torch.no_grad():\n        y_hat_trained = mlp(test_x, trained_params)\n        y_hat_trained = y_hat_trained.cpu().numpy()\n        y_preds_list.append(y_hat_trained)\n        \nplot_deep_ensemble(test_x.cpu(), y_preds_list)\n\n\n\n\n\ndef plot_deep_ensemble_uncertainty(x_test, y_preds_list):\n    def add_predictions(ax):\n        y_mean = np.array(y_preds_list).mean(axis=0)\n        y_std = np.array(y_preds_list).std(axis=0)\n        ax.plot(x_test, y_mean, f'C2', label=f'mean_prediction')\n        ax.fill_between(x_test.ravel(), y_mean - 2*y_std, y_mean + 2*y_std, alpha=0.2, color='C2', label='95% CI')\n\n    plot_generic(add_predictions)\n    \nplot_deep_ensemble_uncertainty(test_x.cpu(), y_preds_list)\n\n\n\n\n\ndef plot_just_uncertainty_deep_ensemble(x_test, y_preds_list):\n    def add_predictions(ax):\n        y_std = np.array(y_preds_list).std(axis=0)\n        ax.fill_between(x_test.ravel(), -2*y_std, 2*y_std, alpha=0.2, color='C2', label='95% CI')\n\n    plot_generic(add_predictions)\n    \nplot_just_uncertainty_deep_ensemble(test_x.cpu(), y_preds_list)"
  },
  {
    "objectID": "notebooks/monte-carlo-dropout.html#dropout-from-scratch",
    "href": "notebooks/monte-carlo-dropout.html#dropout-from-scratch",
    "title": "Monte Carlo Dropout and Deep Ensemble for Bayesian Deep Learning",
    "section": "Dropout from scratch",
    "text": "Dropout from scratch\n\n\n### Now adding dropout to the model by manually masking the activations\nW1, b1, W2, b2, W3, b3 = parameters\nh1 = torch.tanh(test_x @ W1 + b1)\nh1.shape\n\ntorch.Size([1000, 100])\n\n\n\n# probability of dropping out each neuron\np = 0.2\n\n\nmask = torch.rand_like(h1) &gt; p\nmask.shape\n\ntorch.Size([1000, 100])\n\n\n\nh1.shape\n\ntorch.Size([1000, 100])\n\n\n\nmask.sum()\n\ntensor(79933, device='cuda:0')\n\n\n\nmask.numel()\n\n100000\n\n\n\nmasked_activations = (h1 * mask)/(1-p)\nmasked_activations.shape\n\ntorch.Size([1000, 100])\n\n\n\nm = nn.Dropout(p=0.2)\nout_h = m(h1)\n\n\npd.DataFrame(out_h.cpu().detach().numpy()).head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n\n\n\n\n0\n0.280216\n-0.000000\n-0.372197\n-0.000000\n0.000000\n0.346530\n0.000000\n-0.000000\n-0.000000\n1.226020\n...\n-0.667853\n0.510881\n1.116930\n-1.071659\n0.000000\n-1.239488\n0.782513\n-0.000000\n-1.115131\n0.299465\n\n\n1\n0.278635\n-1.232603\n-0.370602\n-1.182943\n1.102740\n0.000000\n1.181993\n-0.631225\n-0.738800\n1.225449\n...\n-0.665564\n0.508894\n0.000000\n-1.069398\n0.000000\n-1.239203\n0.779832\n-0.965390\n-1.113328\n0.298324\n\n\n2\n0.277052\n-0.000000\n-0.369005\n-1.181741\n0.000000\n0.343636\n1.180778\n-0.628681\n-0.735845\n1.224865\n...\n-0.663269\n0.506903\n1.113202\n-1.067112\n1.168637\n-0.000000\n0.777140\n-0.962910\n-1.111502\n0.000000\n\n\n3\n0.275469\n-1.231721\n-0.367408\n-1.180517\n1.098915\n0.000000\n1.179542\n-0.626130\n-0.732877\n1.224268\n...\n-0.000000\n0.504909\n1.111301\n-0.000000\n1.167272\n-1.238610\n0.000000\n-0.960412\n-1.109653\n0.296039\n\n\n4\n0.273885\n-1.231263\n-0.000000\n-1.179272\n1.096968\n0.340738\n1.178285\n-0.623572\n-0.729897\n1.223656\n...\n-0.658661\n0.000000\n1.109375\n-1.062458\n1.165885\n-0.000000\n0.771719\n-0.957894\n-1.107781\n0.294896\n\n\n\n\n5 rows × 100 columns\n\n\n\n\npd.DataFrame(h1.cpu().detach().numpy()/(1-p)).head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n\n\n\n\n0\n0.280216\n-1.233028\n-0.372197\n-1.184125\n1.104618\n0.346530\n1.183187\n-0.633762\n-0.741743\n1.226020\n...\n-0.667853\n0.510881\n1.116930\n-1.071659\n1.171302\n-1.239488\n0.782513\n-0.967852\n-1.115131\n0.299465\n\n\n1\n0.278635\n-1.232603\n-0.370602\n-1.182943\n1.102740\n0.345083\n1.181993\n-0.631225\n-0.738800\n1.225449\n...\n-0.665564\n0.508894\n1.115078\n-1.069398\n1.169980\n-1.239203\n0.779832\n-0.965390\n-1.113328\n0.298324\n\n\n2\n0.277052\n-1.232167\n-0.369005\n-1.181740\n1.100839\n0.343636\n1.180778\n-0.628681\n-0.735845\n1.224865\n...\n-0.663269\n0.506903\n1.113202\n-1.067111\n1.168637\n-1.238910\n0.777139\n-0.962910\n-1.111502\n0.297182\n\n\n3\n0.275469\n-1.231721\n-0.367408\n-1.180517\n1.098915\n0.342187\n1.179542\n-0.626130\n-0.732877\n1.224268\n...\n-0.660968\n0.504909\n1.111301\n-1.064798\n1.167272\n-1.238610\n0.774435\n-0.960412\n-1.109653\n0.296039\n\n\n4\n0.273885\n-1.231263\n-0.365808\n-1.179272\n1.096968\n0.340738\n1.178285\n-0.623572\n-0.729897\n1.223656\n...\n-0.658661\n0.502913\n1.109375\n-1.062458\n1.165885\n-1.238301\n0.771719\n-0.957894\n-1.107781\n0.294896\n\n\n\n\n5 rows × 100 columns\n\n\n\n\npd.DataFrame(masked_activations.cpu().detach().numpy()).head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n\n\n\n\n0\n0.224173\n-0.986423\n-0.297758\n-0.947300\n0.883695\n0.277224\n0.946549\n-0.507009\n-0.593395\n0.980816\n...\n-0.000000\n0.408705\n0.000000\n-0.857327\n0.937041\n-0.991591\n0.626010\n-0.000000\n-0.892105\n0.239572\n\n\n1\n0.222908\n-0.000000\n-0.296482\n-0.946354\n0.882192\n0.276067\n0.945594\n-0.504980\n-0.000000\n0.980359\n...\n-0.000000\n0.000000\n0.000000\n-0.000000\n0.935984\n-0.991363\n0.623866\n-0.000000\n-0.890662\n0.238659\n\n\n2\n0.221642\n-0.000000\n-0.000000\n-0.000000\n0.000000\n0.274909\n0.944622\n-0.502945\n-0.588676\n0.979892\n...\n-0.530616\n0.000000\n0.890561\n-0.853689\n0.000000\n-0.991128\n0.621712\n-0.000000\n-0.889202\n0.237745\n\n\n3\n0.220375\n-0.985376\n-0.293926\n-0.944414\n0.000000\n0.273750\n0.943634\n-0.000000\n-0.586302\n0.979414\n...\n-0.528775\n0.403928\n0.000000\n-0.851838\n0.933818\n-0.990888\n0.619548\n-0.768329\n-0.887722\n0.236831\n\n\n4\n0.000000\n-0.985010\n-0.292647\n-0.000000\n0.877574\n0.272590\n0.942628\n-0.498858\n-0.583917\n0.978925\n...\n-0.526929\n0.000000\n0.000000\n-0.849966\n0.932708\n-0.990641\n0.617375\n-0.000000\n-0.886225\n0.235917\n\n\n\n\n5 rows × 100 columns\n\n\n\n\nmask_pytorch = h1!=out_h*(1-p)\nmask_pytorch\n\ntensor([[ True,  True, False,  ...,  True,  True,  True],\n        [ True,  True, False,  ..., False, False, False],\n        [False,  True, False,  ..., False, False,  True],\n        ...,\n        [False, False, False,  ...,  True, False, False],\n        [ True, False, False,  ...,  True,  True, False],\n        [ True, False,  True,  ...,  True, False,  True]], device='cuda:0')\n\n\n\npd.DataFrame(mask.cpu().detach().numpy()).head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n\n\n\n\n0\nFalse\nTrue\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\nFalse\nTrue\n...\nTrue\nTrue\nTrue\nTrue\nFalse\nTrue\nTrue\nTrue\nFalse\nTrue\n\n\n1\nTrue\nTrue\nTrue\nFalse\nTrue\nTrue\nTrue\nTrue\nFalse\nFalse\n...\nTrue\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\n\n\n2\nTrue\nTrue\nTrue\nTrue\nFalse\nTrue\nTrue\nTrue\nFalse\nTrue\n...\nTrue\nTrue\nTrue\nFalse\nFalse\nTrue\nTrue\nFalse\nTrue\nTrue\n\n\n3\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nFalse\nTrue\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n4\nFalse\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n...\nTrue\nTrue\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n\n\n5 rows × 100 columns"
  },
  {
    "objectID": "notebooks/monte-carlo-dropout.html#mc-dropout",
    "href": "notebooks/monte-carlo-dropout.html#mc-dropout",
    "title": "Monte Carlo Dropout and Deep Ensemble for Bayesian Deep Learning",
    "section": "MC-dropout",
    "text": "MC-dropout\n\n### Rewriting the model with dropout\n\ndef mlp_dropout(x, params, p=0.0, training=True):\n    W1, b1, W2, b2, W3, b3 = params\n    h1 = torch.tanh(x @ W1 + b1)\n    #h2 = torch.sin(h1 @ W2 + b2)\n    #h3 = h2 @ W3 + b3\n    #return h3\n    if training:\n        # probability of dropping out each neuron\n        mask = torch.rand_like(h1) &gt; p\n        h1 = h1 * mask\n        # scale activations to account for dropout\n        h1 = h1 / (1 - p)\n    h2 = torch.tanh(h1 @ W2 + b2)\n    if training:\n        # probability of dropping out each neuron\n        mask = torch.rand_like(h2) &gt; p\n        h2 = h2 * mask\n        # scale activations to account for dropout\n        h2 = h2 / (1 - p)\n    h3 = h2 @ W3 + b3\n    return h3.ravel()\n\n\n# Train the model\np = 0.1\nparameters = init_params()\n\n# Continue with optimizer\noptimizer = torch.optim.Adam(parameters, lr=3e-4)\ntrained_params = train(parameters, optimizer, mlp_dropout, train_x, train_y, epochs=10000, dropout=True, p=p)\n\nEpoch 0, loss 44.786\nEpoch 300, loss 9.598\nEpoch 600, loss 7.986\nEpoch 900, loss 6.820\nEpoch 1200, loss 4.998\nEpoch 1500, loss 5.564\nEpoch 1800, loss 4.685\nEpoch 2100, loss 4.184\nEpoch 2400, loss 3.587\nEpoch 2700, loss 3.388\nEpoch 3000, loss 3.114\nEpoch 3300, loss 2.926\nEpoch 3600, loss 2.245\nEpoch 3900, loss 2.271\nEpoch 4200, loss 1.819\nEpoch 4500, loss 1.889\nEpoch 4800, loss 1.622\nEpoch 5100, loss 1.395\nEpoch 5400, loss 1.058\nEpoch 5700, loss 0.993\nEpoch 6000, loss 0.838\nEpoch 6300, loss 0.794\nEpoch 6600, loss 0.623\nEpoch 6900, loss 0.541\nEpoch 7200, loss 0.531\nEpoch 7500, loss 0.395\nEpoch 7800, loss 0.387\nEpoch 8100, loss 0.318\nEpoch 8400, loss 0.274\nEpoch 8700, loss 0.270\nEpoch 9000, loss 0.222\nEpoch 9300, loss 0.223\nEpoch 9600, loss 0.210\nEpoch 9900, loss 0.194\n\n\n\n# Predictions with dropout\nwith torch.no_grad():\n    y_hat_dropout = mlp_dropout(test_x, trained_params, training=False).ravel()\n\n# Detach and convert to numpy\ny_hat_dropout = y_hat_dropout.cpu().detach().numpy()\n\nplot_predictions(test_x.cpu(), y_hat_dropout)\n\n    \n\n\n\n\n\n# Get the predictions for the test set with dropout set to True\n\npreds = []\nfor i in range(100):\n    with torch.no_grad():\n        y_hat_dropout = mlp_dropout(test_x, trained_params, training=True, p=p).ravel()\n    # Detach and convert to numpy\n    y_hat_dropout = y_hat_dropout.cpu().detach().numpy()\n    preds.append(y_hat_dropout)\n\n\n# Plot MC dropout predictions\n\npreds = np.array(preds)\npreds.shape\n\n(100, 1000)\n\n\n\n# Plot mean and variance of MC dropout predictions\n\nmean = preds.mean(axis=0)\nstd = preds.std(axis=0)\n\ndef plot_predictions_with_uncertainty(x_test, y_preds, y_std):\n    def add_predictions(ax):\n        ax.plot(x_test, y_preds, 'C2', label='neural net prediction')\n        ax.fill_between(x_test.ravel(), y_preds - y_std, y_preds + y_std, alpha=0.2, color='C2', label='uncertainty')\n\n    plot_generic(add_predictions)\n\nplot_predictions_with_uncertainty(test_x.cpu(), mean, std)\n\n\n\n\n\n# Just plot the uncertainty\n\ndef plot_uncertainty(x_test, y_std):\n    def add_predictions(ax):\n        ax.fill_between(x_test.ravel(), -y_std, y_std, alpha=0.2, color='C2', label='uncertainty')\n\n    plot_generic(add_predictions)\n    \nplot_uncertainty(test_x.cpu(), std)"
  },
  {
    "objectID": "notebooks/monte-carlo-dropout.html#mlp-with-residual-connections",
    "href": "notebooks/monte-carlo-dropout.html#mlp-with-residual-connections",
    "title": "Monte Carlo Dropout and Deep Ensemble for Bayesian Deep Learning",
    "section": "MLP with residual connections",
    "text": "MLP with residual connections\n\nWe need to have same number of neurons in all layers to form a residual connection.\n\n\n# Define a simple MLP without using nn.Linear\n\ninput_dim = 1\n\nhidden_l1_dim = 50\nhidden_l2_dim = 50\n\n\ndef init_params():\n    W1 = nn.Parameter(torch.randn(input_dim, hidden_l1_dim, requires_grad=True).to(device))\n    b1 = nn.Parameter(torch.zeros(hidden_l1_dim, requires_grad=True).to(device))\n    W2 = nn.Parameter(torch.randn(hidden_l1_dim, hidden_l2_dim, requires_grad=True).to(device))\n    b2 = nn.Parameter(torch.zeros(hidden_l2_dim, requires_grad=True).to(device))\n    W3 = nn.Parameter(torch.randn(hidden_l2_dim, 1, requires_grad=True).to(device))\n    b3 = nn.Parameter(torch.zeros(1, requires_grad=True).to(device))\n    return [W1, b1, W2, b2, W3, b3]\n\n\ndef mlp(x, params, p=0.0):\n    W1, b1, W2, b2, W3, b3 = params\n    h1 = torch.tanh(x @ W1 + b1)\n    h2 = torch.tanh(h1 @ W2 + b2)\n    # residual connection\n    h2 = h2 + h1\n    return (h2 @ W3 + b3).ravel()\n\n\nparameters = init_params()\n\n# Continue with optimizer\noptimizer = torch.optim.Adam(parameters, lr=3e-4)\n\ntrained_params = train(parameters, optimizer, mlp, train_x, train_y, epochs=6000, dropout=False, p=0.0)\n\nEpoch 0, loss 5.608\nEpoch 300, loss 0.153\nEpoch 600, loss 0.050\nEpoch 900, loss 0.030\nEpoch 1200, loss 0.018\nEpoch 1500, loss 0.016\nEpoch 1800, loss 0.015\nEpoch 2100, loss 0.014\nEpoch 2400, loss 0.014\nEpoch 2700, loss 0.013\nEpoch 3000, loss 0.013\nEpoch 3300, loss 0.013\nEpoch 3600, loss 0.012\nEpoch 3900, loss 0.012\nEpoch 4200, loss 0.011\nEpoch 4500, loss 0.010\nEpoch 4800, loss 0.010\nEpoch 5100, loss 0.010\nEpoch 5400, loss 0.010\nEpoch 5700, loss 0.009\n\n\n\n# Plot the trained model\nwith torch.no_grad():\n    y_hat_trained = mlp(test_x, trained_params).ravel()\n# Detach and convert to numpy\ny_hat_trained = y_hat_trained.cpu().detach().numpy()\n\nplot_predictions(test_x.cpu(), y_hat_trained)"
  },
  {
    "objectID": "notebooks/monte-carlo-dropout.html#deep-ensemble-with-residual-connections",
    "href": "notebooks/monte-carlo-dropout.html#deep-ensemble-with-residual-connections",
    "title": "Monte Carlo Dropout and Deep Ensemble for Bayesian Deep Learning",
    "section": "Deep ensemble with residual connections",
    "text": "Deep ensemble with residual connections\n\nn_ensembles = 5\n\nensemble_params_list = []\nfor i in range(n_ensembles):\n    print(f\"Training ensemble member {i}\")\n    parameters = init_params()\n    optimizer = torch.optim.Adam(parameters, lr=3e-4)\n    trained_params = train(parameters, optimizer, mlp, train_x, train_y, epochs=6000, dropout=False, p=0.0)\n    ensemble_params_list.append(trained_params)\n\nTraining ensemble member 0\nEpoch 0, loss 2.491\nEpoch 300, loss 0.038\nEpoch 600, loss 0.018\nEpoch 900, loss 0.015\nEpoch 1200, loss 0.015\nEpoch 1500, loss 0.014\nEpoch 1800, loss 0.014\nEpoch 2100, loss 0.013\nEpoch 2400, loss 0.013\nEpoch 2700, loss 0.013\nEpoch 3000, loss 0.013\nEpoch 3300, loss 0.012\nEpoch 3600, loss 0.012\nEpoch 3900, loss 0.012\nEpoch 4200, loss 0.011\nEpoch 4500, loss 0.011\nEpoch 4800, loss 0.010\nEpoch 5100, loss 0.010\nEpoch 5400, loss 0.010\nEpoch 5700, loss 0.010\nTraining ensemble member 1\nEpoch 0, loss 1.597\nEpoch 300, loss 0.038\nEpoch 600, loss 0.020\nEpoch 900, loss 0.017\nEpoch 1200, loss 0.016\nEpoch 1500, loss 0.015\nEpoch 1800, loss 0.015\nEpoch 2100, loss 0.014\nEpoch 2400, loss 0.014\nEpoch 2700, loss 0.013\nEpoch 3000, loss 0.013\nEpoch 3300, loss 0.012\nEpoch 3600, loss 0.011\nEpoch 3900, loss 0.011\nEpoch 4200, loss 0.010\nEpoch 4500, loss 0.010\nEpoch 4800, loss 0.009\nEpoch 5100, loss 0.009\nEpoch 5400, loss 0.009\nEpoch 5700, loss 0.009\nTraining ensemble member 2\nEpoch 0, loss 5.152\nEpoch 300, loss 0.040\nEpoch 600, loss 0.018\nEpoch 900, loss 0.016\nEpoch 1200, loss 0.014\nEpoch 1500, loss 0.014\nEpoch 1800, loss 0.013\nEpoch 2100, loss 0.012\nEpoch 2400, loss 0.012\nEpoch 2700, loss 0.011\nEpoch 3000, loss 0.011\nEpoch 3300, loss 0.010\nEpoch 3600, loss 0.010\nEpoch 3900, loss 0.010\nEpoch 4200, loss 0.009\nEpoch 4500, loss 0.009\nEpoch 4800, loss 0.009\nEpoch 5100, loss 0.009\nEpoch 5400, loss 0.009\nEpoch 5700, loss 0.009\nTraining ensemble member 3\nEpoch 0, loss 14.183\nEpoch 300, loss 0.090\nEpoch 600, loss 0.053\nEpoch 900, loss 0.036\nEpoch 1200, loss 0.025\nEpoch 1500, loss 0.019\nEpoch 1800, loss 0.015\nEpoch 2100, loss 0.014\nEpoch 2400, loss 0.013\nEpoch 2700, loss 0.012\nEpoch 3000, loss 0.012\nEpoch 3300, loss 0.012\nEpoch 3600, loss 0.011\nEpoch 3900, loss 0.011\nEpoch 4200, loss 0.011\nEpoch 4500, loss 0.011\nEpoch 4800, loss 0.010\nEpoch 5100, loss 0.010\nEpoch 5400, loss 0.010\nEpoch 5700, loss 0.010\nTraining ensemble member 4\nEpoch 0, loss 72.144\nEpoch 300, loss 0.123\nEpoch 600, loss 0.054\nEpoch 900, loss 0.025\nEpoch 1200, loss 0.015\nEpoch 1500, loss 0.014\nEpoch 1800, loss 0.013\nEpoch 2100, loss 0.013\nEpoch 2400, loss 0.013\nEpoch 2700, loss 0.013\nEpoch 3000, loss 0.013\nEpoch 3300, loss 0.013\nEpoch 3600, loss 0.013\nEpoch 3900, loss 0.013\nEpoch 4200, loss 0.012\nEpoch 4500, loss 0.012\nEpoch 4800, loss 0.012\nEpoch 5100, loss 0.012\nEpoch 5400, loss 0.011\nEpoch 5700, loss 0.011\n\n\n\ny_preds_list = []\nfor trained_params in ensemble_params_list:\n    with torch.no_grad():\n        y_hat_trained = mlp(test_x, trained_params)\n        y_hat_trained = y_hat_trained.cpu().numpy()\n        y_preds_list.append(y_hat_trained)\n        \nplot_deep_ensemble(test_x.cpu(), y_preds_list)\n\n\n\n\n\nplot_deep_ensemble_uncertainty(test_x.cpu(), y_preds_list)\n\n\n\n\n\nplot_just_uncertainty_deep_ensemble(test_x.cpu(), y_preds_list)"
  },
  {
    "objectID": "notebooks/monte-carlo-dropout.html#mc-dropout-with-residual-connections",
    "href": "notebooks/monte-carlo-dropout.html#mc-dropout-with-residual-connections",
    "title": "Monte Carlo Dropout and Deep Ensemble for Bayesian Deep Learning",
    "section": "MC-dropout with residual connections",
    "text": "MC-dropout with residual connections\n\n### Rewriting the model with dropout\n\ninput_dim = 1\n\nhidden_l1_dim = 30\nhidden_l2_dim = 30\n\ndef init_params():\n    W1 = nn.Parameter(torch.randn(input_dim, hidden_l1_dim, requires_grad=True).to(device))\n    b1 = nn.Parameter(torch.zeros(hidden_l1_dim, requires_grad=True).to(device))\n    W2 = nn.Parameter(torch.randn(hidden_l1_dim, hidden_l2_dim, requires_grad=True).to(device))\n    b2 = nn.Parameter(torch.zeros(hidden_l2_dim, requires_grad=True).to(device))\n    W3 = nn.Parameter(torch.randn(hidden_l2_dim, 1, requires_grad=True).to(device))\n    b3 = nn.Parameter(torch.zeros(1, requires_grad=True).to(device))\n    return [W1, b1, W2, b2, W3, b3]\n\ndef mlp_dropout(x, params, p=0.0, training=True):\n    W1, b1, W2, b2, W3, b3 = params\n    h1 = torch.tanh(x @ W1 + b1)\n    #####################\n    # We don't want to apply dropout to the input layer\n    #####################\n    # if training:\n        # probability of dropping out each neuron\n        # mask = torch.rand_like(h1) &gt; p\n        # h1 = h1 * mask\n        # scale activations to account for dropout\n        #h1 = h1 / (1 - p)\n    h2 = torch.tanh(h1 @ W2 + b2)\n    if training:\n        # probability of dropping out each neuron\n        mask = torch.rand_like(h2) &gt; p\n        h2 = h2 * mask\n        # scale activations to account for dropout\n        h2 = h2 / (1 - p)\n    # add residual connection\n    h2 = h2 + h1\n    h3 = h2 @ W3 + b3\n    return h3.ravel()\n\n\n# Train the model\np = 0.5\nparameters = init_params()\n\n# Continue with optimizer\noptimizer = torch.optim.Adam(parameters, lr=1e-3)\ntrained_params = train(parameters, optimizer, mlp_dropout, train_x, train_y, epochs=6000, dropout=True, p=p)\n\nEpoch 0, loss 35.309\nEpoch 300, loss 3.127\nEpoch 600, loss 0.516\nEpoch 900, loss 0.224\nEpoch 1200, loss 0.117\nEpoch 1500, loss 0.100\nEpoch 1800, loss 0.080\nEpoch 2100, loss 0.062\nEpoch 2400, loss 0.053\nEpoch 2700, loss 0.049\nEpoch 3000, loss 0.047\nEpoch 3300, loss 0.047\nEpoch 3600, loss 0.039\nEpoch 3900, loss 0.037\nEpoch 4200, loss 0.034\nEpoch 4500, loss 0.039\nEpoch 4800, loss 0.035\nEpoch 5100, loss 0.033\nEpoch 5400, loss 0.033\nEpoch 5700, loss 0.039\n\n\n\n# Predictions with dropout\nwith torch.no_grad():\n    y_hat_dropout = mlp_dropout(test_x, trained_params, training=False).ravel()\n\n# Detach and convert to numpy\ny_hat_dropout = y_hat_dropout.cpu().detach().numpy()\n\nplot_predictions(test_x.cpu(), y_hat_dropout)\n\n\n\n\n\n# Get the predictions for the test set with dropout set to True\n\npreds = []\nfor i in range(100):\n    with torch.no_grad():\n        y_hat_dropout = mlp_dropout(test_x, trained_params, training=True, p=p).ravel()\n    # Detach and convert to numpy\n    y_hat_dropout = y_hat_dropout.cpu().detach().numpy()\n    preds.append(y_hat_dropout)\n\npreds = np.array(preds)\npreds.shape\n\n(100, 1000)\n\n\n\n# Plot mean and variance of MC dropout predictions\n\nmean = preds.mean(axis=0)\nstd = preds.std(axis=0)\n\ndef plot_predictions_with_uncertainty(x_test, y_preds, y_std):\n    def add_predictions(ax):\n        ax.plot(x_test, y_preds, 'C2', label='neural net prediction')\n        ax.fill_between(x_test.ravel(), y_preds - y_std, y_preds + y_std, alpha=0.2, color='C2', label='uncertainty')\n\n    plot_generic(add_predictions)\n\nplot_predictions_with_uncertainty(test_x.cpu(), mean, std)\n\n\n\n\n\n# Just plot the uncertainty\n\ndef plot_uncertainty(x_test, y_std):\n    def add_predictions(ax):\n        ax.fill_between(x_test.ravel(), -y_std, y_std, alpha=0.2, color='C2', label='uncertainty')\n\n    plot_generic(add_predictions)\n    \nplot_uncertainty(test_x.cpu(), std)"
  },
  {
    "objectID": "notebooks/sampling/prediction_var_mc_pi.html",
    "href": "notebooks/sampling/prediction_var_mc_pi.html",
    "title": "Variance in Value of Pi in MC Sampling",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n# Retina display\n%config InlineBackend.figure_format = 'retina'\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\n\ndef approximate_pi(num_samples, rng):\n    torch.manual_seed(rng)\n    points = torch.rand(num_samples, 2)\n    inside_circle = torch.sum(torch.norm(points, dim=1) &lt;= 1)\n    pi_estimate = (inside_circle / num_samples) * 4\n    return pi_estimate.item()\n\n\nnum_samples = 5000\nnum_runs = 100\n\nnum_samples_list = [10, 50, 100, 500, 1000, 5000]\npi_estimates = []\nfor i in range(num_runs):\n    pi_estimates.append([approximate_pi(num_samples, i)\n                        for num_samples in num_samples_list])\npi_estimates = np.array(pi_estimates)\npi_mean = np.mean(pi_estimates, axis=0)\npi_variance = np.var(pi_estimates, axis=0)\n\n\nfor i in range(num_runs):\n    plt.plot(num_samples_list, pi_estimates[i])\nplt.plot(num_samples_list, pi_mean, color='k', label='Mean Estimate')\nplt.fill_between(num_samples_list, pi_mean - np.sqrt(pi_variance), pi_mean +\n                 np.sqrt(pi_variance), color='tab:blue', alpha=0.1, label='$\\pm$ 1 SD')\nplt.fill_between(num_samples_list, pi_mean - 2 * np.sqrt(pi_variance), pi_mean +\n                 2*np.sqrt(pi_variance), color='tab:blue', alpha=0.2, label='$\\pm$ 2 SD')\nplt.fill_between(num_samples_list, pi_mean - 3 * np.sqrt(pi_variance), pi_mean +\n                 3*np.sqrt(pi_variance), color='tab:blue', alpha=0.4, label='$\\pm$ 3 SD')\nplt.fill_between(num_samples_list, pi_mean - 4 * np.sqrt(pi_variance), pi_mean +\n                 4*np.sqrt(pi_variance), color='tab:blue', alpha=0.6, label='$\\pm$ 4 SD')\nplt.axhline(y=np.pi, color='r', linestyle='--', label='True π')\nplt.xlabel('Number of Samples')\nplt.ylabel('Value')\nplt.title('Mean Estimate of π and Variance across Seeds')\nplt.legend()\n\nplt.tight_layout()\n# plt.savefig('figures/sampling/prediction_var_mc_pi.pdf', transparent=True)\n\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed"
  },
  {
    "objectID": "notebooks/sampling/importance_sampling_var.html",
    "href": "notebooks/sampling/importance_sampling_var.html",
    "title": "Importance Sampling",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n# Retina display\n%config InlineBackend.figure_format = 'retina'\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\n\nmu1 = 0\nsigma1 = 0.25\n\nmu2 = 0.5\nsigma2 = 0.5\n\n\nx_range = torch.linspace(-6, 6, 1000)\npdf1 = torch.exp(-0.5 * ((x_range - mu1) / sigma1)**2) / \\\n    (sigma1 * (2 * torch.tensor(np.pi).sqrt()))\npdf2 = torch.exp(-0.5 * ((x_range - mu2) / sigma2)**2) / \\\n    (sigma2 * (2 * torch.tensor(np.pi).sqrt()))\nratio_pdf = pdf1 / pdf2\n\n\nplt.plot(x_range, pdf1.numpy(), label='p(x)')\nplt.plot(x_range, pdf2.numpy(), label='q(x)')\nplt.plot(x_range, ratio_pdf.numpy(), label='w(x)')\nplt.xlim(-2, 2)\nplt.title('Weight function w(x)')\nplt.xlabel('X')\nplt.ylabel('w(x)')\nplt.legend()\nplt.grid()\n# plt.savefig('figures/sampling/importance_sampling_weight_function.pdf', dpi=300, transparent=True)\n\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed"
  },
  {
    "objectID": "notebooks/biased-mle-normal.html",
    "href": "notebooks/biased-mle-normal.html",
    "title": "Biased and Unbiased Estimators",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n%matplotlib inline\n\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\n\ndist = torch.distributions.Normal(0, 1)\n\n# Generate data\ndata = dist.sample((100,))\n\n# Plot data\n_ = sns.displot(data, kde=True)\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/seaborn/axisgrid.py:88: UserWarning: This figure was using constrained_layout, but that is incompatible with subplots_adjust and/or tight_layout; disabling constrained_layout.\n  self._figure.tight_layout(*args, **kwargs)\n\n\n\n\n\n\ntorch.std??\n\nDocstring:\nstd(input, dim=None, *, correction=1, keepdim=False, out=None) -&gt; Tensor\n\nCalculates the standard deviation over the dimensions specified by :attr:`dim`.\n:attr:`dim` can be a single dimension, list of dimensions, or ``None`` to\nreduce over all dimensions.\n\nThe standard deviation (:math:`\\sigma`) is calculated as\n\n.. math:: \\sigma = \\sqrt{\\frac{1}{N - \\delta N}\\sum_{i=0}^{N-1}(x_i-\\bar{x})^2}\n\nwhere :math:`x` is the sample set of elements, :math:`\\bar{x}` is the\nsample mean, :math:`N` is the number of samples and :math:`\\delta N` is\nthe :attr:`correction`.\n\n\n\nIf :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1.\nOtherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the\noutput tensor having 1 (or ``len(dim)``) fewer dimension(s).\n\n\nArgs:\n    input (Tensor): the input tensor.\n    dim (int or tuple of ints): the dimension or dimensions to reduce.\n\nKeyword args:\n    correction (int): difference between the sample size and sample degrees of freedom.\n        Defaults to `Bessel's correction`_, ``correction=1``.\n\n        .. versionchanged:: 2.0\n            Previously this argument was called ``unbiased`` and was a boolean\n            with ``True`` corresponding to ``correction=1`` and ``False`` being\n            ``correction=0``.\n    keepdim (bool): whether the output tensor has :attr:`dim` retained or not.\n    out (Tensor, optional): the output tensor.\n\nExample:\n\n    &gt;&gt;&gt; a = torch.tensor(\n    ...     [[ 0.2035,  1.2959,  1.8101, -0.4644],\n    ...      [ 1.5027, -0.3270,  0.5905,  0.6538],\n    ...      [-1.5745,  1.3330, -0.5596, -0.6548],\n    ...      [ 0.1264, -0.5080,  1.6420,  0.1992]])\n    &gt;&gt;&gt; torch.std(a, dim=1, keepdim=True)\n    tensor([[1.0311],\n            [0.7477],\n            [1.2204],\n            [0.9087]])\n\n.. _Bessel's correction: https://en.wikipedia.org/wiki/Bessel%27s_correction\nType:      builtin_function_or_method\n\n\n\nnp.std(data.numpy()), pd.Series(data.numpy()).std(), torch.std(data), torch.std(data, correction=0)\n\n(1.0807172, 1.0861616, tensor(1.0862), tensor(1.0807))\n\n\n\n# Population\nnorm = torch.distributions.Normal(0, 1)\nxs = torch.linspace(-10, 10, 1000)\nys = torch.exp(norm.log_prob(xs))\nplt.plot(xs, ys)\nplt.title('Population Distribution')\nplt.xlabel('x')\nplt.ylabel('p(x)')\nplt.savefig('../figures/mle/population-dist.pdf')\n\npopulation = norm.sample((100000,))\nplt.scatter(population, torch.zeros_like(population),  marker='|', alpha=0.1)\nplt.savefig('../figures/mle/population.pdf')\n\n\n\n\n\nplt.plot(xs, ys)\nsample_1 = population[torch.randperm(population.size(0))[:10]]\nsample_2 = population[torch.randperm(population.size(0))[:10]]\nplt.scatter(sample_1, torch.zeros_like(sample_1),label='sample 1')\nplt.scatter(sample_2, torch.zeros_like(sample_2),  label='sample 2')\nplt.legend()\nplt.savefig('../figures/mle/sample.pdf')\n\n\n\n\n\nnorm = torch.distributions.Normal(0, 1)\npopulation = norm.sample((100000,))\n\ndef plot_fit(seed, num_samples):\n    torch.manual_seed(seed)\n    # Select a random sample of size num_samples from the population\n    data = population[torch.randperm(population.shape[0])[:num_samples]]\n    mu = data.mean()\n    sigma_2 = data.var(correction=0)\n\n    # Plot data scatter\n    plt.scatter(data, torch.zeros_like(data), color='black', marker='x', zorder=10, label='samples')\n    # Plot true distribution\n    x = torch.linspace(-3, 3, 100)\n    plt.plot(x, norm.log_prob(x).exp(), color='black', label='true distribution')\n    # Plot estimated distribution\n    est = torch.distributions.Normal(mu, sigma_2.sqrt())\n    plt.plot(x, est.log_prob(x).exp(), color='red', label='estimated distribution')\n    plt.legend()\n    plt.title(f\"Sample size: {num_samples}\\n\" +fr\"Sample parameters: $\\hat{{\\mu}}={mu:0.02f}$, $\\hat{{\\sigma^2}}={sigma_2:0.02f}$\")\n    plt.ylim(-0.1, 1.2)\n    plt.xlabel(\"$x$\")\n    plt.ylabel(\"$p(x)$\")\n    plt.savefig(f\"../figures/mle/biased-mle-normal-{num_samples}-{seed}.pdf\", bbox_inches='tight')\n    return mu, sigma_2\n\n\nN_samples = 100\nmus = {}\nsigmas = {}\nfor draw in [3, 4, 5, 10, 100]:\n    mus[draw] = torch.zeros(N_samples)\n    sigmas[draw] = torch.zeros(N_samples)\n    for i in range(N_samples):\n        plt.clf()\n        mus[draw][i], sigmas[draw][i] = plot_fit(i, draw)\n\n\n\n\n\nfor draw in [3, 4, 5, 10, 100]:\n    plt.clf()\n    plt.scatter(mus[draw], sigmas[draw])\n    plt.axhline(y=1, color='k', linestyle='--', label=r'$\\sigma^2$')\n    plt.axvline(x=0, color='g', linestyle='-.', label=r'$\\mu$')\n    plt.xlabel(r'$\\hat{\\mu}$')\n    plt.ylabel(r'$\\hat{\\sigma^2}$')\n    plt.legend()\n    plt.title(f'Sample size={draw}\\n'+ fr'$E[\\hat{{\\mu}}] = {mus[draw].mean():0.2f}$ $E[\\hat{{\\sigma^2}}] = {sigmas[draw].mean():0.2f}$ ')\n    plt.savefig(f\"../figures/mle/biased-mle-normal-scatter-{draw}.pdf\", bbox_inches='tight')\n    #plt.clf()\n\n\n\n\n\ndf = pd.DataFrame({draw: \n              sigmas[draw].numpy() \n              for draw in [3, 4, 5, 10, 100]}).mean()\ndf.plot(kind='bar', rot=0)\nplt.axhline(1, color='k', linestyle='--')\n# Put numbers on top of bars\nfor i, v in enumerate(df):\n    plt.text(i - .1, v + .01, f'{v:.3f}', color='k', fontsize=12)\n\nplt.xlabel(\"Sample size (N)\")\nplt.ylabel(\"Estimated standard deviation\")\nplt.savefig('../figures/biased-mle-variance-quality.pdf', bbox_inches='tight')\n\n\n\n\n\n\n\n3      0.981293\n4      1.014205\n5      0.952345\n10     1.022295\n100    0.985779\ndtype: float64\n\n\n\ndf_unbiased = df*(df.index/(df.index-1.0))\n\ndf_unbiased.plot(kind='bar', rot=0)\nplt.axhline(1, color='k', linestyle='--')\n# Put numbers on top of bars\nfor i, v in enumerate(df_unbiased):\n    plt.text(i - .1, v + .01, f'{v:.3f}', color='k', fontsize=12)\n\nplt.xlabel(\"Sample size (N)\")\nplt.ylabel(\"Corrected standard deviation\")\nplt.savefig('../figures/corrected-mle-variance-quality.pdf', bbox_inches='tight')"
  },
  {
    "objectID": "notebooks/basis.html",
    "href": "notebooks/basis.html",
    "title": "Basis Functions Regression",
    "section": "",
    "text": "from sklearn.linear_model import LinearRegression\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n%matplotlib inline\n\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n\n\nf_true = lambda x: np.cos(1.5 * np.pi * x) + (1+x)*np.sin(0.5 * np.pi * x)\nf = lambda x: f_true(x) + np.random.randn(*x.shape) * 0.3\n\nx = np.linspace(0, 1, 100)\ny = f(x)\nplt.plot(x, y, 'o', label='data')\nplt.plot(x, f_true(x), label='true')\nplt.legend(loc='best')\n\n&lt;matplotlib.legend.Legend at 0x7f883e31b850&gt;\n\n\n\n\n\n\n# Fit a phi function transformed linear model\ndef y_hat_basis(x_train, y_train, x_test, phi):\n    model = LinearRegression()\n    model.fit(phi(x_train), y_train)\n    return model.predict(phi(x_test))\n\n\nphi_linear = lambda x: x.reshape(-1, 1)\n\nphi_poly = lambda x, d: np.stack([x**i for i in range(1, d+1)], axis=1)\n\n\nphi_linear(x).shape, phi_poly(x, 3).shape\n\n((100, 1), (100, 3))\n\n\n\nd = 5\nplt.plot(x, phi_poly(x, d))\n# add legend\nplt.legend([fr'$x^{i+1}$' for i in range(d)], loc='best')\n\n&lt;matplotlib.legend.Legend at 0x7f88340ee580&gt;\n\n\n\n\n\n\n# Fit a linear model (identity basis)\ny_hat_linear = y_hat_basis(x, y, x, phi_linear)\ny_hat_poly_2 = y_hat_basis(x, y, x, lambda x: phi_poly(x, 2))\ny_hat_poly_5 = y_hat_basis(x, y, x, lambda x: phi_poly(x, 5))\nplt.plot(x, y, 'o', label='data')\nplt.plot(x, y_hat_linear, label='linear')\nplt.plot(x, y_hat_poly_2, label='poly 2')\nplt.plot(x, y_hat_poly_5, label='poly 5')\nplt.legend(loc='best')\n\n&lt;matplotlib.legend.Legend at 0x7f882f704580&gt;\n\n\n\n\n\n\n# sine basis\ndef phi_sine(x, d):\n    out = [x]\n    for i in range(1, d+1):\n        out.append(np.sin(2*np.pi*x*i))\n        # Append cosine\n        out.append(np.cos(2*np.pi*x*i))\n    return np.stack(out, axis=1)\n\n# Plot sine basis\nd = 3\nplt.plot(x, phi_sine(x, d))\n\n\n\n\n\n# fit sine basis model\ny_hat_sine_3 = y_hat_basis(x, y, x, lambda x: phi_sine(x, 15))\n\nplt.plot(x, y, 'o', label='data')\nplt.plot(x, y_hat_linear, label='linear')\n#plt.plot(x, y_hat_poly_2, label='poly 2')\n#plt.plot(x, y_hat_poly_5, label='poly 5')\nplt.plot(x, y_hat_sine_3, label='sine 3')\nplt.legend(loc='best')\n\n&lt;matplotlib.legend.Legend at 0x7f882f0f3130&gt;\n\n\n\n\n\n\n# Gaussian basis\ndef phi_gaussian(x, d, mu, sigma):\n    \"\"\"\n    x: (n,) denotes the input\n    d: (int) denotes the dimension of the basis\n    mu: (d,) denotes the mean of the basis\n    sigma: (d,) denotes the standard deviation of the basis\n    \"\"\"\n    out = []\n    for i in range(d):\n        out.append(np.exp(-(x-mu[i])**2 / (2*sigma[i]**2)))\n    return np.stack(out, axis=1)\n\n\nphi_gaussian(np.array([0.5]), 1, np.array([0.5]), np.array([0.1]))\n\narray([[1.]])\n\n\n\nphi_gaussian(np.array([1]), 1, np.array([0.5]), np.array([0.1]))\n\narray([[3.72665317e-06]])\n\n\n\nphi_gaussian(np.array([1]), 1, np.array([0.8]), np.array([0.1]))\n\narray([[0.13533528]])\n\n\n\nphi_gaussian(np.array([1]), 1, np.array([0.5]), np.array([4]))\n\narray([[0.99221794]])\n\n\n\n# Now, let us visualize the basis for different x but a single mu and sigma\nx = np.linspace(0, 1, 100)\n\nplt.plot(x, phi_gaussian(x, 1, np.array([0.5]), np.array([0.1])))\n\n\n\n\n\n# Now, let us plot the basis for three different mu and sigma\nd = 3\nmu = np.linspace(0, 1, d)\nsigma = np.ones(d) * 0.1\nplt.plot(x, phi_gaussian(x, d, mu, sigma))\n\n\n\n\n\n# We are seeking coefficients for the basis functions\n# Let us plot the basis functions for different coefficients\n\nd = 3\nmu = np.linspace(0, 1, d)\nsigma = np.ones(d) * 0.1\ncoeffs = np.array([0.2, -0.1, 0.6])\nplt.plot(x, phi_gaussian(x, d, mu, sigma) @ coeffs)\n\n\n\n\n\n\n# Now, let us fit a Gaussian basis model \n\n\nd = 5\nmu = np.linspace(0, 1, d)\nsigma = np.ones(d) * 0.1\ny_hat_gaussian_5 = y_hat_basis(x, y, x, lambda x: phi_gaussian(x, d, mu, sigma))\nplt.plot(x, y, 'o', label='data')\nplt.plot(x, y_hat_linear, label='linear')\n#plt.plot(x, y_hat_poly_2, label='poly 2')\n#plt.plot(x, y_hat_poly_5, label='poly 5')\nplt.plot(x, y_hat_sine_3, label='sine 1')\nplt.plot(x, y_hat_gaussian_5, label='gaussian 5')\nplt.legend(loc='best')\n\n&lt;matplotlib.legend.Legend at 0x7f882ed85c70&gt;\n\n\n\n\n\n\n# Now, let us visualize the different Gaussian basis functions and their coefficients\n\nd = 5\nmu = np.linspace(0, 1, d)\nsigma = np.ones(d) * 0.1\n\nX = phi_gaussian(x, d, mu, sigma)\nplt.plot(x, X)\n\n\n\n\n\nlr = LinearRegression()\nlr.fit(X, y)\nlr.coef_, lr.intercept_\n\n(array([-0.03558785, -0.09144039, -0.92094484, -0.32654565,  0.75825773]),\n 1.0918335515196196)\n\n\n\n# Plot the predictions\nplt.plot(x, y, 'o', label='data')\nplt.plot(x, lr.predict(X), label='gaussian 5')\nplt.legend(loc='best')\n\n&lt;matplotlib.legend.Legend at 0x7f882eb83040&gt;\n\n\n\n\n\n\n# Plot each of the scaled basis functions (scaling by the coefficients)\nfor i in range(d):\n    plt.plot(x, lr.coef_[i] * X[:, i], label=f'basis {i} coefficient {lr.coef_[i]:.2f}')\n\nplt.plot(x, y, 'o', label='data')\nplt.plot(x, lr.predict(X), label='gaussian 5', lw = 5)\n\n# Legend outside the plot\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n\n\n&lt;matplotlib.legend.Legend at 0x7f882e8b6cd0&gt;\n\n\n\n\n\n\n# plot high degree Gaussian basis\nd = 15\nmu = np.linspace(0, 1, d)\nsigma = np.ones(d) * 0.1\n\nfit_phi_gaussian = lambda x: phi_gaussian(x, d, mu, sigma)\ny_hat_gaussian_15 = y_hat_basis(x, y, x, fit_phi_gaussian)\nplt.plot(x, y, 'o', label='data', alpha=0.1)\n\nplt.plot(x, y_hat_gaussian_5, label='gaussian 5')\nplt.plot(x, y_hat_gaussian_15, label='gaussian 15')\n\nd = 30\nmu = np.linspace(0, 1, d)\nsigma = np.ones(d) * 0.1\n\nfit_phi_gaussian = lambda x: phi_gaussian(x, d, mu, sigma)\ny_hat_gaussian_30 = y_hat_basis(x, y, x, fit_phi_gaussian)\n\nplt.plot(x, y_hat_gaussian_30, label='gaussian 30')\nplt.legend(loc='best')\n\n&lt;matplotlib.legend.Legend at 0x7f882e085c10&gt;"
  },
  {
    "objectID": "notebooks/test.html",
    "href": "notebooks/test.html",
    "title": "Probabilistic Machine Learning",
    "section": "",
    "text": "import torch\nimport matplotlib.pyplot as plt\ndist = torch.distributions\n\n%matplotlib inline\n# retina\n%config InlineBackend.figure_format = 'retina'\n\n\nalpha = 20\nbeta = 30\npost = dist.Beta(alpha, beta)\n\n\nx_lin = torch.linspace(0., 1, 500)\n\n\nys = post.log_prob(x_lin).exp()\nplt.plot(x_lin, ys)\n\n\n\n\n\n# MAP of post (Beta(20, 30))\n\ntheta_map = torch.tensor((alpha - 1) / (alpha + beta - 2))\n\n\ntheta_map\n\ntensor(0.3958)\n\n\n\nplt.plot(x_lin, ys)\nplt.axvline(theta_map.item(), color='red')\n\n&lt;matplotlib.lines.Line2D at 0x7f6d507192e0&gt;\n\n\n\n\n\n\nf = lambda x: post.log_prob(x)\n\nfrom torch.autograd.functional import hessian\n\nscale = 1/torch.sqrt(-hessian(f, theta_map))\n\n# Find gradient of log_post wrt theta\n\n\n\nplt.plot(x_lin, post.log_prob(x_lin))\nplt.axvline(theta_map.item(), color='red')\n\nappx = dist.Normal(theta_map, scale)\nplt.plot(x_lin, appx.log_prob(x_lin))\n\n\n\n\n\nimport jax\nimport tensorflow_probability.substrates.jax.distributions as tfd\n\n\nbeta_jax = tfd.Beta(alpha, beta)\n\n\n-jax.hessian(beta_jax.log_prob)(theta_map)\n\nTypeError: Argument '0.3958333432674408' of type &lt;class 'torch.Tensor'&gt; is not a valid JAX type.\n\n\n\njax.hessian(beta_jax.log_prob(theta_map))\n\nTypeError: Expected a callable value, got 1.7470932006835938\n\n\n\nappx_posterior = dist.Normal(theta_map, )"
  },
  {
    "objectID": "notebooks/reliability.html",
    "href": "notebooks/reliability.html",
    "title": "CIFAR10 and CIFAR100 dataset",
    "section": "",
    "text": "https://www.cs.toronto.edu/~kriz/cifar.html\nRef: https://hjweide.github.io/quantifying-uncertainty-in-neural-networks"
  },
  {
    "objectID": "notebooks/reliability.html#reference-slides",
    "href": "notebooks/reliability.html#reference-slides",
    "title": "CIFAR10 and CIFAR100 dataset",
    "section": "Reference slides",
    "text": "Reference slides\nhttps://classifier-calibration.github.io/assets/slides/clacal_tutorial_ecmlpkdd_2020_intro.pdf\nFirst 7 slides from above link\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\n\nfrom sklearn.datasets import make_moons\nfrom sklearn.linear_model import LogisticRegression\n\nX, y = make_moons(n_samples=1000, noise=0.1, random_state=0)\n\nclf = LogisticRegression()\n\nclf.fit(X, y)\n\nLogisticRegression()\n\n\n\np_hats = clf.predict_proba(X)[:, 1]\np_hats[:10]\n\narray([0.60804417, 0.96671272, 0.02867904, 0.41138351, 0.9631718 ,\n       0.72298419, 0.51086972, 0.0148838 , 0.01696895, 0.98714042])\n\n\n\ntrue_labels = y\ntrue_labels[:10]\n\narray([1, 1, 0, 1, 1, 1, 0, 0, 0, 1])\n\n\n\ndf = pd.DataFrame({'p_hat': p_hats, 'y': true_labels})\ndf.head()\n\n\n\n\n\n\n\n\np_hat\ny\n\n\n\n\n0\n0.608044\n1\n\n\n1\n0.966713\n1\n\n\n2\n0.028679\n0\n\n\n3\n0.411384\n1\n\n\n4\n0.963172\n1\n\n\n\n\n\n\n\n\n# Let us look at samples where p_hat is close to 0.5 (say between 0.45 and 0.55)\ndf_close_phat_val = df[(df.p_hat &gt; 0.45) & (df.p_hat &lt; 0.55)]\ndf_close_phat_val.head()\n\n\n\n\n\n\n\n\np_hat\ny\n\n\n\n\n6\n0.510870\n0\n\n\n15\n0.481702\n0\n\n\n20\n0.533697\n0\n\n\n41\n0.522857\n1\n\n\n60\n0.483962\n1\n\n\n\n\n\n\n\n\nlen(df_close_phat_val)\n\n31\n\n\n\n# Histogram of p_hat values\ndf.p_hat.hist(bins=50)\nplt.xlabel('$\\hat{p}$')\nplt.ylabel('Frequency')\nplt.title('Histogram of $\\hat{p}$ values');\n\nfindfont: Font family ['cursive'] not found. Falling back to DejaVu Sans.\nfindfont: Generic family 'cursive' not found because none of the following families were found: Apple Chancery, Textile, Zapf Chancery, Sand, Script MT, Felipa, Comic Neue, Comic Sans MS, cursive\n\n\n\n\n\n\n# Out of these len(df_close_phat_val) samples, how many are actually positive? \n\ndf_close_phat_val.y.sum()\n\n18\n\n\n\n# Fraction of positive samples\ndf_close_phat_val.y.sum() / len(df_close_phat_val)\n\n0.5806451612903226\n\n\n\n# This is the reliability of the classifier at p_hat = 0.5\n\n\nbins = np.linspace(0, 1, 11)\nbins\n\narray([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])\n\n\n\n# Let us add a column to the dataframe that tells us which bin each sample belongs to\n\ndf['bin'] = pd.cut(df.p_hat, bins=bins)\ndf.head()\n\n\n\n\n\n\n\n\np_hat\ny\nbin\n\n\n\n\n0\n0.608044\n1\n(0.6, 0.7]\n\n\n1\n0.966713\n1\n(0.9, 1.0]\n\n\n2\n0.028679\n0\n(0.0, 0.1]\n\n\n3\n0.411384\n1\n(0.4, 0.5]\n\n\n4\n0.963172\n1\n(0.9, 1.0]\n\n\n\n\n\n\n\n\n# Let us compute the reliability of the classifier in each bin\n\ndf.groupby('bin').y.mean()\n\nbin\n(0.0, 0.1]    0.006231\n(0.1, 0.2]    0.241379\n(0.2, 0.3]    0.269231\n(0.3, 0.4]    0.375000\n(0.4, 0.5]    0.620690\n(0.5, 0.6]    0.413793\n(0.6, 0.7]    0.529412\n(0.7, 0.8]    0.722222\n(0.8, 0.9]    0.849315\n(0.9, 1.0]    0.987097\nName: y, dtype: float64\n\n\n\n# Calculate the reliability diagram points\nmean_predicted_probs = df.groupby('bin').p_hat.mean()\nfraction_of_positives = df.groupby('bin').y.mean()\n\n\nbin_sizes = df.groupby('bin').size()\nbin_sizes\n\nbin\n(0.0, 0.1]    321\n(0.1, 0.2]     58\n(0.2, 0.3]     52\n(0.3, 0.4]     40\n(0.4, 0.5]     29\n(0.5, 0.6]     29\n(0.6, 0.7]     34\n(0.7, 0.8]     54\n(0.8, 0.9]     73\n(0.9, 1.0]    310\ndtype: int64\n\n\n\nece = (np.abs(mean_predicted_probs - fraction_of_positives) * bin_sizes / len(df)).sum()\nece\n\n0.036133146181597894\n\n\n\nplt.figure(figsize=(6, 6))\nplt.plot([0, 1], [0, 1], 'k:')\nplt.plot(mean_predicted_probs, fraction_of_positives, 'o-')\nplt.xlabel('Predicted probability')\nplt.ylabel('Fraction of positives')\nplt.title(f'Reliability Diagram (ECE={ece:.4f})')\nplt.gca().set_aspect('equal')\n\n# Plot the bar also\nplt.bar(mean_predicted_probs, fraction_of_positives, width=0.1, alpha=0.1)\n\n&lt;BarContainer object of 10 artists&gt;\n\n\n\n\n\n\nfrom sklearn.calibration import calibration_curve\n\nprob_true, prob_pred = calibration_curve(df['y'], df['p_hat'], n_bins=10, strategy='uniform')\n\n\nassert np.allclose(prob_true, fraction_of_positives)\n\n\nassert np.allclose(prob_pred, mean_predicted_probs)\n\n\npd.DataFrame({'prob_true': prob_true, 'prob_pred': prob_pred})\n\n\n\n\n\n\n\n\nprob_true\nprob_pred\n\n\n\n\n0\n0.006231\n0.030912\n\n\n1\n0.241379\n0.149753\n\n\n2\n0.269231\n0.244433\n\n\n3\n0.375000\n0.341098\n\n\n4\n0.620690\n0.447832\n\n\n5\n0.413793\n0.550410\n\n\n6\n0.529412\n0.650223\n\n\n7\n0.722222\n0.753892\n\n\n8\n0.849315\n0.854304\n\n\n9\n0.987097\n0.970665\n\n\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.calibration import calibration_curve\n\ndef plot_reliability_diagram(classifier, X, y, n_bins=10, ax=None):\n    \"\"\"\n    Generate a reliability diagram and calculate ECE for a classifier.\n\n    Parameters:\n    - classifier: A scikit-learn classifier with a `predict_proba` method.\n    - X: Input features for the dataset.\n    - y: True labels for the dataset.\n    - n_bins: Number of bins for the reliability diagram.\n    - ax: Matplotlib axes object to plot on.\n\n    Returns:\n    - ece: Expected Calibration Error (ECE).\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    # Fit the classifier\n    classifier.fit(X, y)\n\n    # Predict probabilities\n    p_hats = classifier.predict_proba(X)[:, 1]\n\n    # Create a DataFrame\n    df = pd.DataFrame({'p_hat': p_hats, 'y': y})\n\n    # Create bins based on predicted probabilities\n    bins = np.linspace(0, 1, n_bins + 1)\n    df['bin'] = pd.cut(df['p_hat'], bins=bins)\n    \n    mean_predicted_probs = df.groupby('bin').p_hat.mean()\n    fraction_of_positives = df.groupby('bin').y.mean()\n    \n    # Fill NANs with 0\n    mean_predicted_probs = mean_predicted_probs.fillna(0)\n    fraction_of_positives = fraction_of_positives.fillna(0)\n\n    # Calculate the Expected Calibration Error (ECE) weighted by bin sizes\n    bin_sizes = df.groupby('bin').size()\n    \n    # Ensure bin_sizes align with prob_true and prob_pred\n    #bin_sizes = bin_sizes.reindex(range(n_bins), fill_value=0)\n    #return bin_sizes, mean_predicted_probs, fraction_of_positives\n\n    ece = (np.abs(mean_predicted_probs - fraction_of_positives) * bin_sizes / bin_sizes.sum()).sum()\n\n    # Create the reliability diagram plot\n    #plt.figure(figsize=(6, 6))\n    ax.plot([0, 1], [0, 1], 'k:')\n    ax.scatter(mean_predicted_probs, fraction_of_positives)\n    # Plot bar also\n    ax.bar(mean_predicted_probs, fraction_of_positives, width=0.1, alpha=0.1)\n    ax.set_xlabel('Mean Predicted Probability')\n    ax.set_ylabel('Fraction of Positives')\n    ax.set_title(f'Calibration Curve (ECE={ece:.4f})')\n    #plt.gca().set_aspect('equal')\n    return ece\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.calibration import CalibrationDisplay\n\n# Generate the dataset\nX, y = make_moons(n_samples=1000, noise=0.1, random_state=0)\n\n# List of classifiers to evaluate\nclassifiers = [\n    ('Logistic Regression', LogisticRegression()),\n    ('Random Forest', RandomForestClassifier(n_estimators=100, random_state=0)),\n    ('Support Vector Classifier', SVC(probability=True)),\n    ('K-Nearest Neighbors', KNeighborsClassifier(n_neighbors=3))\n]\n\nfig, axarr = plt.subplots(figsize=(6, 6), nrows=len(classifiers), ncols=2)\n\n# Loop through the classifiers and generate reliability diagrams\nfor i, (name, classifier) in enumerate(classifiers):\n    ece = plot_reliability_diagram(classifier, X, y, ax=axarr[i, 0])\n    print(f\"Classifier: {name}\")\n    print(f\"ECE: {ece:.4f}\")\n\n\n    \n    CalibrationDisplay.from_estimator(\n        classifier,\n        X,\n        y,\n        n_bins=10,\n        ax = axarr[i, 1],\n        name=name\n    )\n    print(\"*\"*20)\n    print(\"*\"*20)\n    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n\nClassifier: Logistic Regression\nECE: 0.0361\n********************\n********************\n\n==================================================\n\nClassifier: Random Forest\nECE: 0.0135\n********************\n********************\n\n==================================================\n\nClassifier: Support Vector Classifier\nECE: 0.0047\n********************\n********************\n\n==================================================\n\nClassifier: K-Nearest Neighbors\nECE: 0.0013\n********************\n********************\n\n==================================================\n\n\n\n\n\n\n\nMulticlass reliability diagram\nSlide 26 from https://classifier-calibration.github.io/assets/slides/clacal_tutorial_ecmlpkdd_2020_intro.pdf\n\n# Now, multi-class classification and reliability diagrams\n\nfrom sklearn.datasets import make_classification, make_blobs\n\nX, y = make_blobs(n_samples=1000, centers=3, n_features=2, random_state=0)\n\n# Plot the dataset\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n\n&lt;matplotlib.collections.PathCollection at 0x7f33e5367eb0&gt;\n\n\n\n\n\n\nclf = LogisticRegression()\nclf.fit(X, y)\n\nLogisticRegression()\n\n\n\npreds = pd.DataFrame(clf.predict_proba(X))\n\n\npreds\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0.068741\n0.005454\n0.925805\n\n\n1\n0.983384\n0.015859\n0.000757\n\n\n2\n0.021072\n0.000240\n0.978688\n\n\n3\n0.042703\n0.001372\n0.955925\n\n\n4\n0.000208\n0.000026\n0.999766\n\n\n...\n...\n...\n...\n\n\n995\n0.005001\n0.994489\n0.000510\n\n\n996\n0.050768\n0.015765\n0.933467\n\n\n997\n0.006151\n0.911409\n0.082440\n\n\n998\n0.000271\n0.000021\n0.999708\n\n\n999\n0.021539\n0.962650\n0.015811\n\n\n\n\n1000 rows × 3 columns\n\n\n\n\npreds['y'] = y\n\n\npreds.head(10)\n\n\n\n\n\n\n\n\n0\n1\n2\ny\n\n\n\n\n0\n0.068741\n0.005454\n0.925805\n2\n\n\n1\n0.983384\n0.015859\n0.000757\n0\n\n\n2\n0.021072\n0.000240\n0.978688\n2\n\n\n3\n0.042703\n0.001372\n0.955925\n2\n\n\n4\n0.000208\n0.000026\n0.999766\n2\n\n\n5\n0.179771\n0.797045\n0.023183\n1\n\n\n6\n0.120874\n0.003821\n0.875305\n0\n\n\n7\n0.957242\n0.000002\n0.042756\n0\n\n\n8\n0.000908\n0.998876\n0.000216\n1\n\n\n9\n0.836772\n0.162509\n0.000718\n0\n\n\n\n\n\n\n\n\n# Syntax highlighting to show highest amongst the three columns\nmost_probable_class = preds[[0, 1, 2]].idxmax(axis=1)\n\n# Add the most probable class to the DataFrame\npreds['y_hat'] = most_probable_class\n\n\npreds.head()\n\n\n\n\n\n\n\n\n0\n1\n2\ny\ny_hat\n\n\n\n\n0\n0.068741\n0.005454\n0.925805\n2\n2\n\n\n1\n0.983384\n0.015859\n0.000757\n0\n0\n\n\n2\n0.021072\n0.000240\n0.978688\n2\n2\n\n\n3\n0.042703\n0.001372\n0.955925\n2\n2\n\n\n4\n0.000208\n0.000026\n0.999766\n2\n2\n\n\n\n\n\n\n\n\n# Let us simplify dataframe to only include the most probable class probability and whether the prediction was correct or not\nnew_df = preds.copy()\nnew_df[\"Correct\"] = new_df.y == new_df.y_hat\nnew_df[\"Most_Probable_Class_Probability\"] = new_df[[0, 1, 2]].max(axis=1)\n\nnew_df.head()\n\n\n\n\n\n\n\n\n0\n1\n2\ny\ny_hat\nCorrect\nMost_Probable_Class_Probability\n\n\n\n\n0\n0.068741\n0.005454\n0.925805\n2\n2\nTrue\n0.925805\n\n\n1\n0.983384\n0.015859\n0.000757\n0\n0\nTrue\n0.983384\n\n\n2\n0.021072\n0.000240\n0.978688\n2\n2\nTrue\n0.978688\n\n\n3\n0.042703\n0.001372\n0.955925\n2\n2\nTrue\n0.955925\n\n\n4\n0.000208\n0.000026\n0.999766\n2\n2\nTrue\n0.999766\n\n\n\n\n\n\n\n\n# drop the other columns\nnew_df = new_df.drop(columns=[0, 1, 2, 'y', 'y_hat'])\nnew_df.head()\n\n\n\n\n\n\n\n\nCorrect\nMost_Probable_Class_Probability\n\n\n\n\n0\nTrue\n0.925805\n\n\n1\nTrue\n0.983384\n\n\n2\nTrue\n0.978688\n\n\n3\nTrue\n0.955925\n\n\n4\nTrue\n0.999766\n\n\n\n\n\n\n\n\n# Let us look at samples where the most probable class probability is close to 0.5 (say between 0.45 and 0.55)\ndf_close_phat_val = new_df[(new_df.Most_Probable_Class_Probability &gt; 0.45) & (new_df.Most_Probable_Class_Probability &lt; 0.55)]\ndf_close_phat_val.head()\n\n\n\n\n\n\n\n\nCorrect\nMost_Probable_Class_Probability\n\n\n\n\n27\nTrue\n0.532492\n\n\n127\nFalse\n0.537886\n\n\n135\nTrue\n0.536489\n\n\n171\nTrue\n0.512435\n\n\n203\nFalse\n0.534704\n\n\n\n\n\n\n\n\ndf_close_phat_val.Correct.sum() / len(df_close_phat_val)\n\n0.4838709677419355\n\n\n\n# Now, let us compute the reliability of the classifier in each bin\nbins = np.linspace(0, 1, 11)\n\n# Cuts the data by the bins\nnew_df['bin'] = pd.cut(new_df.Most_Probable_Class_Probability, bins=bins)\n\n\nnew_df.head(10)\n\n\n\n\n\n\n\n\nCorrect\nMost_Probable_Class_Probability\nbin\n\n\n\n\n0\nTrue\n0.925805\n(0.9, 1.0]\n\n\n1\nTrue\n0.983384\n(0.9, 1.0]\n\n\n2\nTrue\n0.978688\n(0.9, 1.0]\n\n\n3\nTrue\n0.955925\n(0.9, 1.0]\n\n\n4\nTrue\n0.999766\n(0.9, 1.0]\n\n\n5\nTrue\n0.797045\n(0.7, 0.8]\n\n\n6\nFalse\n0.875305\n(0.8, 0.9]\n\n\n7\nTrue\n0.957242\n(0.9, 1.0]\n\n\n8\nTrue\n0.998876\n(0.9, 1.0]\n\n\n9\nTrue\n0.836772\n(0.8, 0.9]\n\n\n\n\n\n\n\n\n# Calculate the reliability diagram points\nmean_predicted_probs = new_df.groupby('bin').Most_Probable_Class_Probability.mean()\nfraction_of_positives = new_df.groupby('bin').Correct.mean()\n\nbin_sizes = new_df.groupby('bin').size()\n\nece = (np.abs(mean_predicted_probs - fraction_of_positives) * bin_sizes / len(new_df)).sum()\n\n\nece\n\n0.016551445751129316\n\n\n\n# Plot the reliability diagram\nplt.figure(figsize=(6, 6))\nplt.plot([0, 1], [0, 1], 'k:')\nplt.plot(mean_predicted_probs, fraction_of_positives, 'o-')\nplt.xlabel('Predicted probability')\nplt.ylabel('Fraction of positives')\nplt.title(f'Reliability Diagram (ECE={ece:.4f})')\n\nText(0.5, 1.0, 'Reliability Diagram (ECE=0.0166)')\n\n\n\n\n\n\n# Histogram of p_hat values\n\nnew_df.Most_Probable_Class_Probability.hist(bins=50)\nplt.xlim(0, 1)\n\n(0.0, 1.0)\n\n\n\n\n\n\n\nReliability Diagrams for regression\nhttps://proceedings.mlr.press/v80/kuleshov18a/kuleshov18a.pdf\n\n# Now let us look at the above for regression with heteroscedastic aleatoric uncertainty\n\nX = np.linspace(-3, 3, 1000).reshape(-1, 1)\ny = np.sin(X.flatten()) + np.random.normal(0, 0.1, X.shape[0])\n\nplt.scatter(X, y, s=10, alpha=0.5)\n\n&lt;matplotlib.collections.PathCollection at 0x7f351983e700&gt;\n\n\n\n\n\n\n# Use BayesianRidge for regression\nfrom sklearn.linear_model import BayesianRidge\n\nreg = BayesianRidge()\nreg.fit(X, y)\n\nBayesianRidge()\n\n\n\n# Predict on linspace\nX_test = np.linspace(-3, 3, 100).reshape(-1, 1)\ny_pred, y_std = reg.predict(X_test, return_std=True)\n\n# Plot the predictions\nplt.plot(X_test, y_pred)\nplt.fill_between(X_test.flatten(), y_pred - y_std, y_pred + y_std, alpha=0.5)\nplt.scatter(X, y, alpha=0.1)\n\n&lt;matplotlib.collections.PathCollection at 0x7f350eb599a0&gt;\n\n\n\n\n\n\n# Within 1\\sigma we would like to see 68% of the data points\n# Within 2\\sigma we would like to see 95% of the data points\n\n# Let us see how many points are within k*sigma of the mean\nk = 1\ny_pred, y_std = reg.predict(X, return_std=True)\n# Check how many points are within k*sigma of the mean\nwithin_k_sigma = np.abs(y - y_pred) &lt; k * y_std\n# Fraction of points within k*sigma of the mean\nwithin_k_sigma.mean()\n\n0.585\n\n\n\ndef within_k_sigma(y, y_pred, y_std, k=1):\n    \"\"\"\n    Return the fraction of points within k*sigma of the mean.\n    \"\"\"\n    return np.abs(y - y_pred) &lt; k * y_std\n\n\n# Plot within_k_sigma for k=1, 2, 3, ..\n\nks = np.arange(1, 6)\nwithin_k_sigma_fractions = [within_k_sigma(y, y_pred, y_std, k=k).mean() for k in ks]\n\nplt.plot(ks, within_k_sigma_fractions, 'o-')\nplt.xlabel('k')\nplt.ylabel('Fraction of points within k*sigma of the mean')\n\nText(0, 0.5, 'Fraction of points within k*sigma of the mean')\n\n\n\n\n\n\n# but, ideally we want 95% of the points to be within 2*sigma of the mean or within 95% CI\n# we need a function to map CI to k\n\nfrom scipy.stats import norm\n\ndef ci_to_k(ci):\n    \"\"\"\n    Return the value of k corresponding to the given confidence interval.\n    \"\"\"\n    return norm.ppf((1 + ci) / 2)\n\n\nci_to_k(0.95)\n\n1.959963984540054\n\n\n\nci_to_k(0.68)\n\n0.9944578832097535\n\n\n\n# make reliability diagram for regression\n\npercentiles = np.linspace(0, 100, 11)\nquantiles = percentiles / 100\n\nks = ci_to_k(quantiles)\n\n# Get fraction of points within k*sigma of the mean for each k\nwithin_k_sigma_fractions = [within_k_sigma(y, y_pred, y_std, k=k).mean() for k in ks]\n\nplt.plot(quantiles, within_k_sigma_fractions, 'o-')\n# Plot ideal\nplt.plot([0, 1], [0, 1], 'k:')"
  },
  {
    "objectID": "notebooks/linear_regression.html",
    "href": "notebooks/linear_regression.html",
    "title": "Linear Regression Tutorial",
    "section": "",
    "text": "from tueplots import bundles\nimport numpy as np\nimport scipy.linalg\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams.update(bundles.icml2022())"
  },
  {
    "objectID": "notebooks/linear_regression.html#maximum-likelihood",
    "href": "notebooks/linear_regression.html#maximum-likelihood",
    "title": "Linear Regression Tutorial",
    "section": "1. Maximum Likelihood",
    "text": "1. Maximum Likelihood\nLet us compute the maximum likelihood estimate for a given training set\n\n## EDIT THIS FUNCTION\ndef max_lik_estimate(X, y):\n\n    # X: N x D matrix of training inputs\n    # y: N x 1 vector of training targets/observations\n    # returns: maximum likelihood parameters (D x 1)\n\n    N, D = X.shape\n    theta_ml = np.linalg.solve(X.T @ X, X.T @ y) ## &lt;-- SOLUTION\n    return theta_ml\n\nNow, make a prediction using the maximum likelihood estimate that we just found\n\n## EDIT THIS FUNCTION\ndef predict_with_estimate(Xtest, theta):\n\n    # Xtest: K x D matrix of test inputs\n    # theta: D x 1 vector of parameters\n    # returns: prediction of f(Xtest); K x 1 vector\n\n    prediction = Xtest @ theta ## &lt;-- SOLUTION\n\n    return prediction\n\n\n## EDIT THIS FUNCTION\ndef poly_features(X, K):\n\n    # X: inputs of size N x 1\n    # K: degree of the polynomial\n    # computes the feature matrix Phi (N x (K+1))\n\n    X = X.flatten()\n    N = X.shape[0]\n\n    #initialize Phi\n    Phi = np.zeros((N, K+1))\n\n    # Compute the feature matrix in stages\n    for k in range(K+1):\n        Phi[:,k] = X**k ## &lt;-- SOLUTION\n    return Phi\n\n\n## EDIT THIS FUNCTION\ndef nonlinear_features_maximum_likelihood(Phi, y):\n    # Phi: features matrix for training inputs. Size of N x D\n    # y: training targets. Size of N by 1\n    # returns: maximum likelihood estimator theta_ml. Size of D x 1\n\n    kappa = 1e-08 # 'jitter' term; good for numerical stability\n\n    D = Phi.shape[1]\n\n    # maximum likelihood estimate\n    Pt = Phi.T @ y # Phi^T*y\n    PP = Phi.T @ Phi + kappa*np.eye(D) # Phi^T*Phi + kappa*I\n\n    # maximum likelihood estimate\n    C = scipy.linalg.cho_factor(PP)\n    theta_ml = scipy.linalg.cho_solve(C, Pt) # inv(Phi^T*Phi)*Phi^T*y\n\n    return theta_ml"
  },
  {
    "objectID": "notebooks/linear_regression.html#maximum-a-posteriori-estimation",
    "href": "notebooks/linear_regression.html#maximum-a-posteriori-estimation",
    "title": "Linear Regression Tutorial",
    "section": "2. Maximum A Posteriori Estimation",
    "text": "2. Maximum A Posteriori Estimation\n\n## EDIT THIS FUNCTION\ndef map_estimate_poly(Phi, y, sigma, alpha):\n    # Phi: training inputs, Size of N x D\n    # y: training targets, Size of D x 1\n    # sigma: standard deviation of the noise\n    # alpha: standard deviation of the prior on the parameters\n    # returns: MAP estimate theta_map, Size of D x 1\n\n    D = Phi.shape[1]\n\n    # SOLUTION\n    PP = Phi.T @ Phi + (sigma/alpha)**2 * np.eye(D)\n    theta_map = scipy.linalg.solve(PP, Phi.T @ y)\n\n    return theta_map\n\n\n# define the function we wish to estimate later\ndef g(x, sigma):\n    p = np.hstack([x**0, x**1, np.sin(x)])\n    w = np.array([-1.0, 0.1, 1.0]).reshape(-1,1)\n    return p @ w + sigma*np.random.normal(size=x.shape)\n\n\n# Generate some data\nsigma = 1.0 # noise standard deviation\nalpha = 1.0 # standard deviation of the parameter prior\nN = 20\n\nnp.random.seed(42)\n\nX = (np.random.rand(N)*10.0 - 5.0).reshape(-1,1)\ny = g(X, sigma) # training targets\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.savefig('data.pdf')\n\n\n\n\n\n# get the MAP estimate\nK = 8 # polynomial degree\n\n\n# feature matrix\nPhi = poly_features(X, K)\n\ntheta_map = map_estimate_poly(Phi, y, sigma, alpha)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n\nXtest = np.linspace(-5,5,100).reshape(-1,1)\nytest = g(Xtest, sigma)\n\nPhi_test = poly_features(Xtest, K)\ny_pred_map = Phi_test @ theta_map\n\ny_pred_mle = Phi_test @ theta_ml\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred_map)\nplt.plot(Xtest, y_pred_mle)\n\nplt.legend([\"data\", r\"prediction using $\\theta_{MAP}$\", r\"prediction using $\\theta_{MLE}$\"], frameon = False)\nplt.savefig('map_mle.pdf')\n\n\n\n\n\nprint(np.hstack([theta_ml, theta_map]))\n\n[[-1.49712990e+00 -1.08154986e+00]\n [ 8.56868912e-01  6.09177023e-01]\n [-1.28335730e-01 -3.62071208e-01]\n [-7.75319509e-02 -3.70531732e-03]\n [ 3.56425467e-02  7.43090617e-02]\n [-4.11626749e-03 -1.03278646e-02]\n [-2.48817783e-03 -4.89363010e-03]\n [ 2.70146690e-04  4.24148554e-04]\n [ 5.35996050e-05  1.03384719e-04]]"
  },
  {
    "objectID": "notebooks/linear_regression.html#bayesian-linear-regression",
    "href": "notebooks/linear_regression.html#bayesian-linear-regression",
    "title": "Linear Regression Tutorial",
    "section": "3. Bayesian Linear Regression",
    "text": "3. Bayesian Linear Regression\n\n# Test inputs\nNtest = 200\nXtest = np.linspace(-5, 5, Ntest).reshape(-1,1) # test inputs\n\nprior_var = 2.0 # variance of the parameter prior (alpha^2). We assume this is known.\nnoise_var = 1.0 # noise variance (sigma^2). We assume this is known.\n\npol_deg = 3 # degree of the polynomial we consider at the moment\n\n\n## EDIT THIS CELL\n\n# compute the feature matrix for the test inputs\nPhi_test = poly_features(Xtest, pol_deg) # N x (pol_deg+1) feature matrix SOLUTION\n\n# compute the (marginal) prior at the test input locations\n# prior mean\nprior_mean = np.zeros((Ntest,1)) # prior mean &lt;-- SOLUTION\n\n# prior variance\nfull_covariance = Phi_test @ Phi_test.T * prior_var # N x N covariance matrix of all function values\nprior_marginal_var =  np.diag(full_covariance)\n\n# Let us visualize the prior over functions\nplt.figure()\nplt.plot(Xtest, prior_mean, color=\"k\")\n\nconf_bound1 = np.sqrt(prior_marginal_var).flatten()\nconf_bound2 = 2.0*np.sqrt(prior_marginal_var).flatten()\nconf_bound3 = 2.0*np.sqrt(prior_marginal_var + noise_var).flatten()\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound1,\n             prior_mean.flatten() - conf_bound1, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound2,\n                 prior_mean.flatten() - conf_bound2, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound3,\n                 prior_mean.flatten() - conf_bound3, alpha = 0.1, color=\"k\")\n\nplt.xlabel('$x$')\nplt.ylabel('$y$')\nplt.title(\"Prior over functions\");\n\n\n\n\nNow, we will use this prior distribution and sample functions from it.\n\n## EDIT THIS CELL\n\n# samples from the prior\nnum_samples = 10\n\n# We first need to generate random weights theta_i, which we sample from the parameter prior\nrandom_weights = np.random.normal(size=(pol_deg+1,num_samples), scale=np.sqrt(prior_var))\n\n# Now, we compute the induced random functions, evaluated at the test input locations\n# Every function sample is given as f_i = Phi * theta_i,\n# where theta_i is a sample from the parameter prior\n\nsample_function = Phi_test @ random_weights # &lt;-- SOLUTION\n\nplt.figure()\nplt.plot(Xtest, sample_function, color=\"r\")\nplt.title(\"Plausible functions under the prior\")\nprint(\"Every sampled function is a polynomial of degree \"+str(pol_deg));\n\nEvery sampled function is a polynomial of degree 3\n\n\n\n\n\nNow we are given some training inputs \\(\\boldsymbol x_1, \\dotsc, \\boldsymbol x_N\\), which we collect in a matrix \\(\\boldsymbol X = [\\boldsymbol x_1, \\dotsc, \\boldsymbol x_N]^\\top\\in\\mathbb{R}^{N\\times D}\\)\n\nN = 10\nX = np.random.uniform(high=5, low=-5, size=(N,1)) # training inputs, size Nx1\ny = g(X, np.sqrt(noise_var)) # training targets, size Nx1\n\nNow, let us compute the posterior\n\n## EDIT THIS FUNCTION\n\ndef polyfit(X, y, K, prior_var, noise_var):\n    # X: training inputs, size N x D\n    # y: training targets, size N x 1\n    # K: degree of polynomial we consider\n    # prior_var: prior variance of the parameter distribution\n    # sigma: noise variance\n\n    jitter = 1e-08 # increases numerical stability\n\n    Phi = poly_features(X, K) # N x (K+1) feature matrix\n\n    # Compute maximum likelihood estimate\n    Pt = Phi.T @ y # Phi*y, size (K+1,1)\n    PP = Phi.T @ Phi + jitter*np.eye(K+1) # size (K+1, K+1)\n    C = scipy.linalg.cho_factor(PP)\n    # maximum likelihood estimate\n    theta_ml = scipy.linalg.cho_solve(C, Pt) # inv(Phi^T*Phi)*Phi^T*y, size (K+1,1)\n\n#     theta_ml = scipy.linalg.solve(PP, Pt) # inv(Phi^T*Phi)*Phi^T*y, size (K+1,1)\n\n    # MAP estimate\n    theta_map = scipy.linalg.solve(PP + noise_var/prior_var*np.eye(K+1), Pt)\n\n    # parameter posterior\n    iSN = (np.eye(K+1)/prior_var + PP/noise_var) # posterior precision\n    SN = scipy.linalg.pinv(noise_var*np.eye(K+1)/prior_var + PP)*noise_var  # posterior covariance\n    mN = scipy.linalg.solve(iSN, Pt/noise_var) # posterior mean\n\n    return (theta_ml, theta_map, mN, SN)\n\n\ntheta_ml, theta_map, theta_mean, theta_var = polyfit(X, y, pol_deg, alpha, sigma)\n\nNow, let’s make predictions (ignoring the measurement noise). We obtain three predictors: \\[\\begin{align}\n&\\text{Maximum likelihood: }E[f(\\boldsymbol X_{\\text{test}})] = \\boldsymbol \\phi(X_{\\text{test}})\\boldsymbol \\theta_{ml}\\\\\n&\\text{Maximum a posteriori: } E[f(\\boldsymbol X_{\\text{test}})] = \\boldsymbol \\phi(X_{\\text{test}})\\boldsymbol \\theta_{map}\\\\\n&\\text{Bayesian: } p(f(\\boldsymbol X_{\\text{test}})) = \\mathcal N(f(\\boldsymbol X_{\\text{test}}) \\,|\\, \\boldsymbol \\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{mean}},\\, \\boldsymbol\\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{var}}  \\boldsymbol\\phi(X_{\\text{test}})^\\top)\n\\end{align}\\] We already computed all quantities. Write some code that implements all three predictors.\n\n## EDIT THIS CELL\n\n# predictions (ignoring the measurement/observations noise)\n\nPhi_test = poly_features(Xtest, pol_deg) # N x (K+1)\n\n# maximum likelihood predictions (just the mean)\nm_mle_test = Phi_test @ theta_ml\n\n# MAP predictions (just the mean)\nm_map_test = Phi_test @ theta_map\n\n# predictive distribution (Bayesian linear regression)\n# mean prediction\nmean_blr = Phi_test @ theta_mean\n# variance prediction\ncov_blr =  Phi_test @ theta_var @ Phi_test.T\n\n\n# plot the posterior\nplt.figure()\nplt.plot(X, y, \"+\")\n# plt.plot(Xtest, m_mle_test)\n# plt.plot(Xtest, m_map_test)\nvar_blr = np.diag(cov_blr)\nconf_bound1 = np.sqrt(var_blr).flatten()\nconf_bound2 = 2.0*np.sqrt(var_blr).flatten()\nconf_bound3 = 2.0*np.sqrt(var_blr + sigma).flatten()\n\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound1,\n                 mean_blr.flatten() - conf_bound1, alpha = 0.1, color=\"k\")\n# plt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound2,\n#                  mean_blr.flatten() - conf_bound2, alpha = 0.1, color=\"k\")\n# plt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound3,\n#                  mean_blr.flatten() - conf_bound3, alpha = 0.1, color=\"k\")\nplt.legend([\"Training data\",  \"BLR\"], frameon = False)\nplt.xlabel('$x$')\nplt.ylabel('$y$')\nplt.ylim(-7, 4)\nplt.savefig('blr.pdf')\n\n\n\n\n\n# plot the posterior\nplt.figure()\nplt.plot(X, y, \"+\")\nplt.plot(Xtest, m_mle_test)\nplt.plot(Xtest, m_map_test)\nvar_blr = np.diag(cov_blr)\nconf_bound1 = np.sqrt(var_blr).flatten()\nconf_bound2 = 2.0*np.sqrt(var_blr).flatten()\nconf_bound3 = 2.0*np.sqrt(var_blr + sigma).flatten()\n\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound1,\n                 mean_blr.flatten() - conf_bound1, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound2,\n                 mean_blr.flatten() - conf_bound2, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound3,\n                 mean_blr.flatten() - conf_bound3, alpha = 0.1, color=\"k\")\nplt.legend([\"Training data\", \"MLE\", \"MAP\", \"BLR\"], frameon = False)\nplt.xlabel('$x$')\nplt.ylabel('$y$')\nplt.ylim(-7, 4)\nplt.savefig('blr_mle_map.pdf')"
  },
  {
    "objectID": "notebooks/mc-linreg-predictive.html",
    "href": "notebooks/mc-linreg-predictive.html",
    "title": "Closed form solution for prior predictive distribution",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n%matplotlib inline\n\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\n\nprior_mu = torch.zeros(2)\nprior_cov = torch.eye(2)\ntheta = torch.distributions.MultivariateNormal(prior_mu, prior_cov)\ntheta\n\nMultivariateNormal(loc: torch.Size([2]), covariance_matrix: torch.Size([2, 2]))\n\n\n\ntheta.loc, theta.covariance_matrix\n\n(tensor([0., 0.]),\n tensor([[1., 0.],\n         [0., 1.]]))\n\n\n\n# Plot the prior in 2d contour\n\ntheta1 = torch.linspace(-3, 3, 100)\ntheta2 = torch.linspace(-3, 3, 100)\n\ntheta1, theta2 = torch.meshgrid(theta1, theta2)\n\ntheta_values = torch.stack((theta1, theta2), dim=-1)  # Shape: (100, 100, 2)\n\nz = theta.log_prob(theta_values.view(-1, 2))  # Shape: (10000,)\nz = z.view(100, 100)  # Reshape to (100, 100)\n\nplt.contourf(theta1.numpy(), theta2.numpy(), z.numpy(), 20)\nplt.gca().set_aspect('equal')\n\nplt.colorbar()\n\n&lt;matplotlib.colorbar.Colorbar at 0x7f40b71c0820&gt;\n\n\n\n\n\n\ntheta_sample = theta.sample((1000,))\ntheta_sample\n\ntensor([[-1.8783,  0.7921],\n        [ 0.7293,  0.1063],\n        [ 0.6363, -1.0957],\n        ...,\n        [-1.4223,  0.2716],\n        [ 1.2850,  1.7269],\n        [-0.0650,  0.0788]])\n\n\n\nx_lin = torch.linspace(-5, 5, 100)\ny_lin = theta_sample[:, 0].unsqueeze(1) + theta_sample[:, 1].unsqueeze(1) * x_lin.unsqueeze(0)\n\n\n_ = plt.plot(x_lin.numpy(), y_lin.numpy().T, alpha=0.1, color='red')\n\n\n\n\n\n# Mean prediction\ny_pred_mean = y_lin.mean(dim=0)\nplt.plot(x_lin.numpy(), y_pred_mean.numpy(), color='red', linewidth=3)\n\n# 95% confidence interval\ny_pred_std = y_lin.std(dim=0)\nplt.fill_between(x_lin.numpy(), (y_pred_mean - 2 * y_pred_std).numpy(), (y_pred_mean + 2 * y_pred_std).numpy(), alpha=0.3, color='red', label='95% confidence interval')\nplt.ylim(-12, 12)\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f40b67496a0&gt;\n\n\n\n\n\n\n# Plot the theta samples on the contour plot as scatter points\n\nplt.contourf(theta1.numpy(), theta2.numpy(), z.numpy(), 4, alpha=0.5)\nplt.gca().set_aspect('equal')\nplt.xlabel(r'$\\theta_0$')\nplt.ylabel(r'$\\theta_1$')\n\nplt.xlim(-3, 3)\nplt.ylim(-3, 3)\nplt.scatter(theta_sample[:, 0].numpy(), theta_sample[:, 1].numpy(), alpha=0.1, color='black')\nplt.tight_layout()\n\n/tmp/ipykernel_57304/1762244273.py:11: UserWarning: This figure was using constrained_layout, but that is incompatible with subplots_adjust and/or tight_layout; disabling constrained_layout.\n  plt.tight_layout()\n\n\n\n\n\n\n\n\nCleanShot 2023-06-05 at 17.47.27@2x.png\n\n\n\n### Noiseless case\n\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the prior parameters\nm0 = torch.tensor([0.0, 0.0])  # Mean of the prior\nS0 = torch.tensor([[1.0, 0.0], [0.0, 1.0]])  # Covariance matrix of the prior\nsigma = 0.0  # Noise parameter\n\n# Define the basis function (phi)\ndef phi(x):\n    return torch.tensor([1.0, x])\n\n# Function to calculate the prior predictive distribution\ndef prior_predictive(x_star, m0, S0, sigma):\n    phi_x_star = torch.stack([phi(x) for x in x_star])\n    predictive_mean = phi_x_star @ m0\n    predictive_cov = torch.diagonal(phi_x_star @ S0 @ phi_x_star.t(), offset=0) + sigma**2\n    return predictive_mean, predictive_cov\n\n# Generate x values for prediction\nx_star = torch.linspace(-5, 5, 100)\n\n# Calculate the prior predictive distribution\npredictive_mean, predictive_cov = prior_predictive(x_star, m0, S0, sigma)\n\n# Plot mean +- 2 std\nplt.plot(x_star.numpy(), predictive_mean.numpy(), color='red', linewidth=3)\nplt.fill_between(x_star.numpy(), (predictive_mean - 2 * torch.sqrt(predictive_cov)).numpy(), (predictive_mean + 2 * torch.sqrt(predictive_cov)).numpy(), alpha=0.3, color='red', label='95% confidence interval')\nplt.legend()\nplt.ylim(-12, 12)\n\n(-12.0, 12.0)\n\n\n\n\n\n\ny_pred_std\n\ntensor([5.2264, 5.1248, 5.0232, 4.9218, 4.8205, 4.7192, 4.6181, 4.5170, 4.4161,\n        4.3153, 4.2147, 4.1142, 4.0138, 3.9136, 3.8136, 3.7138, 3.6142, 3.5149,\n        3.4157, 3.3169, 3.2184, 3.1202, 3.0223, 2.9249, 2.8278, 2.7313, 2.6353,\n        2.5399, 2.4452, 2.3512, 2.2580, 2.1658, 2.0747, 1.9848, 1.8963, 1.8094,\n        1.7243, 1.6414, 1.5609, 1.4833, 1.4091, 1.3388, 1.2730, 1.2125, 1.1581,\n        1.1108, 1.0714, 1.0409, 1.0200, 1.0094, 1.0093, 1.0198, 1.0406, 1.0710,\n        1.1103, 1.1576, 1.2119, 1.2723, 1.3381, 1.4084, 1.4826, 1.5601, 1.6405,\n        1.7234, 1.8085, 1.8954, 1.9839, 2.0737, 2.1649, 2.2571, 2.3502, 2.4442,\n        2.5389, 2.6343, 2.7303, 2.8268, 2.9238, 3.0213, 3.1191, 3.2173, 3.3159,\n        3.4147, 3.5138, 3.6132, 3.7128, 3.8126, 3.9126, 4.0128, 4.1131, 4.2136,\n        4.3143, 4.4151, 4.5160, 4.6170, 4.7182, 4.8194, 4.9207, 5.0222, 5.1237,\n        5.2253])\n\n\n\ntorch.sqrt(predictive_cov)\n\ntensor([5.0990, 5.0000, 4.9011, 4.8022, 4.7035, 4.6048, 4.5063, 4.4079, 4.3095,\n        4.2114, 4.1133, 4.0154, 3.9177, 3.8201, 3.7227, 3.6255, 3.5285, 3.4318,\n        3.3353, 3.2390, 3.1431, 3.0475, 2.9523, 2.8575, 2.7631, 2.6692, 2.5758,\n        2.4830, 2.3909, 2.2995, 2.2090, 2.1194, 2.0309, 1.9437, 1.8578, 1.7735,\n        1.6910, 1.6107, 1.5328, 1.4577, 1.3859, 1.3180, 1.2546, 1.1963, 1.1440,\n        1.0985, 1.0607, 1.0314, 1.0114, 1.0013, 1.0013, 1.0114, 1.0314, 1.0607,\n        1.0985, 1.1440, 1.1963, 1.2546, 1.3180, 1.3859, 1.4577, 1.5328, 1.6107,\n        1.6910, 1.7735, 1.8578, 1.9437, 2.0309, 2.1194, 2.2090, 2.2995, 2.3909,\n        2.4830, 2.5758, 2.6692, 2.7631, 2.8575, 2.9523, 3.0475, 3.1431, 3.2390,\n        3.3353, 3.4318, 3.5285, 3.6255, 3.7227, 3.8201, 3.9177, 4.0154, 4.1133,\n        4.2114, 4.3095, 4.4079, 4.5063, 4.6048, 4.7035, 4.8022, 4.9011, 5.0000,\n        5.0990])\n\n\n\nComputing the evidence term\n\\[ I = \\int p(\\mathcal{D} \\mid \\theta) p(\\theta) \\mathrm{d}\\theta \\]\n\\[ I \\approx \\frac{1}{N} \\sum_{i=1}^N p(\\mathcal{D} \\mid \\theta_i) \\]\nwhere \\(\\theta_i \\sim p(\\theta)\\)\n\ntheta\n\nMultivariateNormal(loc: torch.Size([2]), covariance_matrix: torch.Size([2, 2]))\n\n\n\ntheta.sample((10,))\n\ntensor([[-0.7712,  1.9991],\n        [-0.6919, -0.3068],\n        [ 1.2983,  1.2461],\n        [-2.2901,  0.7368],\n        [-0.8971, -0.3141],\n        [ 0.0589,  0.6512],\n        [ 0.0549,  1.1111],\n        [-0.7357,  0.3042],\n        [-1.0303,  1.4628],\n        [-0.8665,  0.4207]])\n\n\n\ndef forward(x, theta):\n    return theta[:, 0].unsqueeze(1) + theta[:, 1].unsqueeze(1) * x.unsqueeze(0)\n\ndef likelihood(y, x, theta, sigma):\n    return torch.distributions.Normal(forward(x, theta), sigma).log_prob(y).sum(dim=1)\n\n\n\n\n# Use a different prior distribution: a mixture of two Gaussians\n\npis = torch.tensor([0.7, 0.3])\nnorm1 = torch.distributions.MultivariateNormal(torch.tensor([0.0, 0.0]), torch.tensor([[2.0, 0.5], [0.5, 1.0]]))\nnorm2 = torch.distributions.MultivariateNormal(torch.tensor([3.0, -2.0]), torch.tensor([[1.0, 0.0], [0.0, 2.0]]))\n\nmog = torch.distributions.MixtureSameFamily(torch.distributions.Categorical(pis), \n                                            torch.distributions.MultivariateNormal(torch.stack((norm1.loc, norm2.loc)),\n                                             torch.stack((norm1.covariance_matrix, norm2.covariance_matrix))))\n\ntheta = mog.sample((1000,))\n\n# Plot the samples\n\n\n# Plot the prior in 2d contour\n\ntheta1 = torch.linspace(-8, 8, 100)\ntheta2 = torch.linspace(-8, 8, 100)\n\ntheta1, theta2 = torch.meshgrid(theta1, theta2)\n\ntheta_values = torch.stack((theta1, theta2), dim=-1)  # Shape: (100, 100, 2)\n\nz = mog.log_prob(theta_values.view(-1, 2))  # Shape: (10000,)\nz = z.view(100, 100)  # Reshape to (100, 100)\n\nplt.contourf(theta1.numpy(), theta2.numpy(), z.numpy(), 20)\nplt.gca().set_aspect('equal')\n\nplt.scatter(theta[:, 0].numpy(), theta[:, 1].numpy(), alpha=0.1, color='black')\n\n\n&lt;matplotlib.collections.PathCollection at 0x7f40c45a2e50&gt;\n\n\n\n\n\n\ntheta_sample = mog.sample((1000,))\ny_lin = theta_sample[:, 0].unsqueeze(1) + theta_sample[:, 1].unsqueeze(1) * x_lin.unsqueeze(0)\n\n\n_ = plt.plot(x_lin.numpy(), y_lin.numpy().T, alpha=0.01, color='black')\n\n# Plot the last sample to see the line and mark the label\n_ = plt.plot(x_lin.numpy(), y_lin.numpy()[-1], alpha=0.01, color='black', label='Samples')\n\n# Mean prediction\ny_pred_mean = y_lin.mean(dim=0)\nplt.plot(x_lin.numpy(), y_pred_mean.numpy(), color='red', linewidth=3)\n\n# 95% confidence interval\ny_pred_std = y_lin.std(dim=0)\nplt.fill_between(x_lin.numpy(), (y_pred_mean - 2 * y_pred_std).numpy(), (y_pred_mean + 2 * y_pred_std).numpy(), alpha=0.3, color='red', label='95% confidence interval')\n\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f40c4615790&gt;"
  },
  {
    "objectID": "notebooks/rejection-sampling-importance-sampling.html",
    "href": "notebooks/rejection-sampling-importance-sampling.html",
    "title": "Probabilistic Machine Learning",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n%matplotlib inline\n\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\n\n## Rejection sampling\n\nimport torch.distributions as D\n\nmix = D.Categorical(torch.ones(2,))\ncomp = D.Normal(torch.tensor([-0.5, 1.0]), torch.tensor([0.3, 0.5]))\nmog = D.MixtureSameFamily(mix, comp)\n\n\ncomp.scale\n\ntensor([0.3000, 0.5000])\n\n\n\n# Plot the mizxture of Gaussians\nxs = torch.linspace(-3, 3, 100)\nplt.plot(xs, mog.log_prob(xs).exp())\n\n\n\n\n\n# Take a proposal distribution q(x) = N(0, 1)\nq = D.Normal(0, 1)\n\n\n# Let $M$ be a constant such that $M \\geq \\frac{p(x)}{q(x)} \\forall x$.\n\nM = torch.max(mog.log_prob(xs) - q.log_prob(xs)).exp()\nM\n\ntensor(1.9451)\n\n\n\ntorch.argmax(mog.log_prob(xs) - q.log_prob(xs))\n\ntensor(72)\n\n\n\nxs[31]\n\ntensor(-1.1212)\n\n\n\ndef plot_base():\n    # Plot the mixture of Gaussians\n    \n    plt.legend()\nplot_base()\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\n\n# Sample from the proposal distribution and accept or reject.\n# Accepted shown in green, rejected shown in red.\n\ndef plot_sample(x, show_q = False, show_Mq = False, show_sample=False, show_vline = False,\n                  show_px=False, show_Mqx=False, show_uMqx=False, show_accept=False):\n    xs = torch.linspace(-3, 3, 100)\n    plt.plot(xs, mog.log_prob(xs).exp(), label=r'$\\tilde{p}(x)$', color='C0')\n    plt.title(r\"Target distribution $\\tilde{p}(x)$\")\n\n    if show_q:\n        # Plot the proposal distribution\n        plt.plot(xs, q.log_prob(xs).exp(), label='$q(x)$', color='C1')\n        plt.title(r\"Proposal distribution $q(x)$\")\n\n    if show_Mq:\n        # Plot the scaled proposal distribution\n        plt.plot(xs, M * q.log_prob(xs).exp(), label='$Mq(x)$', color='C2', linestyle='--')\n        plt.title(\"Scaled proposal distribution Mq(x)\")\n    if show_sample:\n        plt.scatter(x, 0,marker='x', color='k', label = r\"$x\\sim q(x)$\")\n        plt.title(\"Sample from proposal distribution\")\n    \n    if show_vline:\n        plt.axvline(x, color='C3', linestyle='--')\n        plt.title(\"Sample from proposal distribution\")\n\n    if show_px:\n        plt.scatter(x, mog.log_prob(x).exp(), color='C4', label=r\"$\\tilde{p}(x)$\")\n        plt.title(r\"Evaluate target distribution $\\tilde{p}(x)$ at sample x\")\n\n    \n    if show_Mqx:\n        plt.scatter(x, M * q.log_prob(x).exp(), color='k', label=r\"$Mq(x)$\")\n        plt.title(r\"Evaluate scaled proposal distribution $Mq(x)$ at sample x\")\n    \n    if show_uMqx:\n        torch.manual_seed(0)\n        u = torch.rand(1)\n        plt.scatter(x, u * M * q.log_prob(x).exp(), label=r\"$uMq(x)$\", color='purple')\n        plt.title(r\"Draw a uniform u between 0 and 1 and evaluate $uMq(x)$ at sample x\")\n    \n    if show_accept:\n        if u * M * q.log_prob(x).exp() &lt; mog.log_prob(x).exp():\n            plt.scatter(x, u * M * q.log_prob(x).exp(), label=r\"Accepted\", color='g')\n            plt.title(r\"Accept sample as $uMq(x)$ $&lt;$ $\\tilde{p}(x)$\")\n        else:\n            plt.scatter(x, u * M * q.log_prob(x).exp(), label=r\"Rejected\", color='r')\n            plt.title(r\"Reject sample as $uMq(x)$ $&gt;$ $\\tilde{p}(x)$\")\n    \n    plt.ylim(-.05, 1.0)\n    plt.legend()\n    fn  = f\"../figures/sampling/rejection-sampling-{x:0.1f}-{show_q}-{show_Mq}-{show_sample}-{show_vline}-{show_px}-{show_Mqx}-{show_uMqx}-{show_accept}\"\n    plt.savefig(fn + \".pdf\", bbox_inches='tight')\n    plt.savefig(fn + \".png\", bbox_inches='tight', dpi=600)\n\n\nplot_sample(torch.tensor(-1.0))\n\nfindfont: Font family ['cursive'] not found. Falling back to DejaVu Sans.\nfindfont: Generic family 'cursive' not found because none of the following families were found: Apple Chancery, Textile, Zapf Chancery, Sand, Script MT, Felipa, Comic Neue, Comic Sans MS, cursive\n\n\n\n\n\n\nplot_sample(torch.tensor(-1.0), show_q=True)\n\n\n\n\n\nplot_sample(torch.tensor(-1.0), show_q=True, show_Mq=True)\n\n\n\n\n\nplot_sample(torch.tensor(-1.0), show_q=True, show_Mq=True, show_sample=True)\n\n\n\n\n\nplot_sample(torch.tensor(-1.0), show_q=True, show_Mq=True, show_sample=True, show_vline=True)\n\n\n\n\n\nplot_sample(torch.tensor(-1.0), show_q=True, show_Mq=True, show_sample=True, show_vline=True, show_px=True)\n\n\n\n\n\nplot_sample(torch.tensor(-1.0), show_q=True, show_Mq=True, show_sample=True, show_vline=True, show_px=True, show_Mqx=True)\n\n\n\n\n\nplot_sample(torch.tensor(-1.0), show_q=True, show_Mq=True, show_sample=True, show_vline=True, show_px=True, show_Mqx=True, show_uMqx=True)\n\n\n\n\n\nplot_sample(torch.tensor(-1.0), show_q=True, show_Mq=True, show_sample=True, show_vline=True, show_px=True, show_Mqx=True, show_uMqx=True, show_accept=True)\n\n\n\n\n\nplot_sample(torch.tensor(-0.2), show_q=True, show_Mq=True, show_sample=True, show_vline=True, show_px=True, show_Mqx=True, show_uMqx=True, show_accept=True)\n\n\n\n\n\n# Create an animation out of the .png generated above\nimport os\nimport imageio\nimport glob\n\nimages = []\n# Get all the pngs in the figures directory\nfs = sorted(glob.glob('../figures/sampling/rejection-sampling*.png'))\nfor filename in fs:\n    ist = imageio.imread(filename)\n    images.append(ist)\n    # Print image size to make sure they are all the same size\n    print(ist.shape)\n\n# Save with high resolution\nimageio.mimsave('../figures/sampling/rejection-sampling.gif', images, duration=0.6)\n# save as mp4\n# os.system(\"ffmpeg -i figures/sampling/rejection-sampling.gif figures/sampling/rejection-sampling.mp4\")\n\n\n/tmp/ipykernel_678943/3473788105.py:10: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n  ist = imageio.imread(filename)\n\n\n(1456, 3275, 4)\n(1456, 3275, 4)\n(1456, 3275, 4)\n(1456, 3275, 4)\n(1456, 3275, 4)\n(1456, 3275, 4)\n(1456, 3275, 4)\n(1456, 3275, 4)\n(1456, 3275, 4)\n(1456, 3275, 4)\n(1456, 3275, 4)\n(1456, 3275, 4)\n(1456, 3275, 4)\n(1456, 3275, 4)\n\n\n\n\nplot_sample(torch.tensor(-0.5), show_q=True, show_Mq=True, show_sample=True, show_vline=True, show_px=True, show_Mqx=True, show_uMqx=True, show_accept=True)\n\n\n\n\n\nN = 1000\n\ndef plot_N_samples(N=100, seed=0, plot_kde=False):\n    torch.manual_seed(seed)\n    # Now, run the algorithm for a few iterations and plot the results\n    samples_from_q = q.sample((N,))\n\n    # Evaluate the target distribution at the samples\n    pxs = mog.log_prob(samples_from_q).exp()\n\n    # Evaluate the scaled proposal distribution at the samples\n    Mqxs = M * q.log_prob(samples_from_q).exp()\n\n    # Draw a uniform u between 0 and 1\n    us = torch.rand(N)\n\n    # Accept or reject the samples\n    accepted = us * Mqxs &lt; pxs\n\n    # Plot p, q, and Mq\n    plt.plot(xs, mog.log_prob(xs).exp(), label=r'$\\tilde{p}(x)$', lw=2)\n    #plt.plot(xs, q.log_prob(xs).exp(), label='q(x)')\n    plt.plot(xs, M * q.log_prob(xs).exp(), label=r'$Mq(x)$', lw=2)\n\n    if not plot_kde:\n\n        # Plot the sample as red if it was rejected, and green if it was accepted, height is p(x)\n        #plt.scatter(samples_from_q, pxs, color='r', label='Rejected samples')\n        plt.scatter(samples_from_q[accepted], (us*Mqxs)[accepted], color='g', label='Accepted samples', alpha=0.2, marker='.', s=20)\n        plt.scatter(samples_from_q[~accepted], (us*Mqxs)[~accepted], color='r', label='Rejected samples', alpha=0.2, marker='.', s=20)\n        plt.legend()\n\n    if plot_kde:\n        import seaborn as sns\n        sns.kdeplot(samples_from_q[accepted].numpy(), color='g', label='Density of accepted samples', lw=2)\n        plt.legend()\n    plt.title(f\"Rejection sampling with N={N} samples\\n Acceptance rate: {accepted.float().mean().item():.2f}\")\n    fn = f\"../figures/sampling/rejection-sampling-N{N}-{plot_kde}.pdf\"\n    plt.savefig(fn, bbox_inches='tight')\n    print(fn)\n\nplot_N_samples(N=1000, seed=0)\n\n../figures/sampling/rejection-sampling-N1000-False.pdf\n\n\n\n\n\n\nplot_N_samples(N=1000, seed=0, plot_kde=True)\n\n../figures/sampling/rejection-sampling-N1000-True.pdf\n\n\n\n\n\n\nplot_N_samples(N=10, seed=0, plot_kde=True)\n\n../figures/sampling/rejection-sampling-N10-True.pdf\n\n\n\n\n\n\nplot_N_samples(N=10, seed=0, plot_kde=False)\n\n../figures/sampling/rejection-sampling-N10-False.pdf\n\n\n\n\n\n\nplot_N_samples(N=10000, seed=0, plot_kde=True)\n\n../figures/sampling/rejection-sampling-N10000-True.pdf\n\n\n\n\n\n\nplot_N_samples(N=10000, seed=0, plot_kde=False)\n\n../figures/sampling/rejection-sampling-N10000-False.pdf\n\n\n\n\n\n\n# Gaussian p and q\n\nsigma_p = 1.0\nsigma_q = 1.1\n\nDIM = 1\n\np = D.MultivariateNormal(torch.zeros(DIM), sigma_p**2 * torch.eye(DIM))\nq = D.MultivariateNormal(torch.zeros(DIM), sigma_q**2 * torch.eye(DIM))\n\n\np\n\nMultivariateNormal(loc: tensor([0.]), covariance_matrix: tensor([[1.]]))\n\n\n\nq\n\nMultivariateNormal(loc: tensor([0.]), covariance_matrix: tensor([[1.2100]]))\n\n\n\n# plot the distributions\n\nxs = torch.linspace(-3, 3, 100).view(-1, 1)\nplt.plot(xs, p.log_prob(xs).exp(), label=r'$\\tilde{p}(x)$')\nplt.plot(xs, q.log_prob(xs).exp(), label=r'$q(x)$')\nplt.legend()\nplt.savefig('../figures/sampling/rejection-sampling-gaussian-p-q.pdf', bbox_inches='tight')\n\n# Compute the constant M\n\n\n\n\n\nM_emp = torch.max(p.log_prob(xs) - q.log_prob(xs)).exp()\nM_emp\n\ntensor(1.0999)\n\n\n\nM =  sigma_q/sigma_p   \nM \n\n1.1\n\n\n\n# Now, plot for varying D\n\nMs = {}\nfor DIM in [1, 2, 5, 10, 20, 50, 100]:\n    Ms[DIM] = (sigma_q/sigma_p)**DIM\n\n\nMs\n\n{1: 1.1,\n 2: 1.2100000000000002,\n 5: 1.6105100000000006,\n 10: 2.5937424601000023,\n 20: 6.727499949325611,\n 50: 117.39085287969579,\n 100: 13780.61233982238}\n\n\n\nimport pandas as pd\npd.Series(Ms).plot(logy=True, marker='o')\nplt.xlabel(\"Dimensionality\")\nplt.ylabel(\"M (log scale)\")\nplt.savefig('../figures/sampling/rejection-sampling-gaussian-p-q-M.pdf', bbox_inches='tight')\n\n\n\n\n\nimport pandas as pd\nnew_series = 1/pd.Series(Ms)\nnew_series.plot(logy=True, marker='o')\nplt.xlabel(\"Dimensionality\")\nplt.ylabel(\"Acceptance rate (log scale)\")\nplt.savefig('../figures/sampling/rejection-sampling-gaussian-p-q-acceptance.pdf', bbox_inches='tight')\n\n\n\n\n\nmog\n\nMixtureSameFamily(\n  Categorical(probs: torch.Size([2]), logits: torch.Size([2])),\n  Normal(loc: torch.Size([2]), scale: torch.Size([2])))\n\n\n\n# Plot the mizxture of Gaussians\nxs = torch.linspace(-3, 3, 100)\nplt.plot(xs, mog.log_prob(xs).exp(), label=r'$p(x)$', color='C0')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f555c8e1070&gt;\n\n\n\n\n\n\n# Let f(x) = x^2\n\ndef f(x):\n    return (10-x**2)/20.0\n\n\nplt.plot(xs, mog.log_prob(xs).exp(), label=r'$p(x)$', color='C0')\n# Plot f(x)\nplt.plot(xs, f(xs), label=r'$f(x)$', color='C1')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f555c9e5be0&gt;\n\n\n\n\n\n\n# Let q(x) = N(0, 1)\nq = D.Normal(0, 1)\n\n\nplt.plot(xs, mog.log_prob(xs).exp(), label=r'$p(x)$', color='C0')\n# Plot f(x)\nplt.plot(xs, f(xs), label=r'$f(x)$', color='C1')\n# Plot q(x)\nplt.plot(xs, q.log_prob(xs).exp(), label=r'$q(x)$', color='C2')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f555c9482b0&gt;\n\n\n\n\n\n\n# Get a sample from q(x) and evaluate w(x) = f(x)/q(x) and evaluate f(x) at the sample\n\nx = q.sample()\n\ndef plot_sample(x):\n    w = mog.log_prob(x).exp() / q.log_prob(x).exp()\n\n    plt.plot(xs, mog.log_prob(xs).exp(), label=r'$p(x)$', color='C0')\n    # Plot f(x)\n    plt.plot(xs, f(xs), label=r'$f(x)$', color='C1')\n    # Plot q(x)\n    plt.plot(xs, q.log_prob(xs).exp(), label=r'$q(x)$', color='C2')\n    # Plot sample x\n    plt.scatter(x, 0, marker='x', color='k', label=r\"$x\\sim q(x)$\")\n    # Draw vertical line at x\n    plt.axvline(x, color='k', linestyle='--')\n    \n    # Put title containing sample x, f(x), q(x), w(x), f(x)*w(x)\n    plt.title(f\"Sample x={x:0.3f}, f(x)={f(x):0.3f}, q(x)={q.log_prob(x).exp():0.3f}, w(x)={w:0.3f}, f(x)*w(x)={f(x)*w:0.3f}\")    \n    plt.legend()\n\n\n\n# Case 1: p(x) and q(x) are comparable and high\nplot_sample(torch.tensor(-0.9))\n\n\n\n\n\n# Case 2: p(x) is low and q(x) is high\n# In case of rejection sampling, we would reject this sample\nplot_sample(torch.tensor(-1.5))\n\n\n\n\n\n# Case 3: p(x) is high and q(x) is low\n# Rare sample that we would accept in rejection sampling so high weight\nplot_sample(torch.tensor(-0.5))\n\n\n\n\n\n# Case 4: p(x) is low and q(x) is low\nplot_sample(torch.tensor(2.5))\n\n\n\n\n\nplt.plot(xs, mog.log_prob(xs).exp(), label=r'$p(x)$', color='C0')\n# Plot f(x)\nplt.plot(xs, f(xs), label=r'$f(x)$', color='C1')\n# Plot q(x)\nplt.plot(xs, q.log_prob(xs).exp(), label=r'$q(x)$', color='C2')\n# Plot the weight function w(x) = p(x)/q(x)\nplt.plot(xs, mog.log_prob(xs).exp() / q.log_prob(xs).exp(), label=r'$w(x)$', color='C3')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f555c547850&gt;"
  },
  {
    "objectID": "notebooks/log_reg_slider.html",
    "href": "notebooks/log_reg_slider.html",
    "title": "Probabilistic Machine Learning",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output\nfrom matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n%matplotlib inline\n# Retina display\n%config InlineBackend.figure_format = 'retina'\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\n\nplt.rcParams[\"font.family\"] = \"Arial\"\n\n\nnp.random.seed(0)\nX = np.random.rand(60, 1)\ny = np.where(X &gt; 0.5, 1, 0)\n\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\n\ndef compute_log_likelihood(theta0, theta1):\n    h = sigmoid(theta0 + theta1 * X)\n    log_likelihood = np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n    return log_likelihood\n\n\ntheta0_vals = np.linspace(-6, 2, 100)\ntheta1_vals = np.linspace(2, 6, 100)\ntheta0_grid, theta1_grid = np.meshgrid(theta0_vals, theta1_vals)\nloglik_grid = np.zeros_like(theta0_grid)\n\nfor i in range(len(theta0_vals)):\n    for j in range(len(theta1_vals)):\n        loglik_grid[i, j] = compute_log_likelihood(\n            theta0_vals[i], theta1_vals[j])\n\ntheta0_slider = widgets.FloatSlider(\n    value=1, min=-6, max=2, step=0.1, description='Theta0:')\ntheta1_slider = widgets.FloatSlider(\n    value=1, min=2, max=6, step=0.1, description='Theta1:')\n\nfigure_container = widgets.Output(layout={'border': '1px solid black'})\n\n\ndef update_figure(change):\n    with figure_container:\n        clear_output(wait=True)\n\n        plt.subplot(1, 2, 1)\n        plt.contourf(theta0_grid, theta1_grid, loglik_grid,\n                     levels=20, cmap='viridis')\n        plt.colorbar(label='Log Likelihood')\n\n        true_theta0 = -2.0\n        true_theta1 = 4.0\n        plt.scatter([true_theta0], [true_theta1], color='blue',\n                    marker='o', label='True Values')\n        plt.scatter([theta0_slider.value], [theta1_slider.value],\n                    color='red', marker='x', label='Current Values')\n\n        plt.xlabel('Theta0')\n        plt.ylabel('Theta1')\n        plt.title('Log Likelihood Contour Plot')\n        plt.legend()\n\n        plt.subplot(1, 2, 2)\n        plt.scatter(X, y, c=y, cmap=ListedColormap(\n            ['#FF0000', '#0000FF']), edgecolors='k', marker='o')\n\n        if theta1_slider.value != 0:\n            plt.plot([0, 1], [-theta0_slider.value/theta1_slider.value, (1-theta0_slider.value) /\n                     theta1_slider.value], color='red', label='Decision Boundary')\n        else:\n            plt.axvline(x=theta0_slider.value, color='red',\n                        linestyle='--', label='Decision Boundary')\n\n        plt.xlabel('X')\n        plt.ylabel('Class')\n        plt.title('Logistic Regression Decision Boundary')\n        plt.legend()\n\n        # plt.tight_layout()\n        plt.show()\n\n\ntheta0_slider.observe(update_figure, 'value')\ntheta1_slider.observe(update_figure, 'value')\n\nupdate_figure(None)\n\ndisplay(widgets.HBox(\n    [figure_container, widgets.VBox([theta0_slider, theta1_slider])]))\n\n\n\n\n\nplt.subplot(1, 2, 1)\nplt.contourf(theta0_grid, theta1_grid, loglik_grid, levels=20, cmap='viridis')\nplt.colorbar(label='Log Likelihood')\n\ntrue_theta0 = -2.0\ntrue_theta1 = 4.0\nplt.scatter([true_theta0], [true_theta1], color='blue',\n            marker='o', label='True Values')\nplt.scatter([-1], [3], color='red', marker='x', label='Current Values')\n\nplt.xlabel('Theta0')\nplt.ylabel('Theta1')\nplt.title('Log Likelihood Contour Plot')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.scatter(X, y, c=y, cmap=ListedColormap(\n    ['#FF0000', '#0000FF']), edgecolors='k', marker='o')\n\nif theta1_slider.value != 0:\n    plt.plot([0, 1], [1/3, (1+1)/3], color='red', label='Decision Boundary')\nelse:\n    plt.axvline(x=-1, color='red', linestyle='--', label='Decision Boundary')\n\nplt.xlabel('X')\nplt.ylabel('Class')\nplt.title('Logistic Regression Decision Boundary')\nplt.legend()\n\n# plt.tight_layout()\nplt.savefig('figures/mle/log_reg_slider_3.pdf')"
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Probabilistic Machine Learning",
    "section": "",
    "text": "File Name\n\n\nModified\n\n\n\n\n\n\nActive-Learning.pdf\n\n\n11/23/23, 4:11:16 AM\n\n\n\n\nblr_0.pdf\n\n\n11/23/23, 4:11:16 AM\n\n\n\n\nblr_1.pdf\n\n\n11/23/23, 4:11:16 AM\n\n\n\n\nblr_10.pdf\n\n\n11/23/23, 4:11:16 AM\n\n\n\n\nblr_100.pdf\n\n\n11/23/23, 4:11:16 AM\n\n\n\n\nblr_2.pdf\n\n\n11/23/23, 4:11:16 AM\n\n\n\n\nblr_20.pdf\n\n\n11/23/23, 4:11:16 AM\n\n\n\n\nblr_3.pdf\n\n\n11/23/23, 4:11:16 AM\n\n\n\n\nblr_5.pdf\n\n\n11/23/23, 4:11:16 AM\n\n\n\n\nblr_mle_map.pdf\n\n\n11/23/23, 4:11:16 AM\n\n\n\n\nmain.pdf\n\n\n11/23/23, 4:11:16 AM\n\n\n\n\nmvn-blr.pdf\n\n\n11/23/23, 4:11:16 AM\n\n\n\n\nBayesian-Optimization.pdf\n\n\n11/23/23, 4:11:16 AM\n\n\n\n\nInformation-Theory.pdf\n\n\n11/23/23, 4:11:16 AM\n\n\n\n\nMCMC.pdf\n\n\n11/23/23, 4:11:16 AM\n\n\n\n\nMLE.pdf\n\n\n11/23/23, 4:11:16 AM\n\n\n\n\nMLE_Lin_Log_Reg.pdf\n\n\n11/23/23, 4:11:16 AM\n\n\n\n\nMLE_updated.pdf\n\n\n11/23/23, 4:11:16 AM\n\n\n\n\nNeural-Processes.pdf\n\n\n11/23/23, 4:11:16 AM\n\n\n\n\nProb-refresher.pdf\n\n\n11/23/23, 4:11:16 AM\n\n\n\n\nSampling-1.pdf\n\n\n11/23/23, 4:11:16 AM\n\n\n\n\nSampling-2.pdf\n\n\n11/23/23, 4:11:16 AM\n\n\n\n\nSampling.pdf\n\n\n11/23/23, 4:11:16 AM\n\n\n\n\nSampling_haikoo_sarth.pdf\n\n\n11/23/23, 4:11:16 AM\n\n\n\n\nVariational-Inference.pdf\n\n\n11/23/23, 4:11:16 AM\n\n\n\n\nbayesian-logistic-regression.pdf\n\n\n11/23/23, 4:11:16 AM\n\n\n\n\ncalculus-terms.pdf\n\n\n11/23/23, 4:11:16 AM\n\n\n\n\nhmm.pdf\n\n\n11/23/23, 4:11:17 AM\n\n\n\n\nintroduction.pdf\n\n\n11/23/23, 4:11:17 AM\n\n\n\n\nlaplace-approximation.pdf\n\n\n11/23/23, 4:11:17 AM\n\n\n\n\nlogistics.pdf\n\n\n11/23/23, 4:11:17 AM\n\n\n\n\nmadhav-introduction.pdf\n\n\n11/23/23, 4:11:17 AM\n\n\n\n\nmap.pdf\n\n\n11/23/23, 4:11:17 AM\n\n\n\n\nmle-old.pdf\n\n\n11/23/23, 4:11:17 AM\n\n\n\n\nmvn-blr.pdf\n\n\n11/23/23, 4:11:17 AM\n\n\n\n\ntest.pdf\n\n\n11/23/23, 4:11:17 AM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Probabilistic Machine Learning",
    "section": "",
    "text": "This website hosts the materials taught in the Probabilistic Machine Learning course."
  },
  {
    "objectID": "notebooks.html",
    "href": "notebooks.html",
    "title": "Probabilistic Machine Learning",
    "section": "",
    "text": "1D Taylor approximation\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nActive Learning\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nBaseball\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nBasic Imports\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nBasis Functions Regression\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nBayes Librarian\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nBayesOpt Motivation\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nBayesian Linear Regression\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nBayesian Linear Regression Posterior Figures\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nBayesian Logistic Regression\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nBayesian Optimization\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nBiased and Unbiased Estimators\n\n\n\n\n\nBiased and Unbiased Estimators\n\n\n\n\n\n\nJul 12, 2023\n\n\nNipun Batra\n\n\n\n\n\n\n  \n\n\n\n\nCIFAR10 and CIFAR100 dataset\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nCalculus\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nCapturing uncertainty in neural nets:\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nCapturing uncertainty in neural nets:\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nCapturing uncertainty in neural nets:\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nClosed form solution for prior predictive distribution\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nClosed form solution for prior predictive distribution\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nComputing the evidence term\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nDiscrete distributions\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nError Code Dilbert\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nGibbs Sampling for Bivariate Normal Distribution\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nGoals:\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nGoals:\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nHeteroskedastic N\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nHypernetwork\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nImportance Sampling\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nImports\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nInverse CDF Sampling\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nLinear Regression Tutorial\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nMonte Carlo Dropout and Deep Ensemble for Bayesian Deep Learning\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nMonte Carlo Sampling\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nOverconfident Neural Networks\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nParameters\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nSampling from an unnormalized distribution\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nSetting Up & Imports\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nSetting Up & Imports\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nTrain on all tasks\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nVanilla Linear Regression\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nVariance in Value of Pi in MC Sampling\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "notebooks/bayesopt-motivation.html",
    "href": "notebooks/bayesopt-motivation.html",
    "title": "BayesOpt Motivation",
    "section": "",
    "text": "import torch\nimport matplotlib.pyplot as plt\nimport torch.distributions as dist\nimport copy\nfrom functools import partial\n# Matplotlib retina\n%config InlineBackend.figure_format = 'retina'\n\nfrom sklearn.metrics import accuracy_score\nimport torch.nn.functional as F\nimport pandas as pd\nimport numpy as np\n\n\n# From: https://glowingpython.blogspot.com/2017/04/solving-two-spirals-problem-with-keras.html\ndef twospirals(n_points, noise=1.7):\n    \"\"\"\n     Returns the two spirals dataset.\n    \"\"\"\n    n = np.sqrt(np.random.rand(n_points,1)) * 780 * (2*np.pi)/360\n    d1x = -np.cos(n)*n + np.random.rand(n_points,1) * noise\n    d1y = np.sin(n)*n + np.random.rand(n_points,1) * noise\n    return (np.vstack((np.hstack((d1x,d1y)),np.hstack((-d1x,-d1y)))), \n            np.hstack((np.zeros(n_points),np.ones(n_points))))\n\nX, y = twospirals(1000)\n\nplt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.Spectral)\n\n&lt;matplotlib.collections.PathCollection at 0x7f382737a190&gt;\n\n\n\n\n\n\n# Divide the dataset into train, test and validation\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=0)\n\n\nX_train.shape, X_test.shape, X_val.shape\n\n((1125, 2), (500, 2), (375, 2))\n\n\n\n# Move to torch tensors\nX_train = torch.tensor(X_train, dtype=torch.float)\ny_train = torch.tensor(y_train, dtype=torch.float)\n\nX_test = torch.tensor(X_test, dtype=torch.float)\ny_test = torch.tensor(y_test, dtype=torch.float)\n\nX_val = torch.tensor(X_val, dtype=torch.float)\ny_val = torch.tensor(y_val, dtype=torch.float)\n\n\n# Create a simple MLP with hyperparameter being the regularization strength\n\nclass SimpleMLP(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(SimpleMLP, self).__init__()\n        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n        self.fc3 = torch.nn.Linear(hidden_dim, output_dim)\n        \n    def forward(self, x):\n        x = torch.sin(self.fc1(x))\n        x = torch.sin(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n# Define a function to train and evaluate a model with a given regularization rate\ndef train_and_evaluate_model(lam):\n    model = SimpleMLP(input_dim=2, hidden_dim=80, output_dim=1)\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    criterion = torch.nn.BCEWithLogitsLoss()\n\n    epochs = 500\n    for epoch in range(epochs):\n        # Forward pass\n        outputs = model(X_train)\n        loss = criterion(outputs, y_train.unsqueeze(1))\n\n        # L2 regularization\n        l2_reg = torch.tensor(0.)\n        for param in model.parameters():\n            l2_reg += torch.norm(param)\n\n        loss += lam * l2_reg\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    # Evaluate on validation set\n    with torch.no_grad():\n        val_outputs = model(X_val)\n        val_preds = torch.round(torch.sigmoid(val_outputs))\n        accuracy = accuracy_score(y_val.numpy(), val_preds.numpy())\n\n    return accuracy\n\n\n# Define a range of regularization rates to try\nlog_reg_rates = [-1000, -6, -5, -4, -3, -2, -1.9, -1.8, -1, 0, 1]\nreg_rates = [10**x for x in log_reg_rates]\n\naccuracies= {}\n\n# Train and evaluate models for each regularization rate\nfor lam in reg_rates:\n    accuracy = train_and_evaluate_model(lam)\n    accuracies[lam] = accuracy\n    print(f'Regularization Rate: {lam}, Validation Accuracy: {accuracy}')\n\nRegularization Rate: 0.0, Validation Accuracy: 0.872\nRegularization Rate: 1e-06, Validation Accuracy: 0.8773333333333333\nRegularization Rate: 1e-05, Validation Accuracy: 0.8773333333333333\nRegularization Rate: 0.0001, Validation Accuracy: 0.8826666666666667\nRegularization Rate: 0.001, Validation Accuracy: 0.8773333333333333\nRegularization Rate: 0.01, Validation Accuracy: 0.8773333333333333\nRegularization Rate: 0.012589254117941675, Validation Accuracy: 0.896\nRegularization Rate: 0.015848931924611134, Validation Accuracy: 0.8933333333333333\nRegularization Rate: 0.1, Validation Accuracy: 0.8693333333333333\nRegularization Rate: 1, Validation Accuracy: 0.49066666666666664\nRegularization Rate: 10, Validation Accuracy: 0.5093333333333333\n\n\n\nser = pd.Series(accuracies)\nser.plot(logx=True, style='o-')\n\n&lt;AxesSubplot:&gt;"
  },
  {
    "objectID": "notebooks/bayesopt.html",
    "href": "notebooks/bayesopt.html",
    "title": "Bayesian Optimization",
    "section": "",
    "text": "import torch\nimport matplotlib.pyplot as plt\nimport torch.distributions as dist\nimport copy\nfrom functools import partial\n# Matplotlib retina\n%config InlineBackend.figure_format = 'retina'\n\n\nx_lin  = torch.linspace(-3, 3, 100).reshape(-1, 1)\n#f = lambda x: 2*torch.sin(x) + torch.sin(2*x**(1.1) + 1) + 0.3*x + 10\nf = lambda x: 2*torch.sin(x) + 0.5*torch.sin(3*x + 0.5) + 5\n\nplt.plot(x_lin, f(x_lin), label='f(x)')\n\n\n\n\n\ndef phi(x, degree=2):\n    # sin features\n    # [1, x, sin(x), sin(2x), ..., sin(degree*x)]\n    # if degree=1, then [1, x]\n    # if degree=2, then [1, x, sin(x)]\n    # if degree=3, then [1, x, sin(x), sin(2x)]\n    \n    x_new = torch.cat([torch.ones_like(x), x], dim=1)\n    for d in range(2, degree+1):\n        x_new = torch.cat([x_new, torch.sin(d*x), torch.cos(d*x), torch.sin(d*x + .5), torch.cos(d*x + 0.5)], dim=1)\n    return x_new\n\ndef num_params(degree):\n    if degree == 1:\n        return 2 #[1, x]\n    else: \n        return 2 + 4*(degree-1)\n\n\nclass BLR:\n    def __init__(self, mu, sigma, sigma_noise, degree=2):\n        self.current_mean = mu\n        self.current_sigma = sigma\n        self.sigma_noise = sigma_noise  # Add sigma_noise as an instance variable\n        self.is_prior_set = True\n        self.N = 0\n        self.X_total = None\n        self.y_total = None\n        self.degree = degree\n    \n    def __repr__(self):\n        return f'BLR(mu={self.current_mean},\\n sigma={self.current_sigma}, \\nsigma_noise={self.sigma_noise})'\n\n    def update(self, X, y):\n        \"\"\"\n        X: (n_points, n_features)\n        y: (n_points, 1)\n        \"\"\"\n        \n        if not self.is_prior_set:\n            raise Exception('Prior not set')\n        n_points = X.shape[0]\n        X_orig = X\n        X = phi(X, self.degree)\n        self.current_sigma_inverse = torch.inverse(self.current_sigma)\n        XTX = torch.matmul(X.T, X)\n        SN_inverse = self.current_sigma_inverse + XTX / self.sigma_noise**2\n        SN = torch.inverse(SN_inverse)\n\n        self.current_mean = torch.matmul(SN, torch.matmul(self.current_sigma_inverse, self.current_mean) + torch.matmul(X.T, y).ravel() / self.sigma_noise**2)\n        self.current_sigma = SN\n        self.N += n_points\n        if self.X_total is None:\n            self.X_total = X_orig\n            self.y_total = y\n        else:\n            self.X_total = torch.cat((self.X_total, X_orig), 0)\n            self.y_total = torch.cat((self.y_total, y), 0)\n\n    def predict(self, X):\n        if not self.is_prior_set:\n            raise Exception('Prior not set')\n        \n        X_orig = X\n        X = phi(X, self.degree)\n        \n        # Calculate the predictive mean and variance\n        predictive_mean = torch.matmul(X, self.current_mean)\n        predictive_variance = self.sigma_noise**2 + torch.diag(torch.matmul(X @ self.current_sigma, X.T))\n        \n        return predictive_mean, predictive_variance\n\n    def plot_predictive(self, X, ax = None):\n    \n        X_orig = X\n        X = phi(X, self.degree)\n        \n        # Posterior distribution\n        posterior_mean = torch.matmul(X, self.current_mean)\n        posterior_variance = 1 / self.sigma_noise**2 + torch.diag(torch.matmul(X @ self.current_sigma, X.T))\n        \n        if ax is None:\n            fig, ax = plt.subplots(figsize=(10, 6))\n        ax.set_title(\"Posterior Distribution\")\n        ax.fill_between(X_orig.ravel().numpy(), (posterior_mean - 2 * posterior_variance).numpy(), (posterior_mean + 2 * posterior_variance).numpy(), color='r', alpha=0.2, label='Posterior')\n        \n        ax.plot(X_orig.ravel().numpy(), posterior_mean.numpy(), 'r-', label='Posterior Mean')\n        \n        if self.X_total is not None:\n            ax.scatter(self.X_total.numpy(), self.y_total.numpy(), c='purple', marker='*', label='Observed Data', s=200)\n        \n        ax.legend()\n        ax.set_xlim(X_orig.ravel().min(), X_orig.ravel().max())\n        return ax\n\n\n\nd = 2\nphi_x = phi(x_lin, degree=d)\n\n\nfor i in range(num_params(d)):\n    plt.plot(x_lin, phi_x[:, i], label=r'$\\phi_{}(x)$'.format(i, i))\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f22680283a0&gt;\n\n\n\n\n\n\ndef init_prior(d):\n    prior_mu = torch.zeros(num_params(d))  # Adjust for your dimensionality\n    prior_sigma = torch.eye(num_params(d))  # Adjust for your dimensionality\n    sigma_noise = 1.0  # Adjust for your noise level\n    return prior_mu, prior_sigma, sigma_noise\n\ndef init_prior_params(d):\n    prior_params = init_prior(d)\n    return prior_params + (d, )\n\n\ninit_prior_params(d)\n\n(tensor([0., 0., 0., 0., 0., 0.]),\n tensor([[1., 0., 0., 0., 0., 0.],\n         [0., 1., 0., 0., 0., 0.],\n         [0., 0., 1., 0., 0., 0.],\n         [0., 0., 0., 1., 0., 0.],\n         [0., 0., 0., 0., 1., 0.],\n         [0., 0., 0., 0., 0., 1.]]),\n 1.0,\n 2)\n\n\n\nd = 2\nblr = BLR(*init_prior_params(d))\n\n\nblr\n\nBLR(mu=tensor([0., 0., 0., 0., 0., 0.]),\n sigma=tensor([[1., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0.],\n        [0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 1., 0.],\n        [0., 0., 0., 0., 0., 1.]]), \nsigma_noise=1.0)\n\n\n\nax= blr.plot_predictive(x_lin)\n\n\n\n\n\ntorch.manual_seed(0)\nN_TOT = 30\nX_dataset = torch.distributions.Uniform(-3, 3).sample((N_TOT, 1))\nf_dataset = f(X_dataset)\ny_dataset = f_dataset + torch.distributions.Normal(0, 1.0).sample((N_TOT, 1))\n\ndef plot_dataset(ax=None):\n    if ax is None:\n        fig, ax = plt.subplots()\n    # Plot the dataset\n    ax.scatter(X_dataset.numpy(), y_dataset.numpy(), c='g', marker='x', label='Whole Dataset')\n    ax.plot(x_lin, f(x_lin), label='f(x)',c = 'k')\n    ax.legend()\n    ax.set_xlim(-3, 3)\n    \nplot_dataset()\n\n\n\n\n\n# Add the first 1 data points to the model\nd = 2\nblr = BLR(*init_prior_params(d))\nblr.update(X_dataset[:1], y_dataset[:1])\n\n\nblr\n\nBLR(mu=tensor([ 1.2552, -0.0282, -0.0564,  1.2539,  0.5517,  1.1274]),\n sigma=tensor([[ 7.5003e-01,  5.6144e-03,  1.1225e-02, -2.4972e-01, -1.0987e-01,\n         -2.2453e-01],\n        [ 5.6144e-03,  9.9987e-01, -2.5212e-04,  5.6087e-03,  2.4677e-03,\n          5.0430e-03],\n        [ 1.1225e-02, -2.5212e-04,  9.9950e-01,  1.1214e-02,  4.9338e-03,\n          1.0083e-02],\n        [-2.4972e-01,  5.6087e-03,  1.1214e-02,  7.5054e-01, -1.0976e-01,\n         -2.2430e-01],\n        [-1.0987e-01,  2.4677e-03,  4.9338e-03, -1.0976e-01,  9.5171e-01,\n         -9.8688e-02],\n        [-2.2453e-01,  5.0430e-03,  1.0083e-02, -2.2430e-01, -9.8688e-02,\n          7.9832e-01]]), \nsigma_noise=1.0)\n\n\n\nax = blr.plot_predictive(x_lin)\n#plot_dataset(ax=ax)\n\n\n\n\n\n# Add the next 1 data points to the model\nblr.update(X_dataset[1:2], y_dataset[1:2])\n\n\nax = blr.plot_predictive(x_lin)\nplot_dataset(ax=ax)\n\n\n\n\n\nblr\n\nBLR(mu=tensor([ 2.7943,  1.9381, -0.1648,  0.3441,  0.0203,  0.3810]),\n sigma=tensor([[ 5.0107e-01, -3.1243e-01,  2.8769e-02, -1.0255e-01, -2.3920e-02,\n         -1.0379e-01],\n        [-3.1243e-01,  5.9358e-01,  2.2160e-02,  1.9360e-01,  1.1227e-01,\n          1.5928e-01],\n        [ 2.8769e-02,  2.2160e-02,  9.9826e-01,  8.4362e-04, -1.1228e-03,\n          1.5747e-03],\n        [-1.0255e-01,  1.9360e-01,  8.4362e-04,  6.6355e-01, -1.6056e-01,\n         -2.9567e-01],\n        [-2.3920e-02,  1.1227e-01, -1.1228e-03, -1.6056e-01,  9.2204e-01,\n         -1.4037e-01],\n        [-1.0379e-01,  1.5928e-01,  1.5747e-03, -2.9567e-01, -1.4037e-01,\n          7.3977e-01]]), \nsigma_noise=1.0)\n\n\n\n# instead, add the two data points at once\nd = 2\nblr_new = BLR(*init_prior(d))\nblr_new.update(X_dataset[:2], y_dataset[:2])\n\n\nblr\n\nBLR(mu=tensor([ 2.7943,  1.9381, -0.1648,  0.3441,  0.0203,  0.3810]),\n sigma=tensor([[ 5.0107e-01, -3.1243e-01,  2.8769e-02, -1.0255e-01, -2.3920e-02,\n         -1.0379e-01],\n        [-3.1243e-01,  5.9358e-01,  2.2160e-02,  1.9360e-01,  1.1227e-01,\n          1.5928e-01],\n        [ 2.8769e-02,  2.2160e-02,  9.9826e-01,  8.4362e-04, -1.1228e-03,\n          1.5747e-03],\n        [-1.0255e-01,  1.9360e-01,  8.4362e-04,  6.6355e-01, -1.6056e-01,\n         -2.9567e-01],\n        [-2.3920e-02,  1.1227e-01, -1.1228e-03, -1.6056e-01,  9.2204e-01,\n         -1.4037e-01],\n        [-1.0379e-01,  1.5928e-01,  1.5747e-03, -2.9567e-01, -1.4037e-01,\n          7.3977e-01]]), \nsigma_noise=1.0)\n\n\n\nblr_new\n\nBLR(mu=tensor([ 2.7943,  1.9381, -0.1648,  0.3441,  0.0203,  0.3810]),\n sigma=tensor([[ 5.0107e-01, -3.1243e-01,  2.8769e-02, -1.0255e-01, -2.3920e-02,\n         -1.0379e-01],\n        [-3.1243e-01,  5.9358e-01,  2.2160e-02,  1.9360e-01,  1.1227e-01,\n          1.5928e-01],\n        [ 2.8769e-02,  2.2160e-02,  9.9826e-01,  8.4362e-04, -1.1228e-03,\n          1.5747e-03],\n        [-1.0255e-01,  1.9360e-01,  8.4362e-04,  6.6355e-01, -1.6056e-01,\n         -2.9567e-01],\n        [-2.3920e-02,  1.1227e-01, -1.1228e-03, -1.6056e-01,  9.2204e-01,\n         -1.4037e-01],\n        [-1.0379e-01,  1.5928e-01,  1.5747e-03, -2.9567e-01, -1.4037e-01,\n          7.3977e-01]]), \nsigma_noise=1.0)\n\n\n\nblr_new.plot_predictive(x_lin)\n\n&lt;AxesSubplot:title={'center':'Posterior Distribution'}&gt;\n\n\n\n\n\n\n# Entire dataset\ndef plot_fit(degree):\n    blr = BLR(*init_prior_params(degree))\n    blr.update(X_dataset, y_dataset)\n    ax = blr.plot_predictive(x_lin)\n    plot_dataset(ax=ax)\n    plt.title(f'Degree {degree}')\n    plt.show()\n\nplot_fit(1)\n\n\n\n\n\nplot_fit(2)\n\n\n\n\n\nplot_fit(3)\n\n\n\n\n\nplot_fit(4)\n\n\n\n\n\nplot_fit(6)\n\n\n\n\n\nd = 2\nblr = BLR(*init_prior_params(d))\n\n# Fit first 2 points\nblr.update(X_dataset[:2], y_dataset[:2])\n\n\nblr.current_mean\n\ntensor([ 2.7943,  1.9381, -0.1648,  0.3441,  0.0203,  0.3810])\n\n\n\ndef plot_maximum(blr, ax=None):\n    if ax is None:\n        fig, ax = plt.subplots()\n    # Plot the dataset\n    ax.scatter(X_dataset.numpy(), y_dataset.numpy(), c='g', marker='x', label='Whole Dataset')\n    ax.plot(x_lin, f(x_lin), label='f(x)',c = 'k')\n    ax.set_xlim(-3, 3)\n    \n    # Plot the current maximum\n    y_max = blr.y_total.max()\n    x_max = blr.X_total[blr.y_total.argmax()]\n\n    ax.axhline(y_max, c='k', linestyle='--', label='Current Max')\n    ax.axvline(x_max, c='k', linestyle='--')\n    \n    # Plot BLR predictive\n    ax = blr.plot_predictive(x_lin, ax=ax)\n    \n    # Put legend outside of plot\n    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n    \n    return ax\n\nplot_maximum(blr)\n\n&lt;AxesSubplot:title={'center':'Posterior Distribution'}&gt;\n\n\n\n\n\n\ndef plot_maximum_and_acq(blr, acq_fn, candidate_points_x):\n    \n    fig, ax = plt.subplots(nrows=2, sharex=True)\n    \n    plot_maximum(blr, ax=ax[0])\n    # Mark locations of candidate points on PI plot\n    ax[1].scatter(candidate_points_x, torch.zeros_like(candidate_points_x), c='r', marker='x', label='Candidate Points')\n    alpha = acq_fn(blr, x_lin, blr.y_total.max())\n    ax[1].plot(x_lin, alpha, label=r'Acquisition Function $\\alpha(x)$')\n    # legend outside of plot\n    \n    # Chosen candidate point\n    alp_candidate = acq_fn(blr, candidate_points_x, blr.y_total.max())\n    x_candidate = candidate_points_x[alp_candidate.argmax()]\n    ax[1].axvline(x_candidate, c='k', linestyle='--', label='Chosen Candidate')\n    ax[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n\n\n\ndef plot_filled_cdf(mu, sigma, above):\n    # Create a normal distribution with the specified mean and standard deviation\n    normal_dist = dist.Normal(mu, sigma)\n\n    # Generate a range of y values\n    y_values = torch.linspace(mu - 10 * sigma, mu + 10 * sigma, 2000)\n\n    # Calculate the probability density at each y value\n    pdf_values = normal_dist.log_prob(y_values).exp()\n\n    # Create a mask to select values above the specified \"above\" value\n    mask = y_values &gt;= above\n\n    # Plot the vertical line at \"above\"\n    plt.axhline(above, c='k', linestyle='--', label=f'Above {above}')\n\n    # Plot the PDF\n    plt.plot(pdf_values.numpy(), y_values.numpy(), label='PDF')\n    \n    z0 = torch.tensor((mu - above)/sigma)\n    phi = dist.Normal(0, 1).cdf(z0)\n\n    # Fill the area under the CDF curve for values above \"above\"\n    plt.fill_betweenx(y_values.numpy(), pdf_values.numpy(), \n                      where=mask, alpha=0.2, color='r', \n                      label=f'CDF Above {above} = {phi:.4f}')\n\n    plt.xlabel('PDF')\n    plt.ylabel('Y')\n    plt.legend()\n    plt.show()\n\n\nimport ipywidgets as widgets\nfrom IPython.display import display\n\n\n# Create interactive widgets for mu, sigma, and above\nmu_widget = widgets.FloatSlider(value=0.5, min=-1, max=1, step=0.1, description='Mean:')\nsigma_widget = widgets.FloatSlider(value=0.5, min=0.1, max=2, step=0.1, description='Standard Deviation:')\nabove_widget = widgets.FloatSlider(value=0.6, min=-1, max=1, step=0.1, description='Above Value:')\n\n# Create an interactive output\nout = widgets.interactive_output(plot_filled_cdf, {'mu': mu_widget, 'sigma': sigma_widget, 'above': above_widget})\n\n# Display the widgets and output\ndisplay(widgets.VBox([mu_widget, sigma_widget, above_widget, out]))\n\n\n\n\n\ndef alpha_PI(model, x_lin, y_max, eps=1e-1):\n    \"\"\"\n    model: BLR model\n    x_lin: (n_points, d)\n    y_max: current maximum\n    eps: exploration parameter\n    \"\"\"\n    \n    # Get the predictive mean and variance\n    predictive_mean, predictive_variance = model.predict(x_lin)\n  \n    # Calculate the PI acquisition function\n    z0 = (predictive_mean - y_max - eps) / torch.sqrt(predictive_variance)\n    \n    # Evaluate CDF of the standard normal at alpha\n    alpha = torch.distributions.Normal(0, 1).cdf(z0)\n    \n    return alpha\n\n\ndef setdiff1d(a, b):\n    mask = ~a.unsqueeze(1).eq(b).any(dim=1)\n    return torch.masked_select(a, mask)\n\n\nalpha = alpha_PI(blr, x_lin, blr.y_total.max())\ncandidate_points_x = setdiff1d(X_dataset, blr.X_total).reshape(-1, 1)\n\nplot_maximum_and_acq(blr, alpha_PI, candidate_points_x)\n\n\n\n\n\ny_max = blr.y_total.max()\nmu_0, sigma_0 = blr.predict(x_lin[0:1])\nabove = y_max \nplot_filled_cdf(mu_0.item(), sigma_0.item()**0.5, above)\n\n/tmp/ipykernel_225842/1988263787.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  z0 = torch.tensor((mu - above)/sigma)\n\n\n\n\n\n\ndef bo_loop(blr, n_iter, acq_fun):\n    y_max = blr.y_total.max()\n    for iterations in range(n_iter):\n        candidate_points_x = setdiff1d(X_dataset, blr.X_total).reshape(-1, 1)\n        candidate_points_y = setdiff1d(y_dataset, blr.y_total)\n        plot_maximum_and_acq(blr, acq_fun, candidate_points_x)\n        \n        alpha_candidate = acq_fun(blr, candidate_points_x, y_max)\n        to_add_idx = alpha_candidate.argmax()\n        to_add_x = candidate_points_x[to_add_idx].view(-1, 1)\n        to_add_y = candidate_points_y[to_add_idx].view(-1, 1)\n        print(f\"Index: {to_add_idx.item()}, x: {to_add_x.item():0.3f}, y: {to_add_y.item():0.3f}\")\n        \n        blr.update(to_add_x, to_add_y)\n        \n        print(f'Iteration {iterations+1} f(x+) =  {blr.y_total.max():0.3f}')\n        print()\n\n\nimport copy\n\nblr_copy = copy.deepcopy(blr)\nbo_loop(blr_copy, 10, alpha_PI)\n\nIndex: 24, x: 2.716, y: 4.546\nIteration 1 f(x+) =  6.479\n\nIndex: 18, x: 1.090, y: 7.590\nIteration 2 f(x+) =  7.590\n\nIndex: 3, x: 0.804, y: 6.802\nIteration 3 f(x+) =  7.590\n\nIndex: 6, x: 0.794, y: 4.239\nIteration 4 f(x+) =  7.590\n\nIndex: 12, x: 1.186, y: 6.964\nIteration 5 f(x+) =  7.590\n\nIndex: 12, x: 1.800, y: 6.170\nIteration 6 f(x+) =  7.590\n\nIndex: 14, x: 2.491, y: 7.997\nIteration 7 f(x+) =  7.997\n\nIndex: 4, x: 2.379, y: 6.983\nIteration 8 f(x+) =  7.997\n\nIndex: 14, x: 2.245, y: 6.776\nIteration 9 f(x+) =  7.997\n\nIndex: 15, x: 0.317, y: 7.446\nIteration 10 f(x+) =  7.997\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef alpha_ucb(model, x_lin, y_max, beta=100):\n    \"\"\"\n    model: BLR model\n    x_lin: (n_points, d)\n    y_max: current maximum\n    beta: exploration parameter\n    \"\"\"\n    \n    # Get the predictive mean and variance\n    predictive_mean, predictive_variance = model.predict(x_lin)\n  \n    # Calculate the UCB acquisition function\n    alpha = predictive_mean + beta * torch.sqrt(predictive_variance)\n    \n    return alpha\n\n\n\nalpha_ucb_0_1 = partial(alpha_ucb, beta=0.1)\nalpha_ucb_1 = partial(alpha_ucb, beta=1)\nalpha_ucb_10 = partial(alpha_ucb, beta=10)\n\n\nblr_copy = copy.deepcopy(blr)\nbo_loop(blr_copy, 10, alpha_ucb_0_1)\n\nIndex: 24, x: 2.716, y: 4.546\nIteration 1 f(x+) =  6.479\n\nIndex: 19, x: 2.491, y: 7.997\nIteration 2 f(x+) =  7.997\n\nIndex: 5, x: 2.379, y: 6.983\nIteration 3 f(x+) =  7.997\n\nIndex: 19, x: 2.245, y: 6.776\nIteration 4 f(x+) =  7.997\n\nIndex: 14, x: 1.800, y: 6.170\nIteration 5 f(x+) =  7.997\n\nIndex: 13, x: 1.186, y: 6.964\nIteration 6 f(x+) =  7.997\n\nIndex: 15, x: 1.090, y: 7.590\nIteration 7 f(x+) =  7.997\n\nIndex: 3, x: 0.804, y: 6.802\nIteration 8 f(x+) =  7.997\n\nIndex: 5, x: 0.794, y: 4.239\nIteration 9 f(x+) =  7.997\n\nIndex: 15, x: 0.317, y: 7.446\nIteration 10 f(x+) =  7.997\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nblr_copy = copy.deepcopy(blr)\nbo_loop(blr_copy, 10, alpha_ucb_10)\n\nIndex: 24, x: 2.716, y: 4.546\nIteration 1 f(x+) =  6.479\n\nIndex: 8, x: -0.907, y: 2.925\nIteration 2 f(x+) =  6.479\n\nIndex: 0, x: -2.469, y: 2.848\nIteration 3 f(x+) =  6.479\n\nIndex: 16, x: 1.090, y: 7.590\nIteration 4 f(x+) =  7.590\n\nIndex: 12, x: 1.186, y: 6.964\nIteration 5 f(x+) =  7.590\n\nIndex: 2, x: 0.804, y: 6.802\nIteration 6 f(x+) =  7.590\n\nIndex: 11, x: 1.800, y: 6.170\nIteration 7 f(x+) =  7.590\n\nIndex: 5, x: 0.794, y: 4.239\nIteration 8 f(x+) =  7.590\n\nIndex: 12, x: 2.491, y: 7.997\nIteration 9 f(x+) =  7.997\n\nIndex: 3, x: 2.379, y: 6.983\nIteration 10 f(x+) =  7.997\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef alpha_thompson(model, x_lin, y_max, n_samples=100):\n    \"\"\"\n    model: BLR model\n    x_lin: (n_points, d)\n    y_max: current maximum\n    n_samples: number of samples to draw from the posterior\n    \"\"\"\n    torch.manual_seed(0)\n    # Get the predictive mean and variance\n    predictive_mean, predictive_variance = model.predict(x_lin)\n  \n    # Sample from the posterior\n    posterior_sample = dist.Normal(predictive_mean, torch.sqrt(predictive_variance)).sample()\n    \n    # Calculate the PI acquisition function\n    alpha = posterior_sample\n    \n    return alpha\n\n\nblr_copy = copy.deepcopy(blr)\nbo_loop(blr_copy, 10, alpha_thompson)\n\nIndex: 5, x: 2.379, y: 6.983\nIteration 1 f(x+) =  6.983\n\nIndex: 20, x: 2.245, y: 6.776\nIteration 2 f(x+) =  6.983\n\nIndex: 18, x: 2.491, y: 7.997\nIteration 3 f(x+) =  7.997\n\nIndex: 21, x: 2.716, y: 4.546\nIteration 4 f(x+) =  7.997\n\nIndex: 18, x: -0.617, y: 1.858\nIteration 5 f(x+) =  7.997\n\nIndex: 17, x: 1.090, y: 7.590\nIteration 6 f(x+) =  7.997\n\nIndex: 14, x: 1.800, y: 6.170\nIteration 7 f(x+) =  7.997\n\nIndex: 13, x: 1.186, y: 6.964\nIteration 8 f(x+) =  7.997\n\nIndex: 16, x: 0.317, y: 7.446\nIteration 9 f(x+) =  7.997\n\nIndex: 3, x: 0.804, y: 6.802\nIteration 10 f(x+) =  7.997"
  },
  {
    "objectID": "notebooks/markov-chain.html",
    "href": "notebooks/markov-chain.html",
    "title": "Parameters",
    "section": "",
    "text": "import torch\nimport torch.autograd.functional as F\nimport torch.distributions as dist\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n%matplotlib inline\n\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n#plt.rcParams.update(bundles.icml2022())\n\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\n\n# Prior distribution\nP = torch.tensor([0.5, 0.5])\n\n# Transition matrix\nA = torch.tensor([[0.9, 0.1],\n                  [0.2, 0.8]])\n\n# States \nSTATES = {0: 'Sunny', 1: 'Rainy'}\n\n\nSampling from discrete state Markov chain\n\n# Generate data from the Markov chain\n\ndef generate_data(n_samples=10, seed=0):\n    torch.manual_seed(seed)\n    data = torch.zeros(n_samples, dtype=torch.long)\n    data[0] = dist.Categorical(P).sample()\n    for i in range(1, n_samples):\n        data[i] = dist.Categorical(A[data[i-1]]).sample()\n    return data\n\ndef map_state(x_arr):\n    return [STATES[x.item()] for x in x_arr]\n\n\ngenerate_data(10, 0)\nmap_state(generate_data(10, 0))\n\n['Rainy',\n 'Rainy',\n 'Rainy',\n 'Rainy',\n 'Rainy',\n 'Sunny',\n 'Sunny',\n 'Sunny',\n 'Sunny',\n 'Sunny']\n\n\n\nmap_state(generate_data(10, 1))\n\n['Sunny',\n 'Sunny',\n 'Sunny',\n 'Sunny',\n 'Rainy',\n 'Rainy',\n 'Rainy',\n 'Sunny',\n 'Sunny',\n 'Sunny']\n\n\n\n\nStationary distribution\n\nPS = []\nPR = []\n\n\n# P(Sunny) at t=0\nPS.append(P[0].item())\nPS\n\n[0.5]\n\n\n\n# P (Rainy) at t=0\nPR.append(P[1].item())\nPR\n\n[0.5]\n\n\n\ndef PS_t(t):\n    return PS[t-1] * A[0, 0].item() + PR[t-1] * A[1, 0].item()\n\ndef PR_t(t):\n    return PS[t-1] * A[0, 1].item() + PR[t-1] * A[1, 1].item()\n\n\nfor t in range(1, 20):\n    PS.append(PS_t(t))\n    PR.append(PR_t(t))\n\n\nPS, PR\n\n([0.5,\n  0.5499999895691872,\n  0.5849999801814558,\n  0.609499971553684,\n  0.6266499634787454,\n  0.6386549558053933,\n  0.647058448423374,\n  0.6529408912524431,\n  0.657058599234283,\n  0.6599409928265687,\n  0.6619586663486209,\n  0.6633710358232277,\n  0.6643596924658253,\n  0.6650517501268584,\n  0.6655361885013855,\n  0.6658752933757711,\n  0.6661126648003464,\n  0.6662788228102565,\n  0.6663951314300427,\n  0.6664765454768411],\n [0.5,\n  0.45000000670552254,\n  0.4150000105053187,\n  0.39050001224130404,\n  0.37335001251176025,\n  0.36134501174174294,\n  0.352941510233172,\n  0.34705905820045785,\n  0.3429413407958347,\n  0.34005893762736905,\n  0.33804125442175925,\n  0.33662887518843054,\n  0.33564020873449596,\n  0.3349481412252955,\n  0.33446369297681955,\n  0.33412457821043834,\n  0.33388719688123464,\n  0.3337210289578531,\n  0.33360471041840567,\n  0.333523286447613])\n\n\n\nplt.plot(PS, label='P(Sunny)')\nplt.plot(PR, label='P(Rainy)')\nplt.xlabel('Time')\nplt.ylabel('Probability')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f1af7260910&gt;\n\n\n\n\n\n\nA@A, torch.matrix_power(A, 2)\n\n(tensor([[0.8300, 0.1700],\n         [0.3400, 0.6600]]),\n tensor([[0.8300, 0.1700],\n         [0.3400, 0.6600]]))\n\n\n\nVectorised version\n\n# Distribution of states at t = 0\nP\n\ntensor([0.5000, 0.5000])\n\n\n\n# P(Sunny) at t = 1\nprint(\"--\"*20)\nprint(\"P(Sunny) at t = 1\")\nprint(P[0] * A[0, 0] + P[1] * A[1, 0])\n\n# P(Rainy) at t = 1\nprint(\"--\"*20)\nprint(\"P(Rain) at t = 1\")\nprint(P[0] * A[0, 1] + P[1] * A[1, 1])\n\n# Distribution of states at t = 1\nprint(\"--\"*20)\nprint(\"Distribution of states at t = 1\")\nprint(P@A)\n\n----------------------------------------\nP(Sunny) at t = 1\ntensor(0.5500)\n----------------------------------------\nP(Rain) at t = 1\ntensor(0.4500)\n----------------------------------------\nDistribution of states at t = 1\ntensor([0.5500, 0.4500])\n\n\n\n# Distribution of states at t = 2\nprint(\"--\"*20)\nprint(\"Distribution of states at t = 2\")\nprint(P@A@A)\n\n# Using torch.matrix_power\nprint(\"--\"*20)\nprint(\"Distribution of states at t = 2\")\nprint(P@torch.matrix_power(A, 2))\n\n----------------------------------------\nDistribution of states at t = 2\ntensor([0.5850, 0.4150])\n----------------------------------------\nDistribution of states at t = 2\ntensor([0.5850, 0.4150])\n\n\n\n# convergence\nP@torch.matrix_power(A, 100), P@torch.matrix_power(A, 99)\n\n(tensor([0.6667, 0.3333]), tensor([0.6667, 0.3333]))\n\n\n\nPS\n\n[0.5,\n 0.5499999895691872,\n 0.5849999801814558,\n 0.609499971553684,\n 0.6266499634787454,\n 0.6386549558053933,\n 0.647058448423374,\n 0.6529408912524431,\n 0.657058599234283,\n 0.6599409928265687,\n 0.6619586663486209,\n 0.6633710358232277,\n 0.6643596924658253,\n 0.6650517501268584,\n 0.6655361885013855,\n 0.6658752933757711,\n 0.6661126648003464,\n 0.6662788228102565,\n 0.6663951314300427,\n 0.6664765454768411]\n\n\n\n### What if we started with different initial distribution?\n\nP2 = torch.tensor([0.1, 0.9])\nP3 = torch.tensor([0.9, 0.1])\nP4 = torch.tensor([0.999999, 0.000001])\n\nP2@torch.matrix_power(A, 100), P3@torch.matrix_power(A, 100), P4@torch.matrix_power(A, 100)\n\n(tensor([0.6507, 0.3493]), tensor([0.6733, 0.3267]), tensor([0.6761, 0.3239]))\n\n\n\n### Checking for convergence of Markov chain using iterative method\n\neps = 1e-6\nPS = P\nfor i in range(100):\n    PS_new = PS@A\n    if torch.all(torch.abs(PS_new - PS) &lt; eps):\n        print(\"Converged at iteration\", i)\n        break\n    PS = PS_new\n\nConverged at iteration 31\n\n\n\n### Checking for convergence\n\ndef check_convergence(P, A, iter=100, eps=1e-6):\n    for i in range(iter):\n        P_new = P@A\n        if torch.all(torch.abs(P_new - P) &lt; eps):\n            print(\"Converged at iteration\", i)\n            break\n        P = P_new\n    return P\n\n\ncheck_convergence(P, A)\n\nConverged at iteration 31\n\n\ntensor([0.6667, 0.3333])\n\n\n\ncheck_convergence(P2, A)\n\nConverged at iteration 34\n\n\ntensor([0.6667, 0.3333])\n\n\n\ncheck_convergence(P3, A)\n\nConverged at iteration 32\n\n\ntensor([0.6667, 0.3333])\n\n\n\ncheck_convergence(P4, A)\n\nConverged at iteration 33\n\n\ntensor([0.6667, 0.3333])\n\n\n\n\n\nHomogeneous Markov chain\nThe transition matrix is the same for all time steps.\n\n\nIrreducible Markov chain\nA Markov chain is irreducible if it is possible to get to any state from any state.\n\n\nAperiodic Markov chain\nA Markov chain is aperiodic if there are no cycles in the state transition graph.\n\n### Markov chain with aperiodic transition matrix\n\nA = torch.tensor([[0.9, 0.1],\n                    [0.2, 0.8]])\n\n\n\n\n\n\n### Continuous space Markov chain\n\ndef transition(a, b):\n    return dist.Normal(a, 0.1).log_prob(b).exp()"
  },
  {
    "objectID": "notebooks/vae.html",
    "href": "notebooks/vae.html",
    "title": "Basic Imports",
    "section": "",
    "text": "def train_fn(model, inputs, outputs, loss_fn, optimizer, \n             epochs=100, batch_size=32, verbose=False, \n             model_type='autoencoder', beta=0.0):\n    model.train()\n    losses = []\n    for epoch in range(epochs):\n        for i in range(0, len(inputs), batch_size):\n            x = inputs[i:i+batch_size]\n            y = outputs[i:i+batch_size]\n\n            if model_type == 'autoencoder':\n                y_pred = model(x)\n                loss = loss_fn(y_pred, y)\n            elif model_type == 'vae':\n                y_pred, mu, std = model(x)\n                loss = loss_fn(y_pred, y, mu, std, beta)[0]\n            else:\n                raise ValueError(\"Unsupported model type. Use 'autoencoder' or 'vae'.\")\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        losses.append(loss.item())\n        if verbose:\n            print(f\"Epoch {epoch+1}/{epochs}, loss={loss.item():.4f}\")\n\n    return losses\n\n\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport seaborn as sns\nimport pandas as pd\n\ndist =torch.distributions\nimport torchsummary\n\nsns.reset_defaults()\nsns.set_context(context=\"talk\", font_scale=1)\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\nfrom functools import partial\n\n\nn_epochs = 100\n\n\nfrom astra.torch.data import load_mnist\n\n\n# Create a sine activation class similar to ReLU\nclass Sine(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.w = nn.Parameter(torch.tensor(1.0))\n        self.b = nn.Parameter(torch.tensor(0.0))\n        \n    def forward(self, x):\n        return torch.sin(self.w * x + self.b)\n\n\n# Autoencoder class with 1 hidden layer and hidden dim = z\nclass Autoencoder(nn.Module):\n    def __init__(self, input_size, hidden_size=128, z=2, act = nn.ReLU()):\n        super(Autoencoder, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_size, hidden_size),\n            act, # Using Sine activation\n            nn.Linear(hidden_size, z)\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(z, hidden_size),\n            act,\n            nn.Linear(hidden_size, input_size),\n            nn.Sigmoid()  # Sigmoid activation for reconstruction\n        )\n\n    def forward(self, x):\n        z = self.encoder(x)\n        x = self.decoder(z)\n        return x\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice\n\ndevice(type='cuda')\n\n\n\ndataset = load_mnist()\ndataset\n\n\nMNIST Dataset\nlength of dataset: 70000\nshape of images: torch.Size([28, 28])\nlen of classes: 10\nclasses: ['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\ndtype of images: torch.float32\ndtype of labels: torch.int64\n\n\n\n# Train over 1000 images\ntrain_idx = torch.arange(1000)\ntest_idx = torch.arange(1000, 2000)\n\nX = dataset.data[train_idx].to(device)\nX_test = dataset.data[test_idx].to(device)\n# Add a channel dimension\nX = X.unsqueeze(1).float()\nX_test = X_test.unsqueeze(1).float()\n\nX = X/255.0\nX_test = X_test/255.0\n\n\nmodel = Autoencoder(input_size=784, hidden_size=128, z=32,act=Sine()).to(device)\n\n\nX.shape\n\ntorch.Size([1000, 1, 28, 28])\n\n\n\nmodel(X.view(-1, 28*28*1)).shape\n\ntorch.Size([1000, 784])\n\n\n\n# Get reconstruction\ndef get_reconstruction(model, X, model_type='MLP'):\n    with torch.no_grad():\n        model.eval()\n        if model_type == 'MLP':\n           X = X.view(-1, 28*28*1)\n        X_hat = model(X)\n        if type(X_hat) == tuple: # for VAE\n            X_hat = X_hat[0]\n        if model_type == 'MLP':\n           X_hat = X_hat.view(-1, 1, 28, 28)\n        return X_hat\n\n\nr = get_reconstruction(model, X)\nr.max(), r.min(), r.shape\n\n(tensor(0.5956, device='cuda:0'),\n tensor(0.4148, device='cuda:0'),\n torch.Size([1000, 1, 28, 28]))\n\n\n\nnn.MSELoss()(r, X)\n\ntensor(0.2315, device='cuda:0')\n\n\n\n# Plot original and reconstructed images\ndef plot_reconstructions(model, X, n=5, model_type='MLP'):\n    X = X[:n]\n    X_hat = get_reconstruction(model, X, model_type=model_type)\n    # Use torchvision.utils.make_grid to make a grid of images\n    X_grid = torch.cat([X, X_hat], dim=0)\n    X_grid = torchvision.utils.make_grid(X_grid, nrow=n)\n    plt.imshow(X_grid.cpu().permute(1, 2, 0).numpy())\n    plt.axis('off')\n\n\nplot_reconstructions(model, X, 20)\n\n\n\n\n\nsetattr(model, 'device', device)\nl = train_fn(model=model, inputs=X.view(-1, 28*28),\n                                     outputs=X.view(-1, 28*28),\n                                     loss_fn=nn.MSELoss(),\n                                     optimizer=torch.optim.Adam(model.parameters(), lr=1e-3),\n                                     epochs=1000,\n                                        batch_size=500,\n                                        verbose=False)\n\n\nr = get_reconstruction(model, X)\n\n\nr.min(), r.max()\n\n(tensor(4.3413e-08, device='cuda:0'), tensor(1.0000, device='cuda:0'))\n\n\n\n_ = plt.plot(l)\n\n\n\n\n\nplot_reconstructions(model, X, 20)\n\n\n\n\n\nplot_reconstructions(model, X_test, 20)\n\n\n\n\n\nimport torch.nn as nn\n\nclass ConvAutoEncoderMNIST(nn.Module):\n    def __init__(self, latent_dim=2, act=nn.ReLU()):\n        super(ConvAutoEncoderMNIST, self).__init__()\n        self.latent_dim = latent_dim\n\n        # Encoder layers with further reduced filters\n        self.encoder = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, padding=1),  # 1X28X28 -&gt; 4X28X28\n            act,\n            nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, stride=2, padding=1),  # 4X28X28 -&gt; 8X14X14\n            act,\n            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=2, padding=1),  # 8X14X14 -&gt; 16X7X7\n            act,\n            nn.Flatten(),  # 16X7X7 -&gt; 784\n            nn.Linear(784, self.latent_dim)\n        )\n\n        # Decoder layers with further reduced filters\n        self.decoder = nn.Sequential(\n            nn.Linear(self.latent_dim, 784),  # 784 -&gt; 16X7X7\n            nn.Unflatten(1, (16, 7, 7)),  # 784 -&gt; 16X7X7\n            nn.ConvTranspose2d(in_channels=16, out_channels=8, kernel_size=3, stride=2, padding=1, output_padding=1),  # 16X7X7 -&gt; 8X14X14\n            act,\n            nn.ConvTranspose2d(in_channels=8, out_channels=4, kernel_size=3, stride=2, padding=1, output_padding=1),  # 8X14X14 -&gt; 4X28X28\n            act,\n            nn.ConvTranspose2d(in_channels=4, out_channels=1, kernel_size=3, padding=1),  # 4X28X28 -&gt; 1X28X28\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        z = self.encoder(x)\n        x_prime = self.decoder(z)\n        return x_prime\n\n\nm = ConvAutoEncoderMNIST(latent_dim=32, act=Sine()).to(device)\ntorchsummary.summary(m, (1, 28, 28))\n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1            [-1, 4, 28, 28]              40\n              Sine-2            [-1, 4, 28, 28]               0\n              Sine-3            [-1, 4, 28, 28]               0\n            Conv2d-4            [-1, 8, 14, 14]             296\n              Sine-5            [-1, 8, 14, 14]               0\n              Sine-6            [-1, 8, 14, 14]               0\n            Conv2d-7             [-1, 16, 7, 7]           1,168\n              Sine-8             [-1, 16, 7, 7]               0\n              Sine-9             [-1, 16, 7, 7]               0\n          Flatten-10                  [-1, 784]               0\n           Linear-11                   [-1, 32]          25,120\n           Linear-12                  [-1, 784]          25,872\n        Unflatten-13             [-1, 16, 7, 7]               0\n  ConvTranspose2d-14            [-1, 8, 14, 14]           1,160\n             Sine-15            [-1, 8, 14, 14]               0\n             Sine-16            [-1, 8, 14, 14]               0\n  ConvTranspose2d-17            [-1, 4, 28, 28]             292\n             Sine-18            [-1, 4, 28, 28]               0\n             Sine-19            [-1, 4, 28, 28]               0\n  ConvTranspose2d-20            [-1, 1, 28, 28]              37\n          Sigmoid-21            [-1, 1, 28, 28]               0\n================================================================\nTotal params: 53,985\nTrainable params: 53,985\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.26\nParams size (MB): 0.21\nEstimated Total Size (MB): 0.47\n----------------------------------------------------------------\n\n\n\nm(X).shape\n\ntorch.Size([1000, 1, 28, 28])\n\n\n\nlatent_dim_ranges = [2, 4, 8, 16, 32, 64, 128]\n\n\ncaes = {}\nfor latent_dim in latent_dim_ranges[:]:\n    caes[latent_dim] = ConvAutoEncoderMNIST(latent_dim=latent_dim, act=Sine()).to(device)\n    setattr(caes[latent_dim], 'device', device)\n\n\ntorchsummary.summary(caes[2], (1, 28, 28))\n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1            [-1, 4, 28, 28]              40\n              Sine-2            [-1, 4, 28, 28]               0\n              Sine-3            [-1, 4, 28, 28]               0\n            Conv2d-4            [-1, 8, 14, 14]             296\n              Sine-5            [-1, 8, 14, 14]               0\n              Sine-6            [-1, 8, 14, 14]               0\n            Conv2d-7             [-1, 16, 7, 7]           1,168\n              Sine-8             [-1, 16, 7, 7]               0\n              Sine-9             [-1, 16, 7, 7]               0\n          Flatten-10                  [-1, 784]               0\n           Linear-11                    [-1, 2]           1,570\n           Linear-12                  [-1, 784]           2,352\n        Unflatten-13             [-1, 16, 7, 7]               0\n  ConvTranspose2d-14            [-1, 8, 14, 14]           1,160\n             Sine-15            [-1, 8, 14, 14]               0\n             Sine-16            [-1, 8, 14, 14]               0\n  ConvTranspose2d-17            [-1, 4, 28, 28]             292\n             Sine-18            [-1, 4, 28, 28]               0\n             Sine-19            [-1, 4, 28, 28]               0\n  ConvTranspose2d-20            [-1, 1, 28, 28]              37\n          Sigmoid-21            [-1, 1, 28, 28]               0\n================================================================\nTotal params: 6,915\nTrainable params: 6,915\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.26\nParams size (MB): 0.03\nEstimated Total Size (MB): 0.29\n----------------------------------------------------------------\n\n\n\ntorchsummary.summary(caes[128], (1, 28, 28))\n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1            [-1, 4, 28, 28]              40\n              Sine-2            [-1, 4, 28, 28]               0\n              Sine-3            [-1, 4, 28, 28]               0\n            Conv2d-4            [-1, 8, 14, 14]             296\n              Sine-5            [-1, 8, 14, 14]               0\n              Sine-6            [-1, 8, 14, 14]               0\n            Conv2d-7             [-1, 16, 7, 7]           1,168\n              Sine-8             [-1, 16, 7, 7]               0\n              Sine-9             [-1, 16, 7, 7]               0\n          Flatten-10                  [-1, 784]               0\n           Linear-11                  [-1, 128]         100,480\n           Linear-12                  [-1, 784]         101,136\n        Unflatten-13             [-1, 16, 7, 7]               0\n  ConvTranspose2d-14            [-1, 8, 14, 14]           1,160\n             Sine-15            [-1, 8, 14, 14]               0\n             Sine-16            [-1, 8, 14, 14]               0\n  ConvTranspose2d-17            [-1, 4, 28, 28]             292\n             Sine-18            [-1, 4, 28, 28]               0\n             Sine-19            [-1, 4, 28, 28]               0\n  ConvTranspose2d-20            [-1, 1, 28, 28]              37\n          Sigmoid-21            [-1, 1, 28, 28]               0\n================================================================\nTotal params: 204,609\nTrainable params: 204,609\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.26\nParams size (MB): 0.78\nEstimated Total Size (MB): 1.05\n----------------------------------------------------------------\n\n\n\nplot_reconstructions(caes[2], X, 20, model_type='CNN')\n\n\n\n\n\nloss = {}\nfor latent_dim in latent_dim_ranges[:]:\n    print(f\"Training for latent_dim = {latent_dim}\")\n    loss[latent_dim] = train_fn(model=caes[latent_dim],\n                                                        inputs=X,\n                                                        outputs=X,\n                                                        loss_fn=nn.MSELoss(),\n                                                        optimizer=torch.optim.Adam(caes[latent_dim].parameters(), lr=1e-3),\n                                                        epochs=1000,\n                                                        batch_size=500,\n                                                        verbose=False)\n\nTraining for latent_dim = 2\nTraining for latent_dim = 4\nTraining for latent_dim = 8\nTraining for latent_dim = 16\nTraining for latent_dim = 32\nTraining for latent_dim = 64\nTraining for latent_dim = 128\n\n\n\nfor latent_dim in latent_dim_ranges[:]:\n    plt.plot(loss[latent_dim], label=f'Latent dim: {latent_dim}')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f0c2d5fd760&gt;\n\n\n\n\n\n\n# Plot reconstructions\nplot_reconstructions(caes[2], X, 20, model_type='CNN')\n\n\n\n\n\nplot_reconstructions(caes[2], X_test, 20, model_type='CNN')\n\n\n\n\n\nplot_reconstructions(caes[4], X, 20, 'CNN')\n\n\n\n\n\nplot_reconstructions(caes[8], X, 20, 'CNN')\n\n\n\n\n\nplot_reconstructions(caes[16], X, 20, 'CNN')\n\n\n\n\n\nplot_reconstructions(caes[128], X, 20, 'CNN')\n\n\n\n\n\nplot_reconstructions(caes[128], X_test, 20, 'CNN')\n\n\n\n\n\n# Give a random input to the model and get the output\ndef get_random_output(model, n=5, latent_dim=2):\n    with torch.no_grad():\n        model.eval()\n        z = torch.randn(n, latent_dim).to(device)\n        X_hat = model.decoder(z)\n        return X_hat\n\n# Plot random outputs\ndef plot_random_outputs(model, n=5, latent_dim=2):\n    X_hat = get_random_output(model, n, latent_dim)\n    X_grid = torchvision.utils.make_grid(torch.tensor(X_hat), nrow=n)\n    plt.imshow(X_grid.cpu().permute(1, 2, 0).numpy())\n    plt.axis('off')\n\n\nplot_random_outputs(caes[2], n=20)\n\n/tmp/ipykernel_1330344/4035786813.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_grid = torchvision.utils.make_grid(torch.tensor(X_hat), nrow=n)\n\n\n\n\n\n\nplot_random_outputs(caes[8], n=20, latent_dim=8)\n\n/tmp/ipykernel_1330344/4035786813.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_grid = torchvision.utils.make_grid(torch.tensor(X_hat), nrow=n)\n\n\n\n\n\n\nplot_random_outputs(caes[128], n=20, latent_dim=128)\n\n/tmp/ipykernel_1330344/4035786813.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_grid = torchvision.utils.make_grid(torch.tensor(X_hat), nrow=n)\n\n\n\n\n\n\n# Interpolate between two points in latent space\ndef interpolate(model, z1, z2, n=5):\n    with torch.no_grad():\n        model.eval()\n        z = torch.zeros(n, z1.shape[1]).to(device)\n        for i in range(n):\n            z[i] = z1 + (z2 - z1) * (i / (n - 1))\n        X_hat = model.decoder(z)\n        return X_hat\n\n# Plot interpolation\ndef plot_interpolation(model, img1, img2, n=5):\n    z1 = model.encoder(X[img1].unsqueeze(0))\n    z2 = model.encoder(X[img2].unsqueeze(0))\n    X_hat = interpolate(model, z1, z2, n)\n    X_grid = torchvision.utils.make_grid(X_hat, nrow=n)\n    plt.imshow(X_grid.cpu().permute(1, 2, 0).numpy())\n    plt.axis('off')\n\n\nplot_interpolation(caes[2], 0, 1, 20)\n\n\n\n\n\n# Interactive widget to plot interpolation\nfrom ipywidgets import interact, IntSlider\n\ndef plot_interpolation_widget(img1, img2, latent_dim=2, n=20):\n    plot_interpolation(caes[latent_dim], img1, img2, n)\n\ninteract(plot_interpolation_widget, img1=IntSlider(0, 0, 1000), \n         img2=IntSlider(1, 0, 1000), \n         latent_dim=latent_dim_ranges,\n         n=IntSlider(20, 5, 50))\n\n\n\n\n&lt;function __main__.plot_interpolation_widget(img1, img2, latent_dim=2, n=20)&gt;\n\n\n\n# plot scatter plot on 2d space for latent dim = 2 for all images\ndef plot_scatter(model, X, y, n=1000, latent_dim=2):\n    with torch.no_grad():\n        model.eval()\n        z = model.encoder(X[:n].view(-1, 1, 28, 28)).cpu().numpy()\n        plt.scatter(z[:, 0], z[:, 1], c=y[:n], cmap='tab10')\n        plt.colorbar()\n\nplot_scatter(caes[2], X, dataset.targets)\n\n\n\n\n\ndef distance_between_representation(model, img1_id, img2_id):\n    with torch.no_grad():\n        model.eval()\n        z1 = model.encoder(X[img1_id].unsqueeze(0))\n        z2 = model.encoder(X[img2_id].unsqueeze(0))\n        return torch.dist(z1, z2).item()\n    \n\n\ndef find_all_occurences_digit(digit):\n    return torch.where(dataset.targets == digit)[0]\n\n\nall_0s = find_all_occurences_digit(0)\nall_1s = find_all_occurences_digit(1)\n\n\n# Find distance between two 0s\ndistance_between_representation(caes[2], all_0s[0], all_0s[1])\n\n0.2112869769334793\n\n\n\ndistance_between_representation(caes[4], all_0s[0], all_0s[1])\n\n0.9414359331130981\n\n\n\ndistance_between_representation(caes[128], all_0s[0], all_0s[1])\n\n7.251440525054932\n\n\n\ndef plot_latent_space_2d(model, x_min=-2.0, x_max=2.0, y_min=-2.0, y_max=2.0, n=20):\n    \n    \n    xx, yy = torch.meshgrid(torch.linspace(x_min, x_max, n), torch.linspace(y_min, y_max, n))\n    z_grid = torch.cat([xx.reshape(-1, 1), yy.reshape(-1, 1)], dim=1)\n    z_grid = z_grid.to(device)\n    \n    # Get the output from the decoder\n    X_hat = model.decoder(z_grid)\n    X_hat = X_hat.view(n, n, 28, 28)\n    X_hat = X_hat.cpu().detach().numpy()\n    \n    # Plot the output\n    plt.figure(figsize=(8, 8))\n    import numpy as np\n    plt.imshow(np.block(list(map(list, X_hat))), cmap='gray')\n    plt.axis('off')\n\n\ndef find_latent_space_lims(model):\n    with torch.no_grad():\n        model.eval()\n        zs = model.encoder(X.view(-1, 1, 28, 28)).cpu().numpy()\n\n    # Find the min and max of the latent space\n    x_min = zs[:, 0].min() - 0.5\n    x_max = zs[:, 0].max() + 0.5\n\n    y_min = zs[:, 1].min() - 0.5\n    y_max = zs[:, 1].max() + 0.5\n    return x_min, x_max, y_min, y_max\n\n\ndef plot_latent_space_auto_lims(model, n=20):\n    x_min, x_max, y_min, y_max = find_latent_space_lims(model)\n\n    plot_latent_space_2d(model, x_min, x_max, y_min, y_max, n)\n\n\nplot_latent_space_auto_lims(caes[2])\n\n\n\n\n\nclass ConvVAEMNIST(nn.Module):\n    def __init__(self, latent_dim=2, act=nn.ReLU()):\n        super(ConvVAEMNIST, self).__init__()\n        self.latent_dim = latent_dim\n\n        # Encoder layers with further reduced filters\n        self.encoder = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, padding=1),  # 1X28X28 -&gt; 4X28X28\n            act,\n            nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, stride=2, padding=1),  # 4X28X28 -&gt; 8X14X14\n            act,\n            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=2, padding=1),  # 8X14X14 -&gt; 16X7X7\n            act,\n            nn.Flatten(),  # 16X7X7 -&gt; 784\n            nn.Linear(784, self.latent_dim*2)\n        )\n\n        # Decoder layers with further reduced filters\n        self.decoder = nn.Sequential(\n            nn.Linear(self.latent_dim, 784),  # 784 -&gt; 16X7X7\n            nn.Unflatten(1, (16, 7, 7)),  # 784 -&gt; 16X7X7\n            nn.ConvTranspose2d(in_channels=16, out_channels=8, kernel_size=3, stride=2, padding=1, output_padding=1),  # 16X7X7 -&gt; 8X14X14\n            act,\n            nn.ConvTranspose2d(in_channels=8, out_channels=4, kernel_size=3, stride=2, padding=1, output_padding=1),  # 8X14X14 -&gt; 4X28X28\n            act,\n            nn.ConvTranspose2d(in_channels=4, out_channels=1, kernel_size=3, padding=1),  # 4X28X28 -&gt; 1X28X28\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.encoder(x)\n        # Chunk to get mu and logvar\n        mu, logvar = torch.chunk(x, 2, dim=1)\n        std = torch.exp(0.5 * logvar)\n        eps = torch.distributions.normal.Normal(0, 1).sample(std.shape).to(device)\n        z = mu + eps * std\n        x = self.decoder(z)\n        return x, mu, logvar\n\n\ndef VAE_loss(x, x_hat, mu, log_var, beta=1):\n    # Reconstruction loss\n    recon_loss = nn.MSELoss()(x_hat, x)\n    \n    std = torch.exp(0.5 * log_var)\n    prior = torch.distributions.normal.Normal(0, 1)\n    post = torch.distributions.normal.Normal(mu, std)\n    kl_loss = torch.distributions.kl.kl_divergence(post, prior).mean()\n\n    return recon_loss + beta * kl_loss, recon_loss, kl_loss\n\n\n# Forwards pass of untrained model\nm = ConvVAEMNIST(latent_dim=32, act=Sine()).to(device)\nX_hat, mu, std = m(X)\n\n\nX.shape, X_hat.shape, mu.shape, std.shape\n\n(torch.Size([1000, 1, 28, 28]),\n torch.Size([1000, 1, 28, 28]),\n torch.Size([1000, 32]),\n torch.Size([1000, 32]))\n\n\n\nVAE_loss(X, X_hat, mu, std, beta=1)\n\n(tensor(0.2179, device='cuda:0', grad_fn=&lt;AddBackward0&gt;),\n tensor(0.2157, device='cuda:0', grad_fn=&lt;MseLossBackward0&gt;),\n tensor(0.0022, device='cuda:0', grad_fn=&lt;MeanBackward0&gt;))\n\n\n\nloss_dim_betas = {}\nc_vaes = {}\nlatent_dim_subset = [2, 16, 256]\nbetas = [0.0, 0.001, 0.01, 0.1, 0.5, 1.0]\n\nfor latent_dim in latent_dim_subset:\n    print(f\"Training for latent_dim = {latent_dim}\")\n    c_vaes[latent_dim] = {}\n    loss_dim_betas[latent_dim] = {}\n    for beta in betas:\n        print(f\"Training for beta = {beta}\")\n        c_vaes[latent_dim][beta] = ConvVAEMNIST(latent_dim=latent_dim, act=Sine()).to(device)\n        setattr(c_vaes[latent_dim][beta], 'device', device)\n        loss_dim_betas[latent_dim][beta] = train_fn(model=c_vaes[latent_dim][beta],\n                                                        inputs=X,\n                                                        outputs=X,\n                                                        loss_fn=VAE_loss,\n                                                        optimizer=torch.optim.Adam(c_vaes[latent_dim][beta].parameters(), lr=1e-3),\n                                                        epochs=1000,\n                                                        batch_size=500,\n                                                        verbose=False,\n                                                        model_type='vae',\n                                                        beta=beta)\n        \n    \n\nTraining for latent_dim = 2\nTraining for beta = 0.0\nTraining for beta = 0.001\nTraining for beta = 0.01\nTraining for beta = 0.1\nTraining for beta = 0.5\nTraining for beta = 1.0\nTraining for latent_dim = 16\nTraining for beta = 0.0\nTraining for beta = 0.001\nTraining for beta = 0.01\nTraining for beta = 0.1\nTraining for beta = 0.5\nTraining for beta = 1.0\nTraining for latent_dim = 256\nTraining for beta = 0.0\nTraining for beta = 0.001\nTraining for beta = 0.01\nTraining for beta = 0.1\nTraining for beta = 0.5\nTraining for beta = 1.0\n\n\n\n# Plot reconstructions\nfor latent_dim in latent_dim_subset:\n    for beta in betas:\n        plt.figure()\n        plot_reconstructions(c_vaes[latent_dim][beta], X, 20, model_type='CNN')\n        plt.title(f\"Latent dim = {latent_dim}, beta = {beta}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor latent_dim in latent_dim_subset:\n    plt.figure()\n    for beta in betas:\n        plt.plot(loss_dim_betas[latent_dim][beta], label=f'Latent dim: {latent_dim}, beta: {beta}')\n    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n\n\n\n\n\n\n\n\n\n\n\n# Plot scatter\nplot_scatter(c_vaes[2][0.0], X, dataset.targets)\n\n\n\n\n\nplot_scatter(c_vaes[2][0.001], X, dataset.targets)\n\n\n\n\n\nplot_scatter(c_vaes[2][0.01], X, dataset.targets)\n\n\n\n\n\nplot_latent_space_auto_lims(c_vaes[2][0.0])\n\n\n\n\n\nplot_latent_space_auto_lims(c_vaes[2][0.001])\n\n\n\n\n\ndistance_between_representation(c_vaes[2][0.0], all_0s[0], all_0s[1])\n\n0.8384506702423096\n\n\n\ndistance_between_representation(c_vaes[2][0.001], all_0s[0], all_0s[1])\n\n0.22767327725887299\n\n\n\ndistance_between_representation(c_vaes[2][1], all_0s[0], all_0s[1])\n\n0.0021541789174079895\n\n\n\nDistance between different digits\n\ndistance_between_representation(c_vaes[2][0.0], all_0s[0], all_1s[0])\n\n11.220616340637207\n\n\n\ndistance_between_representation(c_vaes[2][0.001], all_0s[0], all_1s[0])\n\n4.362278461456299\n\n\n\n\nTodo\n\nShow Fashion MNIST results\nShow tSNE plots\nCreate GIF of interpolation\nShow on harder datasets (CIFAR, CelebA, etc.)\nShow for varying number of Monte Carlo samples\nShow from the Bayesian perspective\nShow the performance of the model on the test set"
  },
  {
    "objectID": "notebooks/MAP-old.html",
    "href": "notebooks/MAP-old.html",
    "title": "Setting Up & Imports",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\nimport logging\nlogging.getLogger('matplotlib.font_manager').disabled = True\n\nfrom ipywidgets import interact, FloatSlider, IntSlider\n\n\nCoin Toss Problem\n\ndef plot_beta(alpha, beta):\n    dist = torch.distributions.Beta(concentration1=alpha, concentration0=beta)\n    xs = torch.linspace(0, 1, 500)\n    ys = dist.log_prob(xs).exp()\n    plt.plot(xs, ys, color='C0')\n    plt.grid()\n    plt.xlabel(r'$\\theta$')\n    plt.ylabel(r'$p(\\theta)$')\n    plt.title('Beta Distribution')\n    plt.show()\ninteract(plot_beta, alpha=FloatSlider(min=1, max=11, step=0.5, value=1), beta=FloatSlider(min=1, max=11, step=0.5, value=1))\n\n\n\n\n&lt;function __main__.plot_beta(alpha, beta)&gt;\n\n\n\ncombinations = [\n    [1, 1],\n    [5, 1],\n    [1, 3],\n    [2, 2],\n    [2, 5]\n]\nfig, ax = plt.subplots(1, 1)\nfor (alpha, beta) in combinations:\n    dist = torch.distributions.Beta(concentration1=alpha, concentration0=beta)\n    xs = torch.linspace(0, 1, 500)\n    ys = dist.log_prob(xs).exp()\n    ax.plot(xs, ys, label=rf'($\\alpha$={alpha}, $\\beta$={beta})')\nax.legend()\nax.grid()\nax.set_xlabel(r'$\\theta$')\nax.set_ylabel(r'$p(\\theta)$')\nax.set_title('Beta Distribution')\nax.set_ylim(0, 3)\nplt.savefig('../figures/map/beta_distribution.pdf', bbox_inches='tight')\nplt.show()\n\n\n\n\n\nalpha = 11\nbeta = 11\nbernoulli_theta = 0.5\nn_1 = 9\nn_0 = 1\n\ndist = torch.distributions.Beta(concentration1=alpha, concentration0=beta)\nxs = torch.linspace(0, 1, 500)\nys = dist.log_prob(xs).exp()\nplt.plot(xs, ys, label='Beta Prior')\n\nsample_size = n_1 + n_0\nplt.title(f\"H: {n_1}, T: {n_0}\")\n\nmle_estimate = n_1 / (sample_size)#samples.mean()\nplt.axvline(mle_estimate, color='k', linestyle='--', label='MLE')\n\nmap_estimate = (n_1 + alpha - 1) / (sample_size + alpha + beta - 2)\nplt.axvline(map_estimate, color='r', linestyle='-.', label='MAP')\n\nplt.axvline(alpha / (alpha + beta), color='g', linestyle=':', label='Prior Mean')\n\nplt.grid()\nplt.xlabel(r'$\\theta$')\nplt.ylabel(r'$p(\\theta)$')\nplt.legend(bbox_to_anchor=(1.25,1), borderaxespad=0)\nplt.savefig('../figures/map/coin_toss_prior_mle_map.pdf', bbox_inches='tight')\nplt.show()\n\n\n\n\n\ndef plot_beta_all(alpha, beta, bernoulli_theta = 0.5, sample_size=10):\n    torch.manual_seed(42)\n\n    dist = torch.distributions.Beta(concentration1=alpha, concentration0=beta)\n    xs = torch.linspace(0, 1, 500)\n    ys = dist.log_prob(xs).exp()\n    plt.plot(xs, ys, label='Beta Prior')\n\n    samples = torch.empty(sample_size)\n    for s_num in range(sample_size):\n        dist = torch.distributions.Bernoulli(probs=bernoulli_theta)\n        samples[s_num] = dist.sample()\n    n_1 = int(samples.sum())\n    n_0 = int(sample_size - n_1)\n    plt.title(f\"H: {n_1}, T: {n_0}\")\n\n    mle_estimate = n_1 / (sample_size)#samples.mean()\n    plt.axvline(mle_estimate, color='k', linestyle='--', label='MLE')\n\n    map_estimate = (n_1 + alpha - 1) / (sample_size + alpha + beta - 2)\n    plt.axvline(map_estimate, color='r', linestyle='-.', label='MAP')\n\n    plt.axvline(alpha / (alpha + beta), color='g', linestyle=':', label='Prior Mean')\n\n    plt.grid()\n    plt.xlabel(r'$\\theta$')\n    plt.ylabel(r'$p(\\theta)$')\n    plt.legend(bbox_to_anchor=(1.25,1), borderaxespad=0)\n    plt.show()\n\ninteract(\n    plot_beta_all,\n    alpha=FloatSlider(min=1, max=51, step=1, value=11),\n    beta=FloatSlider(min=1, max=51, step=1, value=11),\n    bernoulli_theta = FloatSlider(min=0, max=1, step=0.1, value=0.5),\n    sample_size = IntSlider(min=10, max=1000, step=10, value=10),\n    )\n\n\n\n\n&lt;function __main__.plot_beta_all(alpha, beta, bernoulli_theta=0.5, sample_size=10)&gt;\n\n\n\n\nLinear Regression\n\nn_data = 5\nx = torch.linspace(-1, 1, n_data)\nf = lambda x: 3*x + 1\n\nnoise = torch.distributions.Normal(0, 5).sample((n_data,))\ny = f(x) + noise\n\nplt.figure()\nplt.scatter(x, y, marker='x', c='k', s=20, label=\"Noisy data\")\nplt.plot(x, f(x), c='k', label=\"True function\")\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title(r'$y_{true} = 3 x + 1$')\nplt.savefig(f'../figures/map/linreg_data-{n_data}.pdf', bbox_inches='tight')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fcb67cc6220&gt;\n\n\n\n\n\n\ndef nll(theta):\n    mu = theta[0] + theta[1] * x\n    sigma = torch.tensor(1.0)\n    dist = torch.distributions.normal.Normal(mu, sigma)\n    return -dist.log_prob(y).sum()\n\ndef neg_log_prior(theta):\n    prior_mean = torch.tensor([0.0, 0.0])  # Prior mean for slope and intercept\n    prior_std = torch.tensor([100, 100])    # Prior standard deviation for slope and intercept\n    dist = torch.distributions.normal.Normal(prior_mean, prior_std)\n    return -dist.log_prob(theta).sum()\n\ndef neg_log_joint(theta):\n    return nll(theta) + neg_log_prior(theta)\n\n\n# Create a grid of theta[0] and theta[1] values\ntheta0_values = torch.linspace(-4, 4, 100)\ntheta1_values = torch.linspace(-4, 4, 100)\ntheta0_mesh, theta1_mesh = torch.meshgrid(theta0_values, theta1_values)\nnll_values = torch.zeros_like(theta0_mesh)\nnlp_values = torch.zeros_like(theta0_mesh)\nnlj_values = torch.zeros_like(theta0_mesh)\n\nfor i in range(len(theta0_values)):\n    for j in range(len(theta1_values)):\n        theta_current = torch.tensor([theta0_values[i], theta1_values[j]])\n        nll_values[i, j] = nll(theta_current)\n        nlp_values[i, j] = neg_log_prior(theta_current)\n        nlj_values[i, j] = neg_log_joint(theta_current)\n\n\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n\n\n\n\ndef plot_contour_with_minima(values, theta0_values, theta1_values, title):\n    plt.figure(figsize=(6, 6))\n    \n    contour = plt.contourf(theta0_mesh, theta1_mesh, values, levels=20, cmap='viridis')\n    plt.xlabel(r'$\\theta_0$')\n    plt.ylabel(r'$\\theta_1$')\n    plt.title(title)\n    \n    plt.colorbar(contour)\n    \n    plt.gca().set_aspect('equal', adjustable='box')  # Set aspect ratio to be equal\n    \n    #plt.tight_layout()\n    #plt.show()\n\n# Usage example\nplot_contour_with_minima(nll_values, theta0_values, theta1_values, 'Negative Log-Likelihood')\nplot_contour_with_minima(nlp_values, theta0_values, theta1_values, 'Negative Log-Prior')\nplot_contour_with_minima(nlj_values, theta0_values, theta1_values, 'Negative Log-Joint')\n\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(1,2, figsize=(12, 6))\n\ncontour = ax[1].contourf(theta0_mesh, theta1_mesh, nll_values, levels=20, cmap='viridis')\n# ax[1].colorbar(contour)\nax[1].set_xlabel(r'$\\theta_0$')\nax[1].set_ylabel(r'$\\theta_1$')\nax[1].set_title('NLL (MLE)')\n\n# Adding contour level labels\ncontour_labels = ax[1].contour(theta0_mesh, theta1_mesh, nll_values, levels=20, colors='black', linewidths=0.5)\nax[1].clabel(contour_labels, inline=True, fontsize=8, fmt='%1.1f')\n\n# Find and mark the minimum\nmin_indices = torch.argmin(nll_values)\nmin_theta0 = theta0_mesh.flatten()[min_indices]\nmin_theta1 = theta1_mesh.flatten()[min_indices]\nax[1].scatter(min_theta0, min_theta1, color='red', marker='x', label='Minima')\n\n# Draw lines from the minimum point to the axes\nax[1].axhline(min_theta1, color='gray', linestyle='--')\nax[1].axvline(min_theta0, color='gray', linestyle='--')\n\n# Add labels to the lines\nax[1].text(min_theta0, 4.2, f'{min_theta0:.2f}', color='gray', fontsize=10)\nax[1].text(4.2, min_theta1, f'{min_theta1:.2f}', color='gray', fontsize=10)\n# ax[1].legend(bbox_to_anchor=(0, 1.05), loc='lower left')\n\nax[0].scatter(x, y, marker='x', c='k', s=20, label=\"Noisy data\")\nax[0].plot(x, f(x), c='k', label=\"True function\")\nax[0].plot(x, min_theta0 + min_theta1*x, c = 'b', label=\"MLE\")\nax[0].set_xlabel('x')\nax[0].set_ylabel('y')\nax[0].set_title(r'$y_{true} = 3 x + 1$')\nax[0].legend()\nplt.savefig('../figures/map/linreg_mle.pdf', bbox_inches='tight')\nplt.show()\n\n\n\n\n\n# Create a grid of theta[0] and theta[1] values\ntheta0_values = torch.linspace(-4, 4, 100)\ntheta1_values = torch.linspace(-4, 4, 100)\ntheta0_mesh, theta1_mesh = torch.meshgrid(theta0_values, theta1_values)\nnll_values = torch.zeros_like(theta0_mesh)\n\n# Calculate negative log-likelihood values for each combination of theta[0] and theta[1]\nfor i in range(len(theta0_values)):\n    for j in range(len(theta1_values)):\n        nll_values[i, j] = nll_with_prior([theta0_values[i], theta1_values[j]])\n\n# Create a contour plot\nplt.figure()#figsize=(8, 6))\ncontour = plt.contourf(theta0_mesh, theta1_mesh, nll_values, levels=20, cmap='viridis')\n# plt.colorbar(contour)\nplt.xlabel(r'$\\theta_0$')\nplt.ylabel(r'$\\theta_1$')\n#plt.title('Contour Plot of Negative Log-Likelihood')\n\n# Adding contour level labels\ncontour_labels = plt.contour(theta0_mesh, theta1_mesh, nll_values, levels=20, colors='black', linewidths=0.5)\nplt.clabel(contour_labels, inline=True, fontsize=8, fmt='%1.1f')\n\n# Find and mark the minimum\nmin_indices = torch.argmin(nll_values)\nmin_theta0 = theta0_mesh.flatten()[min_indices]\nmin_theta1 = theta1_mesh.flatten()[min_indices]\nplt.scatter(min_theta0, min_theta1, color='red', marker='x', label='Minima')\n\n# Draw lines from the minimum point to the axes\nplt.axhline(min_theta1, color='gray', linestyle='--')\nplt.axvline(min_theta0, color='gray', linestyle='--')\n\n# Add labels to the lines\nplt.text(min_theta0, 4.2, f'{min_theta0:.2f}', color='gray', fontsize=10)\nplt.text(4.2, min_theta1, f'{min_theta1:.2f}', color='gray', fontsize=10)\nplt.legend(bbox_to_anchor=(0, 1.05), loc='lower left')\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n\n\nNameError: name 'nll_with_prior' is not defined\n\n\n\ndef lin_reg_map(noise_std, prior_std_lambda, n_samples):\n    torch.manual_seed(42)\n\n    x = torch.linspace(-1, 1, n_samples)\n    f = lambda x: 3*x + 1\n\n    noise = torch.distributions.Normal(0, noise_std).sample((n_samples,))\n    y = f(x) + noise\n\n    def nll(theta):\n        mu = theta[0] + theta[1] * x\n        sigma = torch.tensor(1.0)\n        dist = torch.distributions.normal.Normal(mu, sigma)\n        return -dist.log_prob(y).sum()\n\n    def nll_with_prior(theta):\n        theta = torch.tensor(theta)\n        mu = theta[0] + theta[1] * x\n        sigma_likelihood = torch.tensor(1.0)\n        dist_likelihood = torch.distributions.normal.Normal(mu, sigma_likelihood)\n\n        prior_mean = torch.tensor([0.0, 0.0])  # Prior mean for slope and intercept\n        prior_std = torch.tensor([prior_std_lambda, prior_std_lambda])    # Prior standard deviation for slope and intercept\n        dist_prior = torch.distributions.normal.Normal(prior_mean, prior_std)\n\n        nll_likelihood = -dist_likelihood.log_prob(y).sum()\n        log_prior = dist_prior.log_prob(theta).sum()\n\n        return nll_likelihood + log_prior\n\n    fig, ax = plt.subplots(1,3, figsize=(12, 6))\n    ax[0].scatter(x, y, marker='x', c='k', s=20, label=\"Noisy data\")\n    ax[0].plot(x, f(x), c='k', label=\"True function\")\n\n    def plot_theta_contour(ax, func, title):\n        # Create a grid of theta[0] and theta[1] values\n        theta0_values = torch.linspace(-4, 4, 100)\n        theta1_values = torch.linspace(-4, 4, 100)\n        theta0_mesh, theta1_mesh = torch.meshgrid(theta0_values, theta1_values)\n        nll_values = torch.zeros_like(theta0_mesh)\n\n        # Calculate negative log-likelihood values for each combination of theta[0] and theta[1]\n        for i in range(len(theta0_values)):\n            for j in range(len(theta1_values)):\n                nll_values[i, j] = func([theta0_values[i], theta1_values[j]])\n\n        # Create a contour plot\n        contour = ax.contourf(theta0_mesh, theta1_mesh, nll_values, levels=20, cmap='viridis')\n        # plt.colorbar(contour)\n        ax.set_xlabel(r'$\\theta_0$')\n        ax.set_ylabel(r'$\\theta_1$')\n        ax.set_title(title)\n\n        # Adding contour level labels\n        contour_labels = ax.contour(theta0_mesh, theta1_mesh, nll_values, levels=20, colors='black', linewidths=0.5)\n        ax.clabel(contour_labels, inline=True, fontsize=8, fmt='%1.1f')\n\n        # Find and mark the minimum\n        min_indices = torch.argmin(nll_values)\n        min_theta0 = theta0_mesh.flatten()[min_indices]\n        min_theta1 = theta1_mesh.flatten()[min_indices]\n        ax.scatter(min_theta0, min_theta1, color='red', marker='x', label='Minima')\n\n        # Draw lines from the minimum point to the axes\n        ax.axhline(min_theta1, color='gray', linestyle='--')\n        ax.axvline(min_theta0, color='gray', linestyle='--')\n\n        # Add labels to the lines\n        ax.text(min_theta0, 4.2, f'{min_theta0:.2f}', color='gray', fontsize=10)\n        ax.text(4.2, min_theta1, f'{min_theta1:.2f}', color='gray', fontsize=10)\n        # ax.legend(bbox_to_anchor=(0, -0.05), loc='lower left')\n        return min_theta0, min_theta1\n\n    mle_theta0, mle_theta1 = plot_theta_contour(ax[1], nll, 'NLL (MLE)')\n\n    map_theta0, map_theta1 = plot_theta_contour(ax[2], nll_with_prior, 'NLL + Prior (MAP)')\n\n    ax[0].plot(x, mle_theta0 + mle_theta1*x, c='b', label=\"MLE\")\n    ax[0].plot(x, map_theta0 + map_theta1*x, c='r', label=\"MAP\")\n    ax[0].legend()\n    ax[0].set_xlabel('x')\n    ax[0].set_ylabel('y')\n    # ax[0].set_title(r'$y_{true} = 3 x + 1$')\n    plt.savefig('../figures/map/linreg_mle_map.pdf', bbox_inches='tight')\n    plt.show()\n\nlin_reg_map(noise_std = 3.0, prior_std_lambda = 1.0, n_samples = 30)\n\n\n\n\n\ninteract(\n    lin_reg_map,\n    noise_std = FloatSlider(min=0.1, max=3.0, step=0.1, value=0.1),\n    prior_std_lambda = FloatSlider(min=0.1, max=3.0, step=0.1, value=1.0),\n    n_samples = IntSlider(min=10, max=1000, step=10, value=100)\n    )\n\n\n\n\n&lt;function __main__.lin_reg_map(noise_std, prior_std_lambda, n_samples)&gt;\n\n\n\ndef lin_reg_map_laplace(noise_std, prior_std_lambda, n_samples):\n    torch.manual_seed(42)\n\n    x = torch.linspace(-1, 1, n_samples)\n    f = lambda x: 3*x + 1\n\n    noise = torch.distributions.Normal(0, noise_std).sample((n_samples,))\n    y = f(x) + noise\n\n    def nll(theta):\n        mu = theta[0] + theta[1] * x\n        sigma = torch.tensor(1.0)\n        dist = torch.distributions.normal.Normal(mu, sigma)\n        return -dist.log_prob(y).sum()\n\n    def nll_with_prior(theta):\n        theta = torch.tensor(theta)\n        mu = theta[0] + theta[1] * x\n        sigma_likelihood = torch.tensor(1.0)\n        dist_likelihood = torch.distributions.normal.Normal(mu, sigma_likelihood)\n\n        prior_mean = torch.tensor([0.0, 0.0])  # Prior mean for slope and intercept\n        prior_std = torch.tensor([prior_std_lambda, prior_std_lambda])    # Prior standard deviation for slope and intercept\n        dist_prior = torch.distributions.normal.Normal(prior_mean, prior_std)\n\n        nll_likelihood = -dist_likelihood.log_prob(y).sum()\n        log_prior = dist_prior.log_prob(theta).sum()\n\n        return nll_likelihood + log_prior\n\n    def nll_with_laplace_prior(theta):\n        theta = torch.tensor(theta)\n        mu = theta[0] + theta[1] * x\n        sigma_likelihood = torch.tensor(1.0)\n        dist_likelihood = torch.distributions.normal.Normal(mu, sigma_likelihood)\n\n        prior_mean = torch.tensor([0.0, 0.0])  # Prior mean for slope and intercept\n        prior_std = torch.tensor([prior_std_lambda, prior_std_lambda])     # Prior standard deviation for slope and intercept\n        dist_prior = torch.distributions.laplace.Laplace(prior_mean, prior_std)\n\n        nll_likelihood = -dist_likelihood.log_prob(y).sum()\n        log_prior = dist_prior.log_prob(theta).sum()\n\n        return nll_likelihood + log_prior\n\n    fig, ax = plt.subplots(1,3, figsize=(12, 6))\n    ax[0].scatter(x, y, marker='x', c='k', s=20, label=\"Noisy data\")\n    ax[0].plot(x, f(x), c='k', label=\"True function\")\n\n    def plot_theta_contour(ax, func, title):\n        # Create a grid of theta[0] and theta[1] values\n        theta0_values = torch.linspace(-4, 4, 100)\n        theta1_values = torch.linspace(-4, 4, 100)\n        theta0_mesh, theta1_mesh = torch.meshgrid(theta0_values, theta1_values)\n        nll_values = torch.zeros_like(theta0_mesh)\n\n        # Calculate negative log-likelihood values for each combination of theta[0] and theta[1]\n        for i in range(len(theta0_values)):\n            for j in range(len(theta1_values)):\n                nll_values[i, j] = func([theta0_values[i], theta1_values[j]])\n\n        # Create a contour plot\n        contour = ax.contourf(theta0_mesh, theta1_mesh, nll_values, levels=20, cmap='viridis')\n        # plt.colorbar(contour)\n        ax.set_xlabel(r'$\\theta_0$')\n        ax.set_ylabel(r'$\\theta_1$')\n        ax.set_title(title)\n\n        # Adding contour level labels\n        contour_labels = ax.contour(theta0_mesh, theta1_mesh, nll_values, levels=20, colors='black', linewidths=0.5)\n        ax.clabel(contour_labels, inline=True, fontsize=8, fmt='%1.1f')\n\n        # Find and mark the minimum\n        min_indices = torch.argmin(nll_values)\n        min_theta0 = theta0_mesh.flatten()[min_indices]\n        min_theta1 = theta1_mesh.flatten()[min_indices]\n        ax.scatter(min_theta0, min_theta1, color='red', marker='x', label='Minima')\n\n        # Draw lines from the minimum point to the axes\n        ax.axhline(min_theta1, color='gray', linestyle='--')\n        ax.axvline(min_theta0, color='gray', linestyle='--')\n\n        # Add labels to the lines\n        ax.text(min_theta0, 4.2, f'{min_theta0:.2f}', color='gray', fontsize=10)\n        ax.text(4.2, min_theta1, f'{min_theta1:.2f}', color='gray', fontsize=10)\n        # ax.legend(bbox_to_anchor=(0, -0.05), loc='lower left')\n        return min_theta0, min_theta1\n\n    mle_theta0, mle_theta1 = plot_theta_contour(ax[1], nll, 'NLL (MLE)')\n\n    # map_theta0, map_theta1 = plot_theta_contour(ax[2], nll_with_prior, 'NLL + Prior (MAP)')\n    map_theta0, map_theta1 = plot_theta_contour(ax[2], nll_with_laplace_prior, 'NLL + Prior (MAP)')\n\n    ax[0].plot(x, mle_theta0 + mle_theta1*x, c='b', label=\"MLE\")\n    ax[0].plot(x, map_theta0 + map_theta1*x, c='r', label=\"MAP\")\n    ax[0].legend()\n    ax[0].set_xlabel('x')\n    ax[0].set_ylabel('y')\n    ax[0].set_title(r'$y_{true} = 3 x + 1$')\n    plt.savefig('../figures/map/linreg_mle_map_laplace.pdf', bbox_inches='tight')\n    plt.show()\n\nlin_reg_map_laplace(noise_std = 3.0, prior_std_lambda = 1.0, n_samples = 30)"
  },
  {
    "objectID": "notebooks/variational-inference.html",
    "href": "notebooks/variational-inference.html",
    "title": "Goals:",
    "section": "",
    "text": "Let us first look at G1. Look at the illustration below. We have a normal distribution \\(p\\) and two other normal distributions \\(q_1\\) and \\(q_2\\). Which of \\(q_1\\) and \\(q_2\\), would we consider closer to \\(p\\)? \\(q_2\\), right?\n\nTo understand the notion of similarity, we use a metric called the KL-divergence given as \\(D_{KL}(a || b)\\) where \\(a\\) and \\(b\\) are the two distributions.\nFor G1, we can say \\(q_2\\) is closer to \\(p\\) compared to \\(q_1\\) as:\n\\(D_{KL}(q_2 || p) \\lt D_{KL}(q_1 || p)\\)\nFor the above example, we have the values as \\(D_{KL}(q_2|| p) = 0.07\\) and \\(D_{KL}(q_1|| p)= 0.35\\)"
  },
  {
    "objectID": "notebooks/variational-inference.html#stochastic-vi-todo",
    "href": "notebooks/variational-inference.html#stochastic-vi-todo",
    "title": "Goals:",
    "section": "Stochastic VI [TODO]",
    "text": "Stochastic VI [TODO]\n\nWhen data is large\nWe can use stochastic gradient descent to optimize the ELBO. We can use the following formula to compute the gradient of the ELBO.\nNow, given our linear regression problem setup, we want to maximize the ELBO.\nWe can do so by the following. As a simple example, let us assume \\(\\theta \\in R^2\\)\n\nAssume some q. Say, a Normal distribution. So, \\(q\\sim \\mathcal{N}_2\\)\nDraw samples from q. Say N samples.\nInitilize ELBO = 0.0\nFor each sample:\n\nLet us assume drawn sample is \\([\\theta_1, \\theta_2]^T\\)\nCompute log_prob of prior on \\([\\theta_1, \\theta_2]^T\\) or lp = p.log_prob(θ1, θ2)\nCompute log_prob of likelihood on \\([\\theta_1, \\theta_2]^T\\) or ll = l.log_prob(θ1, θ2)\nCompute log_prob of q on \\([\\theta_1, \\theta_2]^T\\) or lq = q.log_prob(θ1, θ2)\nELBO = ELBO + (ll+lp-q)\n\nReturn ELBO/N\n\n\nprior = dist.Normal(loc = 0., scale = 1.)\np = dist.Normal(loc = 5., scale = 1.)\n\n\nsamples = p.sample([1000])\n\n\nmu = torch.tensor(1.0, requires_grad=True)\n\ndef surrogate_sample(mu):\n    std_normal = dist.Normal(loc = 0., scale=1.)\n    sample_std_normal  = std_normal.sample()\n    return mu + sample_std_normal\n\n\nsamples_from_surrogate = surrogate_sample(mu)\n\n\nsamples_from_surrogate\n\ntensor(0.1894, grad_fn=&lt;AddBackward0&gt;)\n\n\n\ndef logprob_prior(mu):\n    return prior.log_prob(mu)\n\nlp = logprob_prior(samples_from_surrogate)\n\ndef log_likelihood(mu, samples):\n    di = dist.Normal(loc=mu, scale=1)\n    return torch.sum(di.log_prob(samples))\n\nll = log_likelihood(samples_from_surrogate, samples)\n\nls = surrogate.log_prob(samples_from_surrogate)\n\n\n\ndef elbo_loss(mu, data_samples):\n    samples_from_surrogate = surrogate_sample(mu)\n    lp = logprob_prior(samples_from_surrogate)\n    ll = log_likelihood(samples_from_surrogate, data_samples)\n    ls = surrogate.log_prob(samples_from_surrogate)\n\n    return -lp - ll + ls\n\n\nmu = torch.tensor(1.0, requires_grad=True)\n\nloc_array = []\nloss_array = []\n\nopt = torch.optim.Adam([mu], lr=0.02)\nfor i in range(2000):\n    loss_val = elbo_loss(mu, samples)\n    loss_val.backward()\n    loc_array.append(mu.item())\n    loss_array.append(loss_val.item())\n\n    if i % 100 == 0:\n        print(\n            f\"Iteration: {i}, Loss: {loss_val.item():0.2f}, Loc: {mu.item():0.3f}\"\n        )\n    opt.step()\n    opt.zero_grad()\n\nIteration: 0, Loss: 11693.85, Loc: 1.000\nIteration: 100, Loss: 2550.90, Loc: 2.744\nIteration: 200, Loss: 2124.30, Loc: 3.871\nIteration: 300, Loss: 2272.48, Loc: 4.582\nIteration: 400, Loss: 2025.17, Loc: 4.829\nIteration: 500, Loss: 1434.45, Loc: 5.079\nIteration: 600, Loss: 1693.33, Loc: 5.007\nIteration: 700, Loss: 1495.89, Loc: 4.957\nIteration: 800, Loss: 2698.28, Loc: 5.149\nIteration: 900, Loss: 2819.85, Loc: 5.117\nIteration: 1000, Loss: 1491.79, Loc: 5.112\nIteration: 1100, Loss: 1767.87, Loc: 4.958\nIteration: 1200, Loss: 1535.30, Loc: 4.988\nIteration: 1300, Loss: 1458.61, Loc: 4.949\nIteration: 1400, Loss: 1400.21, Loc: 4.917\nIteration: 1500, Loss: 2613.42, Loc: 5.073\nIteration: 1600, Loss: 1411.46, Loc: 4.901\nIteration: 1700, Loss: 1587.94, Loc: 5.203\nIteration: 1800, Loss: 1461.40, Loc: 5.011\nIteration: 1900, Loss: 1504.93, Loc: 5.076\n\n\n\nplt.plot(loss_array)\n\n\n\n\n\nfrom numpy.lib.stride_tricks import sliding_window_view\nplt.plot(np.average(sliding_window_view(loss_array, window_shape = 10), axis=1))\n\n\n\n\n\nLinear Regression\n\ntrue_theta_0 = 3.\ntrue_theta_1 = 4.\n\nx = torch.linspace(-5, 5, 100)\ny_true = true_theta_0 + true_theta_1*x\ny_noisy = y_true + torch.normal(mean = torch.zeros_like(x), std = torch.ones_like(x))\n\nplt.plot(x, y_true)\nplt.scatter(x, y_noisy, s=20, alpha=0.5)\n\n&lt;matplotlib.collections.PathCollection at 0x1407aaac0&gt;\n\n\n\n\n\n\ny_pred = x_dash@theta_prior.sample()\nplt.plot(x, y_pred, label=\"Fit\")\nplt.scatter(x, y_noisy, s=20, alpha=0.5, label='Data')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x14064e850&gt;\n\n\n\n\n\n\ntheta_prior = dist.MultivariateNormal(loc = torch.tensor([0., 0.]), covariance_matrix=torch.eye(2))\n\n\ndef likelihood(theta, x, y):\n    x_dash = torch.vstack((torch.ones_like(x), x)).t()\n    d = dist.Normal(loc=x_dash@theta, scale=torch.ones_like(x))\n    return torch.sum(d.log_prob(y))\n\n\nlikelihood(theta_prior.sample(), x, y_noisy)\n\ntensor(-3558.0769)\n\n\n\nloc = torch.tensor([-1., 1.], requires_grad=True)\nsurrogate_mvn = dist.MultivariateNormal(loc = loc, covariance_matrix=torch.eye(2))\nsurrogate_mvn\n\nMultivariateNormal(loc: torch.Size([2]), covariance_matrix: torch.Size([2, 2]))\n\n\n\nsurrogate_mvn.sample()\n\ntensor([-1.1585,  2.6212])\n\n\n\ndef surrogate_sample_mvn(loc):\n    std_normal_mvn = dist.MultivariateNormal(loc = torch.zeros_like(loc), covariance_matrix=torch.eye(loc.shape[0]))\n    sample_std_normal  = std_normal_mvn.sample()\n    return loc + sample_std_normal\n\n\ndef elbo_loss(loc, x, y):\n    samples_from_surrogate_mvn = surrogate_sample_mvn(loc)\n    lp = theta_prior.log_prob(samples_from_surrogate_mvn)\n    ll = likelihood(samples_from_surrogate_mvn, x, y_noisy)\n    ls = surrogate_mvn.log_prob(samples_from_surrogate_mvn)\n\n    return -lp - ll + ls\n\n\nloc.shape, x.shape, y_noisy.shape\n\n(torch.Size([2]), torch.Size([100]), torch.Size([100]))\n\n\n\nelbo_loss(loc, x, y_noisy)\n\ntensor(2850.3154, grad_fn=&lt;AddBackward0&gt;)\n\n\n\nloc = torch.tensor([-1., 1.], requires_grad=True)\n\n\nloc_array = []\nloss_array = []\n\nopt = torch.optim.Adam([loc], lr=0.02)\nfor i in range(10000):\n    loss_val = elbo_loss(loc, x, y_noisy)\n    loss_val.backward()\n    loc_array.append(mu.item())\n    loss_array.append(loss_val.item())\n\n    if i % 1000 == 0:\n        print(\n            f\"Iteration: {i}, Loss: {loss_val.item():0.2f}, Loc: {loc}\"\n        )\n    opt.step()\n    opt.zero_grad()\n\nIteration: 0, Loss: 5479.97, Loc: tensor([-1.,  1.], requires_grad=True)\nIteration: 1000, Loss: 566.63, Loc: tensor([2.9970, 4.0573], requires_grad=True)\nIteration: 2000, Loss: 362.19, Loc: tensor([2.9283, 3.9778], requires_grad=True)\nIteration: 3000, Loss: 231.23, Loc: tensor([2.8845, 4.1480], requires_grad=True)\nIteration: 4000, Loss: 277.94, Loc: tensor([2.9284, 3.9904], requires_grad=True)\nIteration: 5000, Loss: 1151.51, Loc: tensor([2.9620, 4.0523], requires_grad=True)\nIteration: 6000, Loss: 582.19, Loc: tensor([2.8003, 4.0540], requires_grad=True)\nIteration: 7000, Loss: 178.48, Loc: tensor([2.8916, 3.9968], requires_grad=True)\nIteration: 8000, Loss: 274.76, Loc: tensor([3.0807, 4.1957], requires_grad=True)\nIteration: 9000, Loss: 578.37, Loc: tensor([2.9830, 4.0174], requires_grad=True)\n\n\n\nlearnt_surrogate = dist.MultivariateNormal(loc = loc, covariance_matrix=torch.eye(2))\n\n\ny_samples_surrogate = x_dash@learnt_surrogate.sample([500]).t()\nplt.plot(x, y_samples_surrogate, alpha = 0.02, color='k');\nplt.scatter(x, y_noisy, s=20, alpha=0.5)\n\n&lt;matplotlib.collections.PathCollection at 0x144709a90&gt;\n\n\n\n\n\n\nx_dash@learnt_surrogate.loc.detach().t()\ntheta_sd = torch.linalg.cholesky(learnt_surrogate.covariance_matrix)\n\n\n#y_samples_surrogate = x_dash@learnt_surrogate.loc.t()\n#plt.plot(x, y_samples_surrogate, alpha = 0.02, color='k');\n#plt.scatter(x, y_noisy, s=20, alpha=0.5)\n\ntensor([-1.6542e+01, -1.6148e+01, -1.5754e+01, -1.5360e+01, -1.4966e+01,\n        -1.4572e+01, -1.4178e+01, -1.3784e+01, -1.3390e+01, -1.2996e+01,\n        -1.2602e+01, -1.2208e+01, -1.1814e+01, -1.1420e+01, -1.1026e+01,\n        -1.0632e+01, -1.0238e+01, -9.8441e+00, -9.4501e+00, -9.0561e+00,\n        -8.6621e+00, -8.2681e+00, -7.8741e+00, -7.4801e+00, -7.0860e+00,\n        -6.6920e+00, -6.2980e+00, -5.9040e+00, -5.5100e+00, -5.1160e+00,\n        -4.7220e+00, -4.3280e+00, -3.9340e+00, -3.5400e+00, -3.1460e+00,\n        -2.7520e+00, -2.3579e+00, -1.9639e+00, -1.5699e+00, -1.1759e+00,\n        -7.8191e-01, -3.8790e-01,  6.1054e-03,  4.0011e-01,  7.9412e-01,\n         1.1881e+00,  1.5821e+00,  1.9761e+00,  2.3702e+00,  2.7642e+00,\n         3.1582e+00,  3.5522e+00,  3.9462e+00,  4.3402e+00,  4.7342e+00,\n         5.1282e+00,  5.5222e+00,  5.9162e+00,  6.3102e+00,  6.7043e+00,\n         7.0983e+00,  7.4923e+00,  7.8863e+00,  8.2803e+00,  8.6743e+00,\n         9.0683e+00,  9.4623e+00,  9.8563e+00,  1.0250e+01,  1.0644e+01,\n         1.1038e+01,  1.1432e+01,  1.1826e+01,  1.2220e+01,  1.2614e+01,\n         1.3008e+01,  1.3402e+01,  1.3796e+01,  1.4190e+01,  1.4584e+01,\n         1.4978e+01,  1.5372e+01,  1.5766e+01,  1.6160e+01,  1.6554e+01,\n         1.6948e+01,  1.7342e+01,  1.7736e+01,  1.8131e+01,  1.8525e+01,\n         1.8919e+01,  1.9313e+01,  1.9707e+01,  2.0101e+01,  2.0495e+01,\n         2.0889e+01,  2.1283e+01,  2.1677e+01,  2.2071e+01,  2.2465e+01])\n\n\nTODO\n\nPyro for linear regression example\nHandle more samples in ELBO\nReuse some methods\nAdd figure on reparameterization\nLinear regression learn covariance also\nLinear regression posterior compare with analytical posterior (refer Murphy book)\nClean up code and reuse code whwrever possible\nImprove figures and make them consistent\nAdd background maths wherever needed\nplot the Directed graphical model (refer Maths ML book and render in Pyro)\nLook at the TFP post on https://www.tensorflow.org/probability/examples/Probabilistic_Layers_Regression\nShow the effect of data size (less data, solution towards prior, else dominated by likelihood)\nMean Firld (full covariance v/s diagonal) for surrogate\n\nReferences\n\nhttps://www.youtube.com/watch?v=HUsznqt2V5I\nhttps://www.youtube.com/watch?v=x9StQ8RZ0ag&list=PLISXH-iEM4JlFsAp7trKCWyxeO3M70QyJ&index=9\nhttps://colab.research.google.com/github/goodboychan/goodboychan.github.io/blob/main/_notebooks/2021-09-13-02-Minimizing-KL-Divergence.ipynb#scrollTo=gd_ev8ceII8q\nhttps://goodboychan.github.io/python/coursera/tensorflow_probability/icl/2021/09/13/02-Minimizing-KL-Divergence.html"
  },
  {
    "objectID": "notebooks/variational-inference.html#sandbox",
    "href": "notebooks/variational-inference.html#sandbox",
    "title": "Goals:",
    "section": "Sandbox",
    "text": "Sandbox\n\nfrom typing import Sequence\nt = torch.tensor([1., 2., 3.])\nisinstance(t, Sequence)\n\nFalse\n\n\n\n\n\ntorch.Size([])\n\n\ntensor(1.)"
  },
  {
    "objectID": "notebooks/calculus.html",
    "href": "notebooks/calculus.html",
    "title": "Calculus",
    "section": "",
    "text": "import torch\nimport torch.autograd.functional as F\nimport torch.distributions as dist\nimport jax.numpy as jnp\nimport jax\n\n%matplotlib inline\n\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n\n\nDerivative\n\ndef f1(x):\n    return 3*x**2\n\nx = torch.tensor(2.0, requires_grad=True)\n\n\n# Torch version 1 (using .backward)\nz = f1(x)\nz.backward()\nprint(\"Using backwards in torch\", x.grad)\n\n# Torch version 2 (using autograd.grad)\nprint(\"Using autograd.grad in torch\", torch.autograd.grad(f1(x), x)[0])\n\n# Jax version\nprint(\"Using grad in jax\", jax.grad(f1)(jnp.array(2.0)))\n\nUsing backwards in torch tensor(12.)\nUsing autograd.grad in torch tensor(12.)\nUsing grad in jax 12.0\n\n\n\n\nPartial Derivative\n\ndef f2(x, y):\n    return 2*x**2 + 3*y\nx = torch.tensor(2.0, requires_grad=True)\ny = torch.tensor(1.5, requires_grad=True)\n\n# Torch version 1 (using .backward)\nz = f2(x, y)\nz.backward()\nprint(\"\\nUsing Method 1 Torch\")\nprint(\"Partial wrt x: \", x.grad)\nprint(\"Partial wrt y: \", y.grad)\n\n\n# Torch version 2 (using autograd.grad)\nprint(\"\\nUsing Method 2 Torch\")\nprint(\"Partial wrt x: \", torch.autograd.grad(f2(x, y), x)[0])\nprint(\"Partial wrt y: \", torch.autograd.grad(f2(x, y), y)[0])\n\n# Jax version\nprint(\"\\nUsing Jax\")\nprint(\"Partial wrt x: \", jax.grad(f2, argnums=0)(jnp.array(2.0), jnp.array(1.5)))\nprint(\"Partial wrt y: \", jax.grad(f2, argnums=1)(jnp.array(2.0), jnp.array(1.5)))\n\n\nUsing Method 1 Torch\nPartial wrt x:  tensor(8.)\nPartial wrt y:  tensor(3.)\n\nUsing Method 2 Torch\nPartial wrt x:  tensor(8.)\nPartial wrt y:  tensor(3.)\n\nUsing Jax\nPartial wrt x:  8.0\nPartial wrt y:  3.0\n\n\n\n\nGradient\n\n# Torch version 1 (using .backward)\ngrad_f2_v1 = torch.tensor([x.grad, y.grad])\nprint(\"\\nUsing Method 1 Torch\")\nprint(grad_f2_v1)\n# Torch version 2 (using autograd.grad)\ngrad_f2_v2 = torch.tensor(torch.autograd.grad(f2(x, y), (x, y)))\nprint(\"\\nUsing Method 2 Torch\")\nprint(grad_f2_v2)\n\n# Jax version\ngrad_f2_jax = jax.grad(f2, argnums=[0, 1])(jnp.array(2.0), jnp.array(1.5))\nprint(\"\\nUsing Jax\")\nprint(grad_f2_jax)\n\n\nUsing Method 1 Torch\ntensor([8., 3.])\n\nUsing Method 2 Torch\ntensor([8., 3.])\n\nUsing Jax\n(Array(8., dtype=float32, weak_type=True), Array(3., dtype=float32, weak_type=True))\n\n\n\n\nGradient (vectorized)\n\ndef f2_vectorized(input):\n    x, y = input\n    return 2*x**2 + 3*y\n\ninput = torch.tensor([2.0, 1.5], requires_grad=True)\n\n# Torch version 1 (using .backward)\nz = f2_vectorized(input)\nz.backward()\nprint(\"\\nUsing Method 1 Torch\")\nprint(\"Gradient: \", input.grad)\n\n# Torch version 2 (using autograd.grad)\nprint(\"\\nUsing Method 2 Torch\")\nprint(\"Gradient: \", torch.autograd.grad(f2_vectorized(input), input))\n\n# Jax version\nprint(\"\\nUsing Jax\")\nprint(\"Gradient: \", jax.grad(f2_vectorized)(jnp.array([2.0, 1.5])))\n\n\nUsing Method 1 Torch\nGradient:  tensor([8., 3.])\n\nUsing Method 2 Torch\nGradient:  (tensor([8., 3.]),)\n\nUsing Jax\nGradient:  [8. 3.]\n\n\n\n\nJacobian\n\n# We take the Jacobian of the function f(x, y, z) = [x**2 + y**2, y - z]\n# The Jacobian analytically is [[2x, 2y, 0], [0, 1, -1]]\ndef f1(x, y, z):\n        return x**2 + y**2\ndef f2(x, y, z):\n        return y - z\n\ndef f(x, y, z):\n    return torch.stack([f1(x, y, z), f2(x, y, z)])\n\nx = torch.tensor(2.0, requires_grad=True)\ny = torch.tensor(1.0, requires_grad=True)\nz = torch.tensor(3.0, requires_grad=True)\n\noutput = f(x, y, z)\noutput\n\ntensor([ 5., -2.], grad_fn=&lt;StackBackward0&gt;)\n\n\nIf we are to directly call: z.backward() we get the following error.\n\ntry:\n    output.backward()\nexcept Exception as e:\n    print(e)\n\ngrad can be implicitly created only for scalar outputs\n\n\nBut, output.backward() has a parameter called gradient\n\nThe graph is differentiated using the chain rule. If the tensor is non-scalar (i.e. its data has more than one element) and requires gradient, the function additionally requires specifying gradient. It should be a tensor of matching type and location, that contains the gradient of the differentiated function w.r.t. self.\n\n\nimport torch.autograd.functional as F\n\njacobian = torch.vstack(F.jacobian(f, (x, y, z))).T\nprint(\"Jacobian with Functional method:\")\nprint(jacobian)\n\n# use torch.autograd.grad\njacobian = torch.zeros(2, 3)\nfor output_index in range(2):\n    jacobian[output_index] = torch.vstack(torch.autograd.grad(f(x, y, z)[output_index], (x, y, z))).ravel()\nprint(\"Jacobian with grad method:\")\nprint(jacobian)\n\nJacobian with Functional method:\ntensor([[ 4.,  2., -0.],\n        [ 0.,  1., -1.]])\nJacobian with grad method:\ntensor([[ 4.,  2., -0.],\n        [ 0.,  1., -1.]])\n\n\n\n# Jax version using jax.jacobian\ndef f_jax(x, y, z):\n    return jnp.stack([f1(x, y, z), f2(x, y, z)])\n\nprint(\"Jax Jacobian using jax.jacobian:\")\nprint(jnp.array(jax.jacobian(f_jax, argnums=[0, 1, 2])(jnp.array(2.0), jnp.array(1.0), jnp.array(3.0))).T)\n\ng1 = jnp.array(jax.grad(f1, argnums=[0, 1, 2])(jnp.array(2.0), jnp.array(1.0), jnp.array(3.0)))\ng2 = jnp.array(jax.grad(f2, argnums=[0, 1, 2])(jnp.array(2.0), jnp.array(1.0), jnp.array(3.0)))\nprint(\"Jax Jacobian using jax.grad done manually:\")\nprint(jnp.vstack([g1.T, g2.T]))\n\nJax Jacobian using jax.jacobian:\n[[ 4.  2. -0.]\n [ 0.  1. -1.]]\nJax Jacobian using jax.grad done manually:\n[[ 4.  2.  0.]\n [ 0.  1. -1.]]\n\n\n\n\nJacobian (vectorized)\n\ndef f_vectorized(input):\n    x, y, z = input\n    return torch.stack([f1(x, y, z), f2(x, y, z)])\n\nprint(\"Torch Functional method\")\nprint(F.jacobian(f_vectorized, torch.tensor([2.0, 1.0, 3.0])))\n\ndef f_vectorized_jax(input):\n    x, y, z = input\n    return jnp.stack([f1(x, y, z), f2(x, y, z)])\n\nprint(\"Jax Jacobian using jax.jacobian:\")\nprint(jax.jacobian(f_vectorized_jax)(jnp.array([2.0, 1.0, 3.0])))\n\nTorch Functional method\ntensor([[ 4.,  2., -0.],\n        [ 0.,  1., -1.]])\nJax Jacobian using jax.jacobian:\n[[ 4.  2.  0.]\n [ 0.  1. -1.]]\n\n\n\n\nHessian\n\ndef f(x, y, z):\n    return x**2 + y**2 + x * y * z\n\nx = torch.tensor(2.0, requires_grad=True)\ny = torch.tensor(1.0, requires_grad=True)\nz = torch.tensor(3.0, requires_grad=True)\n\n# Torch version using autograd.functional.hessian\nprint(\"Hessian using autograd.functional.hessian:\")\ntorch_v1_hessian = torch.tensor(F.hessian(f, (x, y, z)))\nprint(torch_v1_hessian)\n\n# Jax version using jax.hessian\njax_hessian = jnp.array(jax.hessian(f, argnums=[0, 1, 2])(jnp.array(2.0), jnp.array(1.0), jnp.array(3.0)))\nprint(\"Jax Hessian using jax.hessian:\")\nprint(jax_hessian)\n\n# Jax version using jax.jacobian\njacobian_fn = jax.jacobian(f, argnums=[0, 1, 2])\nhessian_fn = jax.jacobian(jacobian_fn, argnums=[0, 1, 2])\njax_hessian = jnp.array(hessian_fn(jnp.array(2.0), jnp.array(1.0), jnp.array(3.0)))\nprint(\"Jax Hessian using jax.jacobian:\")\nprint(jax_hessian)\n\nHessian using autograd.functional.hessian:\ntensor([[2., 3., 1.],\n        [3., 2., 2.],\n        [1., 2., 0.]])\nJax Hessian using jax.hessian:\n[[2. 3. 1.]\n [3. 2. 2.]\n [1. 2. 0.]]\nJax Hessian using jax.jacobian:\n[[2. 3. 1.]\n [3. 2. 2.]\n [1. 2. 0.]]\n\n\n\n\nHessian (vectorized)\n\ndef f_vectorized(input):\n    x, y, z = input\n    return x**2 + y**2 + x * y * z\n\nprint(\"Torch Functional method\")\nprint(F.hessian(f_vectorized, torch.tensor([2.0, 1.0, 3.0])))\n\nprint(\"Jax Hessian using jax.hessian:\")\nprint(jax.hessian(f_vectorized)(jnp.array([2.0, 1.0, 3.0])))\n\nprint(\"Jax Hessian using jax.jacobian:\")\nprint(jax.jacobian(jax.jacobian(f_vectorized))(jnp.array([2.0, 1.0, 3.0])))\n\nTorch Functional method\ntensor([[2., 3., 1.],\n        [3., 2., 2.],\n        [1., 2., 0.]])\nJax Hessian using jax.hessian:\n[[2. 3. 1.]\n [3. 2. 2.]\n [1. 2. 0.]]\nJax Hessian using jax.jacobian:\n[[2. 3. 1.]\n [3. 2. 2.]\n [1. 2. 0.]]"
  },
  {
    "objectID": "notebooks/mc-linreg-evidence.html",
    "href": "notebooks/mc-linreg-evidence.html",
    "title": "Computing the evidence term",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n%matplotlib inline\n\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\n\nprior_mu = torch.zeros(2)\nprior_cov = torch.eye(2)\ntheta = torch.distributions.MultivariateNormal(prior_mu, prior_cov)\ntheta\n\nMultivariateNormal(loc: torch.Size([2]), covariance_matrix: torch.Size([2, 2]))\n\n\n\ntheta.loc, theta.covariance_matrix\n\n(tensor([0., 0.]),\n tensor([[1., 0.],\n         [0., 1.]]))\n\n\n\n### True data\n\n# True parameters\ntrue_theta = torch.tensor([1.5, 0.5])\n\n# Generate data\nx = torch.linspace(-1, 1, 100)\ntorch.manual_seed(42)\ny = true_theta[0] + true_theta[1] * x + torch.randn_like(x) * 0.2\n\nplt.scatter(x, y, marker='x', color='k')\nplt.xlabel('x')\nplt.ylabel('y')\n\nplt.plot(x, true_theta[0] + true_theta[1] * x, color='k', linestyle='--')\n\n\n\n\n\\[ I = \\int p(\\mathcal{D} \\mid \\theta) p(\\theta) \\mathrm{d}\\theta \\]\n\\[ I \\approx \\frac{1}{N} \\sum_{i=1}^N p(\\mathcal{D} \\mid \\theta_i) \\]\nwhere \\(\\theta_i \\sim p(\\theta)\\)\n\n# Plot the prior in 2d contour\n\ntheta1 = torch.linspace(-3, 3, 100)\ntheta2 = torch.linspace(-3, 3, 100)\n\ntheta1, theta2 = torch.meshgrid(theta1, theta2)\n\ntheta_values = torch.stack((theta1, theta2), dim=-1)  # Shape: (100, 100, 2)\n\nz = theta.log_prob(theta_values.view(-1, 2))  # Shape: (10000,)\nz = z.view(100, 100)  # Reshape to (100, 100)\n\nplt.contourf(theta1.numpy(), theta2.numpy(), z.numpy(), 20)\nplt.gca().set_aspect('equal')\n\nplt.colorbar()\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n\n\n&lt;matplotlib.colorbar.Colorbar at 0x7feea97f91c0&gt;\n\n\n\n\n\n\ntheta.sample((10,))\n\ntensor([[ 0.7262,  0.0912],\n        [-0.3891,  0.5279],\n        [-0.3609, -0.0606],\n        [ 0.0733,  0.8187],\n        [ 1.4805,  0.3449],\n        [-1.4241, -0.1163],\n        [ 0.2176, -0.0467],\n        [-1.4335, -0.5665],\n        [-0.4253,  0.2625],\n        [-1.4391,  0.5214]])\n\n\n\ntheta_sample = theta.sample((1000,))\ntheta_sample\n\ntensor([[ 1.0414, -0.3997],\n        [-2.2933,  0.4976],\n        [-0.4257, -1.3371],\n        ...,\n        [-0.5654,  0.2558],\n        [-1.5377, -0.1796],\n        [-0.1124,  0.2712]])\n\n\n\ndef log_likelihood(theta, x, y):\n    \"\"\"Compute the likelihood of a linear regression model.\"\"\"\n    theta = theta.view(1, 2)  # Shape: (1, 2)\n    x = x.view(-1, 1)  # Shape: (100, 1)\n    y = y.view(-1, 1)  # Shape: (100, 1)\n    mean = theta[:, 0] + theta[:, 1] * x  # Shape: (100, 1)\n    return torch.distributions.Normal(mean, 0.2).log_prob(y).sum()  # Shape: (1,)\n\n\nlog_likelihood(theta_sample[0], x, y), log_likelihood(theta_sample[1], x, y), log_likelihood(true_theta, x, y)\n\n(tensor(-595.4868), tensor(-18079.3672), tensor(20.7114))\n\n\n\ntheta_sample[0], theta_sample[1], true_theta\n\n(tensor([ 1.0414, -0.3997]),\n tensor([-2.2933,  0.4976]),\n tensor([1.5000, 0.5000]))\n\n\n\nN = 100\nfor i in range(N):\n    theta_s = theta.sample()\n    plt.plot(x, theta_s[0] + theta_s[1] * x, color='k', alpha=0.1)\nplt.plot(x, true_theta[0] + true_theta[1] * x, color='k', linestyle='--')\n\n\n\n\n\ntheta_sample\n\ntensor([[ 1.0414, -0.3997],\n        [-2.2933,  0.4976],\n        [-0.4257, -1.3371],\n        ...,\n        [-0.5654,  0.2558],\n        [-1.5377, -0.1796],\n        [-0.1124,  0.2712]])\n\n\n\n# Print the log likelihood of the true parameters, and first two samples\nll1 = log_likelihood(true_theta, x, y)\nll2 = log_likelihood(theta_sample[0], x, y)\nll3 = log_likelihood(theta_sample[1], x, y)\nprint(f'LL true: {ll1:.2f}, LL sample 1: {ll2:.2f}, LL sample 2: {ll3:.2f}')\nprint(f'Likelihood true: {ll1.exp():.2f}, Likelihood sample 1: {ll2.exp():.2f}, Likelihood sample 2: {ll3.exp():.2f}')\n\n\nLL true: 20.71, LL sample 1: -595.49, LL sample 2: -18079.37\nLikelihood true: 988233536.00, Likelihood sample 1: 0.00, Likelihood sample 2: 0.00\n\n\n\n# It seems we will have numerical problems. Let us use the log-sum-exp trick to compute the log evidence.\nN = 15000\n# Initialize a list to store log likelihood values\nlog_likelihood_values = []\n\n# Monte Carlo estimation of the log marginal likelihood\nfor i in range(N):\n    # Sample θ from the prior distribution\n    theta_sample = theta.sample()\n\n    # Calculate the log likelihood using your provided function\n    log_likelihood_value = log_likelihood(theta_sample, x, y)\n\n    # Append the log likelihood value to the list\n    log_likelihood_values.append(log_likelihood_value.item())\n\n# Use the log-sum-exp trick to compute the log marginal likelihood\nlog_marginal_likelihood = -torch.log(torch.tensor(N)) + torch.logsumexp(torch.tensor(log_likelihood_values), dim=0)\n\nprint(\"Log marginal likelihood:\", log_marginal_likelihood.item())\n\nLog marginal likelihood: 11.202832221984863\n\n\n\ntorch.tensor(log_likelihood_values).min()\n\ntensor(-36610.6055)\n\n\n\ntorch.tensor(log_likelihood_values).max()\n\ntensor(19.7690)\n\n\n\n# Design matrix X\nX = torch.stack([torch.ones_like(x), x], dim=1)\n\n# Variance of the Gaussian noise in the likelihood\nsigma_squared = 0.2**2\n\n# Calculate the marginal likelihood (evidence) using the provided expression\nmarginal_likelihood = torch.distributions.MultivariateNormal(torch.matmul(X, prior_mu), torch.matmul(torch.matmul(X, prior_cov), X.t()) + sigma_squared * torch.eye(len(x)))\n\n\nmarginal_likelihood\n\nMultivariateNormal(loc: torch.Size([100]), covariance_matrix: torch.Size([100, 100]))\n\n\n\nlog_prob_data = marginal_likelihood.log_prob(y)\n\n\nlog_prob_data\n\ntensor(12.3548)"
  },
  {
    "objectID": "notebooks/sampling/gibbs_sampling_bivariate_normal.html",
    "href": "notebooks/sampling/gibbs_sampling_bivariate_normal.html",
    "title": "Gibbs Sampling for Bivariate Normal Distribution",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n# Retina display\n%config InlineBackend.figure_format = 'retina'\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\n\ntorch.manual_seed(42)\nmean = torch.tensor([0.0, 0.0], dtype=torch.float64)\ncovariance = torch.tensor([[2.0, 1.0], [1.0, 2.0]], dtype=torch.float64)\n\ntheta0_range = np.linspace(-5, 5, 100)\ntheta1_range = np.linspace(-5, 5, 100)\nTheta0, Theta1 = np.meshgrid(theta0_range, theta1_range)\n\n\nrho = covariance[0, 1] / torch.sqrt(covariance[0, 0] * covariance[1, 1])\n\n\npdf_values = np.zeros_like(Theta0)\nfor i in range(pdf_values.shape[0]):\n    for j in range(pdf_values.shape[1]):\n        theta = torch.tensor([Theta0[i, j], Theta1[i, j]], dtype=torch.float64)\n        diff = theta - mean\n        inv_covariance = torch.linalg.inv(covariance)\n        exponent = -0.5 * torch.dot(diff, torch.mv(inv_covariance, diff))\n        pdf_values[i, j] = torch.exp(exponent).item()\n\n\ninitial_guess = torch.tensor([10.0, 10.0])\nsamples = []\nsamples.append(initial_guess.clone().detach())\ncurrent_estimate = initial_guess.clone()\nfor i in range(6):\n    if (i % 2 == 0):\n        conditional_mean_theta0 = current_estimate[1]*rho\n        conditional_std_theta0 = (1-rho**2)**0.5\n        theta0_sample = torch.normal(\n            conditional_mean_theta0, conditional_std_theta0)\n        old_theta1 = current_estimate[1]\n        current_estimate = torch.tensor([theta0_sample, old_theta1])\n    else:\n        conditional_mean_theta1 = current_estimate[0]*rho\n        conditional_std_theta1 = (1-rho**2)**0.5\n        theta1_sample = torch.normal(\n            conditional_mean_theta1, conditional_std_theta1)\n        old_theta0 = current_estimate[0]\n        current_estimate = torch.tensor([old_theta0, theta1_sample])\n\n    samples.append(current_estimate.clone().detach())\n\nsamples_tensor = torch.stack(samples)\n\n\ncontour = plt.contour(Theta0, Theta1, pdf_values)\nplt.clabel(contour, inline=1)\nplt.plot(samples_tensor[:, 0], samples_tensor[:, 1],\n         marker='o', label='Gibbs Samples')\nplt.plot(samples_tensor[:, 0], samples_tensor[:, 1], linewidth=1, alpha=0.1)\nplt.xlabel('Theta 0')\nplt.ylabel('Theta 1')\nplt.title('Gibb\\'s Sampling for Bivariate Normal distribution')\nplt.scatter(mean[0], mean[1], marker='x', s=100, label='True Mean')\nplt.legend()\nplt.colorbar(contour)\n# plt.savefig('figures/sampling/gibbs_sampling_bivariate_normal.pdf', transparent=True, bbox_inches='tight', dpi=300)\n\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed"
  },
  {
    "objectID": "notebooks/sampling/inverse_cdf_normal.html",
    "href": "notebooks/sampling/inverse_cdf_normal.html",
    "title": "Inverse CDF Sampling",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n# Retina display\n%config InlineBackend.figure_format = 'retina'\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\n\nmu = 0.0\nsigma = 1.0\nnum_samples = [25, 100, 1000]\n\n\nu1 = torch.rand(num_samples[0], generator=torch.Generator().manual_seed(1))\nu2 = torch.rand(num_samples[1], generator=torch.Generator().manual_seed(2))\nu3 = torch.rand(num_samples[2], generator=torch.Generator().manual_seed(3))\nx_samples1 = mu + sigma * torch.erfinv(2*u1 - 1) * (2 ** 0.5)\nx_samples2 = mu + sigma * torch.erfinv(2*u2 - 1) * (2 ** 0.5)\nx_samples3 = mu + sigma * torch.erfinv(2*u3 - 1) * (2 ** 0.5)\n\n\nfig, ax = plt.subplots(1, 2)\nax[0].axhline(1, linewidth=0.5)\nax[0].hist(u1.numpy(), bins=10, density=True)\nax[0].set_title('Samples from Uniform Distribution')\nax[0].set_xlabel('Value')\nax[0].set_ylabel('Density')\nax[1].hist(x_samples1.numpy(), bins=10, density=True)\nsns.kdeplot(x_samples1.numpy())\nax[1].set_title('Approximated Normal Distribution')\nax[1].set_xlabel('Value')\nax[1].set_ylabel('Density')\n# plt.savefig('figures/sampling/inverse_cdf_normal1.pdf', transparent=True, bbox_inches='tight', dpi=300)\n\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\n\n\n\n\n\n\nfig, ax = plt.subplots(1, 2)\nax[0].axhline(1, linewidth=0.5)\nax[0].hist(u2.numpy(), bins=10, density=True)\nax[0].set_title('Samples from Uniform Distribution')\nax[0].set_xlabel('Value')\nax[0].set_ylabel('Density')\nax[1].hist(x_samples2.numpy(), bins=10, density=True)\nsns.kdeplot(x_samples2.numpy())\nax[1].set_title('Approximated Normal Distribution')\nax[1].set_xlabel('Value')\nax[1].set_ylabel('Density')\n# plt.savefig('figures/sampling/inverse_cdf_normal2.pdf', transparent=True, bbox_inches='tight', dpi=300)\n\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\n\n\n\n\n\n\nfig, ax = plt.subplots(1, 2)\nax[0].axhline(1, linewidth=0.5)\nax[0].hist(u3.numpy(), bins=10, density=True)\nax[0].set_title('Samples from Uniform Distribution')\nax[0].set_xlabel('Value')\nax[0].set_ylabel('Density')\nax[1].hist(x_samples3.numpy(), bins=10, density=True)\nsns.kdeplot(x_samples3.numpy())\nax[1].set_title('Approximated Normal Distribution')\nax[1].set_xlabel('Value')\nax[1].set_ylabel('Density')\n# plt.savefig('figures/sampling/inverse_cdf_normal3.pdf', transparent=True, bbox_inches='tight', dpi=300)\n\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed"
  },
  {
    "objectID": "notebooks/sampling/mc_sampling_dist.html",
    "href": "notebooks/sampling/mc_sampling_dist.html",
    "title": "Monte Carlo Sampling",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n# Retina display\n%config InlineBackend.figure_format = 'retina'\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\n\nx_samples = torch.linspace(-2, 2, 200)\nlower_limit = -2\nupper_limit = 2\npx_uniform = 1 / (upper_limit - lower_limit) * torch.ones(len(x_samples))\n\n\ndef custom_fn(x): return x**2\n\n\ny = custom_fn(x_samples)\ny_pdf = 1 / (2 * torch.sqrt(y + 1e-2))\n\n# monte carlo samples\nn = 1000\n\n\ntorch.manual_seed(0)  # Set random seed for reproducibility\nuniform_samples = torch.rand(n, 1) * (upper_limit - lower_limit) + lower_limit\nfn_samples = custom_fn(uniform_samples)\n\nfig, ax = plt.subplots(nrows=1, ncols=3)\n\nax[0].set_title(\"Uniform distribution\")\nax[0].plot(x_samples, px_uniform, \"-\")\nax[0].set_xlabel(\"$x$\")\nax[0].set_ylabel(\"$p(x)$\")\n\nax[1].set_title(\"Analytical p(y), $f(x)$ = $x^2$\")\nax[1].plot(y, y_pdf, \"-\", linewidth=2)\nax[1].set_xlabel(\"$y$\")\nax[1].set_ylabel(\"$p(y)$\")\n\nax[2].set_title(\"Monte carlo approximation\")\nsns.histplot(fn_samples.numpy(), kde=False,\n             ax=ax[2], bins=20, stat=\"density\", edgecolor=\"k\", linewidth=1, legend=False)\nax[2].set_xlabel(\"$y$\")\nax[2].set_ylabel(\"$Frequency$\")\n\nsns.despine()\n# plt.savefig(\"figures/sampling/mc_sampling_ex.pdf\", bbox_inches=\"tight\")\nplt.show()\n\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed"
  },
  {
    "objectID": "notebooks/variational-inference-zeel.html",
    "href": "notebooks/variational-inference-zeel.html",
    "title": "Goals:",
    "section": "",
    "text": "Let us first look at G1. Look at the illustration below. We have a normal distribution \\(p\\) and two other normal distributions \\(q_1\\) and \\(q_2\\). Which of \\(q_1\\) and \\(q_2\\), would we consider closer to \\(p\\)? \\(q_2\\), right?\n\nTo understand the notion of similarity, we use a metric called the KL-divergence given as \\(D_{KL}(a || b)\\) where \\(a\\) and \\(b\\) are the two distributions.\nFor G1, we can say \\(q_2\\) is closer to \\(p\\) compared to \\(q_1\\) as:\n\\(D_{KL}(q_2 || p) \\lt D_{KL}(q_1 || p)\\)\nFor the above example, we have the values as \\(D_{KL}(q_2|| p) = 0.07\\) and \\(D_{KL}(q_1|| p)= 0.35\\)"
  },
  {
    "objectID": "notebooks/variational-inference-zeel.html#stochastic-vi-todo",
    "href": "notebooks/variational-inference-zeel.html#stochastic-vi-todo",
    "title": "Goals:",
    "section": "Stochastic VI [TODO]",
    "text": "Stochastic VI [TODO]\n\nWhen data is large\nWe can use stochastic gradient descent to optimize the ELBO. We can use the following formula to compute the gradient of the ELBO.\nNow, given our linear regression problem setup, we want to maximize the ELBO.\nWe can do so by the following. As a simple example, let us assume \\(\\theta \\in R^2\\)\n\nAssume some q. Say, a Normal distribution. So, \\(q\\sim \\mathcal{N}_2\\)\nDraw samples from q. Say N samples.\nInitilize ELBO = 0.0\nFor each sample:\n\nLet us assume drawn sample is \\([\\theta_1, \\theta_2]^T\\)\nCompute log_prob of prior on \\([\\theta_1, \\theta_2]^T\\) or lp = p.log_prob(θ1, θ2)\nCompute log_prob of likelihood on \\([\\theta_1, \\theta_2]^T\\) or ll = l.log_prob(θ1, θ2)\nCompute log_prob of q on \\([\\theta_1, \\theta_2]^T\\) or lq = q.log_prob(θ1, θ2)\nELBO = ELBO + (ll+lp-q)\n\nReturn ELBO/N\n\n\nprior = dist.Normal(loc = 0., scale = 1.)\np = dist.Normal(loc = 5., scale = 1.)\n\n\nsamples = p.sample([1000])\n\n\nmu = torch.tensor(1.0, requires_grad=True)\n\ndef surrogate_sample(mu):\n    std_normal = dist.Normal(loc = 0., scale=1.)\n    sample_std_normal  = std_normal.sample()\n    return mu + sample_std_normal\n\n\nsamples_from_surrogate = surrogate_sample(mu)\n\n\nsamples_from_surrogate\n\ntensor(0.1894, grad_fn=&lt;AddBackward0&gt;)\n\n\n\ndef logprob_prior(mu):\n    return prior.log_prob(mu)\n\nlp = logprob_prior(samples_from_surrogate)\n\ndef log_likelihood(mu, samples):\n    di = dist.Normal(loc=mu, scale=1)\n    return torch.sum(di.log_prob(samples))\n\nll = log_likelihood(samples_from_surrogate, samples)\n\nls = surrogate.log_prob(samples_from_surrogate)\n\n\n\ndef elbo_loss(mu, data_samples):\n    samples_from_surrogate = surrogate_sample(mu)\n    lp = logprob_prior(samples_from_surrogate)\n    ll = log_likelihood(samples_from_surrogate, data_samples)\n    ls = surrogate.log_prob(samples_from_surrogate)\n\n    return -lp - ll + ls\n\n\nmu = torch.tensor(1.0, requires_grad=True)\n\nloc_array = []\nloss_array = []\n\nopt = torch.optim.Adam([mu], lr=0.02)\nfor i in range(2000):\n    loss_val = elbo_loss(mu, samples)\n    loss_val.backward()\n    loc_array.append(mu.item())\n    loss_array.append(loss_val.item())\n\n    if i % 100 == 0:\n        print(\n            f\"Iteration: {i}, Loss: {loss_val.item():0.2f}, Loc: {mu.item():0.3f}\"\n        )\n    opt.step()\n    opt.zero_grad()\n\nIteration: 0, Loss: 11693.85, Loc: 1.000\nIteration: 100, Loss: 2550.90, Loc: 2.744\nIteration: 200, Loss: 2124.30, Loc: 3.871\nIteration: 300, Loss: 2272.48, Loc: 4.582\nIteration: 400, Loss: 2025.17, Loc: 4.829\nIteration: 500, Loss: 1434.45, Loc: 5.079\nIteration: 600, Loss: 1693.33, Loc: 5.007\nIteration: 700, Loss: 1495.89, Loc: 4.957\nIteration: 800, Loss: 2698.28, Loc: 5.149\nIteration: 900, Loss: 2819.85, Loc: 5.117\nIteration: 1000, Loss: 1491.79, Loc: 5.112\nIteration: 1100, Loss: 1767.87, Loc: 4.958\nIteration: 1200, Loss: 1535.30, Loc: 4.988\nIteration: 1300, Loss: 1458.61, Loc: 4.949\nIteration: 1400, Loss: 1400.21, Loc: 4.917\nIteration: 1500, Loss: 2613.42, Loc: 5.073\nIteration: 1600, Loss: 1411.46, Loc: 4.901\nIteration: 1700, Loss: 1587.94, Loc: 5.203\nIteration: 1800, Loss: 1461.40, Loc: 5.011\nIteration: 1900, Loss: 1504.93, Loc: 5.076\n\n\n\nplt.plot(loss_array)\n\n\n\n\n\nfrom numpy.lib.stride_tricks import sliding_window_view\nplt.plot(np.average(sliding_window_view(loss_array, window_shape = 10), axis=1))\n\n\n\n\n\nLinear Regression\n\ntrue_theta_0 = 3.\ntrue_theta_1 = 4.\n\nx = torch.linspace(-5, 5, 100)\ny_true = true_theta_0 + true_theta_1*x\ny_noisy = y_true + torch.normal(mean = torch.zeros_like(x), std = torch.ones_like(x))\n\nplt.plot(x, y_true)\nplt.scatter(x, y_noisy, s=20, alpha=0.5)\n\n&lt;matplotlib.collections.PathCollection at 0x1407aaac0&gt;\n\n\n\n\n\n\ny_pred = x_dash@theta_prior.sample()\nplt.plot(x, y_pred, label=\"Fit\")\nplt.scatter(x, y_noisy, s=20, alpha=0.5, label='Data')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x14064e850&gt;\n\n\n\n\n\n\ntheta_prior = dist.MultivariateNormal(loc = torch.tensor([0., 0.]), covariance_matrix=torch.eye(2))\n\n\ndef likelihood(theta, x, y):\n    x_dash = torch.vstack((torch.ones_like(x), x)).t()\n    d = dist.Normal(loc=x_dash@theta, scale=torch.ones_like(x))\n    return torch.sum(d.log_prob(y))\n\n\nlikelihood(theta_prior.sample(), x, y_noisy)\n\ntensor(-3558.0769)\n\n\n\nloc = torch.tensor([-1., 1.], requires_grad=True)\nsurrogate_mvn = dist.MultivariateNormal(loc = loc, covariance_matrix=torch.eye(2))\nsurrogate_mvn\n\nMultivariateNormal(loc: torch.Size([2]), covariance_matrix: torch.Size([2, 2]))\n\n\n\nsurrogate_mvn.sample()\n\ntensor([-1.1585,  2.6212])\n\n\n\ndef surrogate_sample_mvn(loc):\n    std_normal_mvn = dist.MultivariateNormal(loc = torch.zeros_like(loc), covariance_matrix=torch.eye(loc.shape[0]))\n    sample_std_normal  = std_normal_mvn.sample()\n    return loc + sample_std_normal\n\n\ndef elbo_loss(loc, x, y):\n    samples_from_surrogate_mvn = surrogate_sample_mvn(loc)\n    lp = theta_prior.log_prob(samples_from_surrogate_mvn)\n    ll = likelihood(samples_from_surrogate_mvn, x, y_noisy)\n    ls = surrogate_mvn.log_prob(samples_from_surrogate_mvn)\n\n    return -lp - ll + ls\n\n\nloc.shape, x.shape, y_noisy.shape\n\n(torch.Size([2]), torch.Size([100]), torch.Size([100]))\n\n\n\nelbo_loss(loc, x, y_noisy)\n\ntensor(2850.3154, grad_fn=&lt;AddBackward0&gt;)\n\n\n\nloc = torch.tensor([-1., 1.], requires_grad=True)\n\n\nloc_array = []\nloss_array = []\n\nopt = torch.optim.Adam([loc], lr=0.02)\nfor i in range(10000):\n    loss_val = elbo_loss(loc, x, y_noisy)\n    loss_val.backward()\n    loc_array.append(mu.item())\n    loss_array.append(loss_val.item())\n\n    if i % 1000 == 0:\n        print(\n            f\"Iteration: {i}, Loss: {loss_val.item():0.2f}, Loc: {loc}\"\n        )\n    opt.step()\n    opt.zero_grad()\n\nIteration: 0, Loss: 5479.97, Loc: tensor([-1.,  1.], requires_grad=True)\nIteration: 1000, Loss: 566.63, Loc: tensor([2.9970, 4.0573], requires_grad=True)\nIteration: 2000, Loss: 362.19, Loc: tensor([2.9283, 3.9778], requires_grad=True)\nIteration: 3000, Loss: 231.23, Loc: tensor([2.8845, 4.1480], requires_grad=True)\nIteration: 4000, Loss: 277.94, Loc: tensor([2.9284, 3.9904], requires_grad=True)\nIteration: 5000, Loss: 1151.51, Loc: tensor([2.9620, 4.0523], requires_grad=True)\nIteration: 6000, Loss: 582.19, Loc: tensor([2.8003, 4.0540], requires_grad=True)\nIteration: 7000, Loss: 178.48, Loc: tensor([2.8916, 3.9968], requires_grad=True)\nIteration: 8000, Loss: 274.76, Loc: tensor([3.0807, 4.1957], requires_grad=True)\nIteration: 9000, Loss: 578.37, Loc: tensor([2.9830, 4.0174], requires_grad=True)\n\n\n\nlearnt_surrogate = dist.MultivariateNormal(loc = loc, covariance_matrix=torch.eye(2))\n\n\ny_samples_surrogate = x_dash@learnt_surrogate.sample([500]).t()\nplt.plot(x, y_samples_surrogate, alpha = 0.02, color='k');\nplt.scatter(x, y_noisy, s=20, alpha=0.5)\n\n&lt;matplotlib.collections.PathCollection at 0x144709a90&gt;\n\n\n\n\n\n\nx_dash@learnt_surrogate.loc.detach().t()\ntheta_sd = torch.linalg.cholesky(learnt_surrogate.covariance_matrix)\n\n\n#y_samples_surrogate = x_dash@learnt_surrogate.loc.t()\n#plt.plot(x, y_samples_surrogate, alpha = 0.02, color='k');\n#plt.scatter(x, y_noisy, s=20, alpha=0.5)\n\ntensor([-1.6542e+01, -1.6148e+01, -1.5754e+01, -1.5360e+01, -1.4966e+01,\n        -1.4572e+01, -1.4178e+01, -1.3784e+01, -1.3390e+01, -1.2996e+01,\n        -1.2602e+01, -1.2208e+01, -1.1814e+01, -1.1420e+01, -1.1026e+01,\n        -1.0632e+01, -1.0238e+01, -9.8441e+00, -9.4501e+00, -9.0561e+00,\n        -8.6621e+00, -8.2681e+00, -7.8741e+00, -7.4801e+00, -7.0860e+00,\n        -6.6920e+00, -6.2980e+00, -5.9040e+00, -5.5100e+00, -5.1160e+00,\n        -4.7220e+00, -4.3280e+00, -3.9340e+00, -3.5400e+00, -3.1460e+00,\n        -2.7520e+00, -2.3579e+00, -1.9639e+00, -1.5699e+00, -1.1759e+00,\n        -7.8191e-01, -3.8790e-01,  6.1054e-03,  4.0011e-01,  7.9412e-01,\n         1.1881e+00,  1.5821e+00,  1.9761e+00,  2.3702e+00,  2.7642e+00,\n         3.1582e+00,  3.5522e+00,  3.9462e+00,  4.3402e+00,  4.7342e+00,\n         5.1282e+00,  5.5222e+00,  5.9162e+00,  6.3102e+00,  6.7043e+00,\n         7.0983e+00,  7.4923e+00,  7.8863e+00,  8.2803e+00,  8.6743e+00,\n         9.0683e+00,  9.4623e+00,  9.8563e+00,  1.0250e+01,  1.0644e+01,\n         1.1038e+01,  1.1432e+01,  1.1826e+01,  1.2220e+01,  1.2614e+01,\n         1.3008e+01,  1.3402e+01,  1.3796e+01,  1.4190e+01,  1.4584e+01,\n         1.4978e+01,  1.5372e+01,  1.5766e+01,  1.6160e+01,  1.6554e+01,\n         1.6948e+01,  1.7342e+01,  1.7736e+01,  1.8131e+01,  1.8525e+01,\n         1.8919e+01,  1.9313e+01,  1.9707e+01,  2.0101e+01,  2.0495e+01,\n         2.0889e+01,  2.1283e+01,  2.1677e+01,  2.2071e+01,  2.2465e+01])\n\n\nTODO\n\nPyro for linear regression example\nHandle more samples in ELBO\nReuse some methods\nAdd figure on reparameterization\nLinear regression learn covariance also\nLinear regression posterior compare with analytical posterior (refer Murphy book)\nClean up code and reuse code whwrever possible\nImprove figures and make them consistent\nAdd background maths wherever needed\nplot the Directed graphical model (refer Maths ML book and render in Pyro)\nLook at the TFP post on https://www.tensorflow.org/probability/examples/Probabilistic_Layers_Regression\nShow the effect of data size (less data, solution towards prior, else dominated by likelihood)\nMean Firld (full covariance v/s diagonal) for surrogate\n\nReferences\n\nhttps://www.youtube.com/watch?v=HUsznqt2V5I\nhttps://www.youtube.com/watch?v=x9StQ8RZ0ag&list=PLISXH-iEM4JlFsAp7trKCWyxeO3M70QyJ&index=9\nhttps://colab.research.google.com/github/goodboychan/goodboychan.github.io/blob/main/_notebooks/2021-09-13-02-Minimizing-KL-Divergence.ipynb#scrollTo=gd_ev8ceII8q\nhttps://goodboychan.github.io/python/coursera/tensorflow_probability/icl/2021/09/13/02-Minimizing-KL-Divergence.html"
  },
  {
    "objectID": "notebooks/variational-inference-zeel.html#sandbox",
    "href": "notebooks/variational-inference-zeel.html#sandbox",
    "title": "Goals:",
    "section": "Sandbox",
    "text": "Sandbox\n\nfrom typing import Sequence\nt = torch.tensor([1., 2., 3.])\nisinstance(t, Sequence)\n\nFalse\n\n\n\n\n\ntorch.Size([])\n\n\ntensor(1.)"
  },
  {
    "objectID": "notebooks/nn-variants-final.html",
    "href": "notebooks/nn-variants-final.html",
    "title": "Capturing uncertainty in neural nets:",
    "section": "",
    "text": "aleatoric\n\nhomoskedastic (fixed)\nhomoskedastic (learnt)\nheteroskedastic\n\nepistemic uncertainty (Laplace approximation)\nboth aleatoric and epistemic uncertainty"
  },
  {
    "objectID": "notebooks/nn-variants-final.html#notation",
    "href": "notebooks/nn-variants-final.html#notation",
    "title": "Capturing uncertainty in neural nets:",
    "section": "Notation",
    "text": "Notation"
  },
  {
    "objectID": "notebooks/nn-variants-final.html#models-capturing-aleatoric-uncertainty",
    "href": "notebooks/nn-variants-final.html#models-capturing-aleatoric-uncertainty",
    "title": "Capturing uncertainty in neural nets:",
    "section": "Models capturing aleatoric uncertainty",
    "text": "Models capturing aleatoric uncertainty\n\nDataset containing homoskedastic noise\n\n# Set a fixed random seed for reproducibility\ntorch.manual_seed(42)\n\n# Define the number of data points\nN = 100\n\n# Create a linearly spaced range of values from -1 to 1\nx_lin = torch.linspace(-1, 1, N)\n\n# Define a function 'f' to model the data\nf = lambda x: 0.5 * x**2 + 0.25 * x**3\n\n# Generate random noise 'eps' with a standard deviation of 0.2\neps = torch.randn(N) * 0.2\n\n# Add noise to the true function to simulate real-world data\ny = f(x_lin) + eps\n\n# Move data to the GPU if available (assuming 'device' is defined)\nx_lin = x_lin.to(device)\ny = y.to(device)\n\n\n# Plot data and true function\nplt.plot(x_lin.cpu(), y.cpu(), \"o\", label=\"data\")\nplt.plot(x_lin.cpu(), f(x_lin).cpu(), label=\"true function\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f613c0934f0&gt;\n\n\n\n\n\n\n\nCase 1.1: Models assuming Homoskedastic noise\n\nCase 1.1.1: Homoskedastic noise is fixed beforehand and not learned\n\n\nclass MeanEstimateNN(torch.nn.Module):\n    def __init__(self, n_hidden=4):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(1, n_hidden)\n        self.fc2 = torch.nn.Linear(n_hidden, n_hidden)\n        self.fc3 = torch.nn.Linear(n_hidden, 1)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = torch.relu(x)\n        x = self.fc2(x)\n        x = torch.relu(x)\n        mu_hat = self.fc3(x)\n        return mu_hat\n\n\ndef loss_homoskedastic_noise(model, x, y, params):\n    \"\"\"\n    Compute the Negative Log Likelihood (NLL) loss for data with Homoskedastic Noise.\n    It assumes that the noise standard deviation is provided as a parameter.\n\n    Args:\n        model (torch.nn.Module): The neural network model used for mean estimation.\n        x (torch.Tensor): Input data tensor.\n        y (torch.Tensor): Target data tensor.\n        params (dict): A dictionary containing model parameters, including 'log_noise_std'.\n\n    Returns:\n        torch.Tensor: The negative log likelihood loss for the given data.\n\n    \"\"\"\n    # Extract the log noise standard deviation from the parameters\n    log_noise_std = params[\"log_noise_std\"]\n\n    # Compute the estimated mean using the model\n    mu_hat = model(x).squeeze()\n\n    # Ensure the shapes of mu_hat and y are compatible\n    assert mu_hat.shape == y.shape\n\n    # Compute the noise standard deviation based on the log value\n    noise_std = torch.exp(log_noise_std).expand_as(mu_hat)\n\n    # Create a Normal distribution with mean mu_hat and standard deviation noise_std\n    dist = torch.distributions.Normal(mu_hat, noise_std)\n\n    # Calculate the negative log likelihood loss and take the mean\n    return -dist.log_prob(y).mean()\n\n\nhomoskedastic_model_fixed_noise = MeanEstimateNN().to(device)\nhomoskedastic_model_fixed_noise\n\nMeanEstimateNN(\n  (fc1): Linear(in_features=1, out_features=4, bias=True)\n  (fc2): Linear(in_features=4, out_features=4, bias=True)\n  (fc3): Linear(in_features=4, out_features=1, bias=True)\n)\n\n\n\nfixed_log_noise_std = torch.log(torch.tensor(0.5)).to(device)\nparams = {\n    \"nn_params\": homoskedastic_model_fixed_noise.state_dict(),\n    \"log_noise_std\": fixed_log_noise_std,\n}\nloss_homoskedastic_noise(homoskedastic_model_fixed_noise, x_lin[:, None], y, params)\n\ntensor(0.7089, device='cuda:0', grad_fn=&lt;NegBackward0&gt;)\n\n\n\ndef plot_results(y_hat, epistemic_std=None, aleatoric_std=None, model_name=\"\"):\n    plt.scatter(x_lin.cpu(), y.cpu(), s=10, color=\"C0\", label=\"Data\")\n    plt.plot(x_lin.cpu(), f(x_lin.cpu()), color=\"C1\", label=\"True function\")\n    plt.plot(x_lin.cpu(), y_hat.cpu(), color=\"C2\", label=model_name)\n    if epistemic_std is not None:\n        plt.fill_between(\n            x_lin.cpu(),\n            (y_hat - 2 * epistemic_std).cpu(),\n            (y_hat + 2 * epistemic_std).cpu(),\n            alpha=0.3,\n            color=\"C3\",\n            label=\"Epistemic uncertainty\",\n        )\n    if aleatoric_std is not None:\n        plt.fill_between(\n            x_lin.cpu(),\n            (y_hat - 2 * aleatoric_std).cpu(),\n            (y_hat + 2 * aleatoric_std).cpu(),\n            alpha=0.3,\n            color=\"C2\",\n            label=\"Aleatoric uncertainty\",\n        )\n\n    plt.xlabel(\"$x$\")\n    plt.ylabel(\"$y$\")\n    plt.legend()\n\n\nwith torch.no_grad():\n    y_hat = homoskedastic_model_fixed_noise(x_lin[:, None]).squeeze()\n\nplot_results(\n    y_hat, aleatoric_std=torch.exp(fixed_log_noise_std), model_name=\"Untrained model\"\n)\n\n\n\n\n\ndef train_fn(model, loss_func, params, x, y, n_epochs=1000, lr=0.01):\n    parameter_leaves = jtu.tree_leaves(params)\n    optimizer = torch.optim.Adam(parameter_leaves, lr=lr)\n    for epoch in range(n_epochs):\n        optimizer.zero_grad()\n        loss = loss_func(model, x, y, params)\n        loss.backward()\n        optimizer.step()\n        # Print every 10 epochs\n        if epoch % 50 == 0:\n            print(f\"Epoch {epoch}: loss {loss.item():.3f}\")\n    return loss.item()\n\n\nhomoskedastic_model_fixed_noise = MeanEstimateNN().to(device)\nparams = {\n    \"nn_params\": list(homoskedastic_model_fixed_noise.parameters()),\n    \"log_noise_std\": fixed_log_noise_std,\n}\n\n\ntrain_fn(\n    homoskedastic_model_fixed_noise,\n    loss_homoskedastic_noise,\n    params,\n    x_lin[:, None],\n    y,\n    n_epochs=1000,\n    lr=0.001,\n)\n\nEpoch 0: loss 0.369\nEpoch 50: loss 0.367\nEpoch 100: loss 0.362\nEpoch 150: loss 0.352\nEpoch 200: loss 0.338\nEpoch 250: loss 0.324\nEpoch 300: loss 0.312\nEpoch 350: loss 0.307\nEpoch 400: loss 0.305\nEpoch 450: loss 0.304\nEpoch 500: loss 0.304\nEpoch 550: loss 0.303\nEpoch 600: loss 0.303\nEpoch 650: loss 0.303\nEpoch 700: loss 0.303\nEpoch 750: loss 0.303\nEpoch 800: loss 0.302\nEpoch 850: loss 0.302\nEpoch 900: loss 0.302\nEpoch 950: loss 0.302\n\n\n0.3021060824394226\n\n\n\nwith torch.no_grad():\n    y_hat = homoskedastic_model_fixed_noise(x_lin[:, None]).squeeze()\n    aleatoric_std = torch.exp(fixed_log_noise_std)\n\nplot_results(y_hat, aleatoric_std=aleatoric_std, model_name=\"Trained model\")\n\n\n\n\n\n\nCase 1.1.2: Homoskedastic noise is learnt from the data\n\nThe model is the same as in case 1.1.1, but the noise is learned from the data.\n\nhomoskedastic_model_learnable_noise = MeanEstimateNN().to(device)\n# We keep the noise_std as a parameter, instead of a fixed value\n# Earlier code was: fixed_log_noise_std = torch.log(torch.tensor(0.5)).to(device)\nlog_noise_std = torch.nn.Parameter(torch.tensor(0.0).to(device))\n\nhomoskedastic_model_learnable_noise\n\nMeanEstimateNN(\n  (fc1): Linear(in_features=1, out_features=4, bias=True)\n  (fc2): Linear(in_features=4, out_features=4, bias=True)\n  (fc3): Linear(in_features=4, out_features=1, bias=True)\n)\n\n\n\n# Plot the untrained model\nwith torch.no_grad():\n    y_hat = homoskedastic_model_learnable_noise(x_lin[:, None]).squeeze()\n    aleatoric_std = torch.exp(log_noise_std)\n\nplot_results(y_hat, aleatoric_std=aleatoric_std, model_name=\"Untrained model\")\n\n\n\n\n\n# Train the model\nhomoskedastic_model_learnable_noise = MeanEstimateNN().to(device)\nparams = {\n    \"nn_params\": list(homoskedastic_model_learnable_noise.parameters()),\n    \"log_noise_std\": log_noise_std,\n}\n\n\njtu.tree_leaves(params) \n\n[Parameter containing:\n tensor(0., device='cuda:0', requires_grad=True),\n Parameter containing:\n tensor([[-0.7584],\n         [-0.9339],\n         [ 0.0176],\n         [ 0.9118]], device='cuda:0', requires_grad=True),\n Parameter containing:\n tensor([ 0.5769, -0.5822, -0.1298, -0.7372], device='cuda:0',\n        requires_grad=True),\n Parameter containing:\n tensor([[-0.2412,  0.0905,  0.2723,  0.4142],\n         [-0.4591,  0.3343, -0.3526,  0.1872],\n         [ 0.4231,  0.0070,  0.4549, -0.4260],\n         [-0.1910,  0.2916, -0.1089, -0.1024]], device='cuda:0',\n        requires_grad=True),\n Parameter containing:\n tensor([-0.2084,  0.3447,  0.2453,  0.1602], device='cuda:0',\n        requires_grad=True),\n Parameter containing:\n tensor([[-0.2810, -0.4059,  0.0541,  0.1481]], device='cuda:0',\n        requires_grad=True),\n Parameter containing:\n tensor([-0.2309], device='cuda:0', requires_grad=True)]\n\n\n\n[log_noise_std] + list(homoskedastic_model_learnable_noise.parameters()) \n\n[Parameter containing:\n tensor(0., device='cuda:0', requires_grad=True),\n Parameter containing:\n tensor([[-0.7584],\n         [-0.9339],\n         [ 0.0176],\n         [ 0.9118]], device='cuda:0', requires_grad=True),\n Parameter containing:\n tensor([ 0.5769, -0.5822, -0.1298, -0.7372], device='cuda:0',\n        requires_grad=True),\n Parameter containing:\n tensor([[-0.2412,  0.0905,  0.2723,  0.4142],\n         [-0.4591,  0.3343, -0.3526,  0.1872],\n         [ 0.4231,  0.0070,  0.4549, -0.4260],\n         [-0.1910,  0.2916, -0.1089, -0.1024]], device='cuda:0',\n        requires_grad=True),\n Parameter containing:\n tensor([-0.2084,  0.3447,  0.2453,  0.1602], device='cuda:0',\n        requires_grad=True),\n Parameter containing:\n tensor([[-0.2810, -0.4059,  0.0541,  0.1481]], device='cuda:0',\n        requires_grad=True),\n Parameter containing:\n tensor([-0.2309], device='cuda:0', requires_grad=True)]\n\n\n\njtu.tree_leaves(params) == [log_noise_std] + list(homoskedastic_model_learnable_noise.parameters()) \n\nTrue\n\n\n\n\ntrain_fn(\n    homoskedastic_model_learnable_noise,\n    loss_homoskedastic_noise,\n    params,\n    x_lin[:, None],\n    y,\n    n_epochs=1000,\n    lr=0.01,\n)\n\nEpoch 0: loss 0.948\nEpoch 50: loss 0.481\nEpoch 100: loss 0.102\nEpoch 150: loss -0.108\nEpoch 200: loss -0.156\nEpoch 250: loss -0.158\nEpoch 300: loss -0.158\nEpoch 350: loss -0.158\nEpoch 400: loss -0.158\nEpoch 450: loss -0.158\nEpoch 500: loss -0.158\nEpoch 550: loss -0.158\nEpoch 600: loss -0.158\nEpoch 650: loss -0.158\nEpoch 700: loss -0.158\nEpoch 750: loss -0.158\nEpoch 800: loss -0.158\nEpoch 850: loss -0.158\nEpoch 900: loss -0.158\nEpoch 950: loss -0.158\n\n\n-0.15807949006557465\n\n\n\n# Plot the trained model\nwith torch.no_grad():\n    y_hat = homoskedastic_model_learnable_noise(x_lin[:, None]).squeeze()\n    aleatoric_std = torch.exp(log_noise_std)\n\nplot_results(y_hat, aleatoric_std=aleatoric_std, model_name=\"Trained model\")\n\n\n\n\n\n\n\nCase 1.2: Models assuming heteroskedastic noise\n\n\nclass HeteroskedasticNN(torch.nn.Module):\n    def __init__(self, n_hidden=10):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(1, n_hidden)\n        self.fc2 = torch.nn.Linear(n_hidden, n_hidden)\n        self.fc3 = torch.nn.Linear(n_hidden, 2)  # we learn both mu and log_noise_std\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = torch.relu(x)\n        x = self.fc2(x)\n        x = torch.relu(x)\n        z = self.fc3(x)\n        mu_hat = z[:, 0]\n        log_noise_std = z[:, 1]\n        return mu_hat, log_noise_std\n\n\nheteroskedastic_model = HeteroskedasticNN().to(device)\nheteroskedastic_model\n\nHeteroskedasticNN(\n  (fc1): Linear(in_features=1, out_features=10, bias=True)\n  (fc2): Linear(in_features=10, out_features=10, bias=True)\n  (fc3): Linear(in_features=10, out_features=2, bias=True)\n)\n\n\n\n# Plot the untrained model\nwith torch.no_grad():\n    y_hat, log_noise_std = heteroskedastic_model(x_lin[:, None])\n    aleatoric_std = torch.exp(log_noise_std)\n\nplot_results(y_hat, aleatoric_std=aleatoric_std, model_name=\"Untrained model\")\n\n\n\n\n\ndef loss_heteroskedastic(model, x, y, params):\n    mu_hat, log_noise_std = model(x)\n    noise_std = torch.exp(log_noise_std)\n    dist = torch.distributions.Normal(mu_hat, noise_std)\n    return -dist.log_prob(y).mean()\n\n\nparams = list(heteroskedastic_model.parameters())\ntrain_fn(\n    heteroskedastic_model,\n    loss_heteroskedastic,\n    params,\n    x_lin[:, None],\n    y,\n    n_epochs=1000,\n    lr=0.01,\n)\n\nEpoch 0: loss 0.702\nEpoch 50: loss -0.130\nEpoch 100: loss -0.230\nEpoch 150: loss -0.250\nEpoch 200: loss -0.256\nEpoch 250: loss -0.260\nEpoch 300: loss -0.262\nEpoch 350: loss -0.264\nEpoch 400: loss -0.266\nEpoch 450: loss -0.266\nEpoch 500: loss -0.267\nEpoch 550: loss -0.267\nEpoch 600: loss -0.267\nEpoch 650: loss -0.267\nEpoch 700: loss -0.267\nEpoch 750: loss -0.267\nEpoch 800: loss -0.267\nEpoch 850: loss -0.267\nEpoch 900: loss -0.267\nEpoch 950: loss -0.267\n\n\n-0.2661149203777313\n\n\n\n# Plot the trained model\nwith torch.no_grad():\n    y_hat, log_noise_std = heteroskedastic_model(x_lin[:, None])\n    aleatoric_std = torch.exp(log_noise_std)\n\nplot_results(y_hat, aleatoric_std=aleatoric_std, model_name=\"Trained model\")\n\n\n\n\n\n\nData with heteroskedastic noise\n\ntorch.manual_seed(42)\nN = 100\nx_lin = torch.linspace(-1, 1, N)\n\nf = lambda x: 0.5 * x**2 + 0.25 * x**3\n\neps = torch.randn(N) * (0.1 + 0.4 * x_lin)\n\ny = f(x_lin) + eps\n\n# Move to GPU\nx_lin = x_lin.to(device)\ny = y.to(device)\n\n# Plot data and true function\nplt.plot(x_lin.cpu(), y.cpu(), \"o\", label=\"data\")\nplt.plot(x_lin.cpu(), f(x_lin).cpu(), label=\"true function\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f5ea8af9df0&gt;\n\n\n\n\n\n\nhomoskedastic_model_fixed_noise = MeanEstimateNN().to(device)\nfixed_log_noise_std = torch.log(torch.tensor(0.5)).to(device)\n\n# Plot the untrained model\nwith torch.no_grad():\n    y_hat = homoskedastic_model_fixed_noise(x_lin[:, None]).squeeze()\n    aleatoric_std = torch.exp(fixed_log_noise_std)\n\nplot_results(y_hat, aleatoric_std=aleatoric_std, model_name=\"Untrained model\\n (Homosdedastic fixed noise)\")\n\n\n\n\n\nparams = {\n    \"nn_params\": list(homoskedastic_model_fixed_noise.parameters()),\n    \"log_noise_std\": fixed_log_noise_std,\n}\ntrain_fn(\n    homoskedastic_model_fixed_noise,\n    loss_homoskedastic_noise,\n    params,\n    x_lin[:, None],\n    y,\n    n_epochs=1000,\n    lr=0.001,\n)\n\n# Plot the trained model\nwith torch.no_grad():\n    y_hat = homoskedastic_model_fixed_noise(x_lin[:, None]).squeeze()\n    aleatoric_std = torch.exp(fixed_log_noise_std)\n\nplot_results(y_hat, aleatoric_std=aleatoric_std, model_name=\"Trained model\\n (Homosdedastic fixed noise)\")\n\nEpoch 0: loss 0.668\nEpoch 50: loss 0.543\nEpoch 100: loss 0.466\nEpoch 150: loss 0.423\nEpoch 200: loss 0.401\nEpoch 250: loss 0.393\nEpoch 300: loss 0.389\nEpoch 350: loss 0.385\nEpoch 400: loss 0.382\nEpoch 450: loss 0.379\nEpoch 500: loss 0.376\nEpoch 550: loss 0.372\nEpoch 600: loss 0.369\nEpoch 650: loss 0.366\nEpoch 700: loss 0.362\nEpoch 750: loss 0.358\nEpoch 800: loss 0.354\nEpoch 850: loss 0.350\nEpoch 900: loss 0.347\nEpoch 950: loss 0.344\n\n\n\n\n\n\n# Now, fit the homoskedastic model with learned noise\n\nhomoskedastic_model_learnable_noise = MeanEstimateNN().to(device)\nlog_noise_std = torch.nn.Parameter(torch.tensor(0.0).to(device))\n\n# Plot the untrained model\nwith torch.no_grad():\n    y_hat = homoskedastic_model_learnable_noise(x_lin[:, None]).squeeze()\n    aleatoric_std = torch.exp(log_noise_std)\n\nplot_results(y_hat, aleatoric_std=aleatoric_std, model_name=\"Untrained model\\n (Homosdedastic learnable noise)\")\n\n\n\n\n\n# Train the model\nparams = {\n    \"nn_params\": list(homoskedastic_model_learnable_noise.parameters()),\n    \"log_noise_std\": log_noise_std,\n}\n\ntrain_fn(\n    homoskedastic_model_learnable_noise,\n    loss_homoskedastic_noise,\n    params,\n    x_lin[:, None],\n    y,\n    n_epochs=1000,\n    lr=0.01,\n)\n\n# Plot the trained model\nwith torch.no_grad():\n    y_hat = homoskedastic_model_learnable_noise(x_lin[:, None]).squeeze()\n    aleatoric_std = torch.exp(log_noise_std)\n\nplot_results(y_hat, aleatoric_std=aleatoric_std, model_name=\"Trained model\\n (Homosdedastic fixed noise)\")\n\nEpoch 0: loss 0.971\nEpoch 50: loss 0.523\nEpoch 100: loss 0.143\nEpoch 150: loss -0.036\nEpoch 200: loss -0.063\nEpoch 250: loss -0.063\nEpoch 300: loss -0.063\nEpoch 350: loss -0.063\nEpoch 400: loss -0.063\nEpoch 450: loss -0.063\nEpoch 500: loss -0.063\nEpoch 550: loss -0.063\nEpoch 600: loss -0.063\nEpoch 650: loss -0.063\nEpoch 700: loss -0.063\nEpoch 750: loss -0.063\nEpoch 800: loss -0.063\nEpoch 850: loss -0.063\nEpoch 900: loss -0.063\nEpoch 950: loss -0.063\n\n\n\n\n\n\n# Now, fit the heteroskedastic model\n\nheteroskedastic_model = HeteroskedasticNN().to(device)\n\n# Plot the untrained model\nwith torch.no_grad():\n    y_hat, log_noise_std = heteroskedastic_model(x_lin[:, None])\n    aleatoric_std = torch.exp(log_noise_std)\n\nplot_results(y_hat, aleatoric_std=aleatoric_std, model_name=\"Untrained model\\n (Heteroskedastic)\")\n\n\n\n\n\n# Train the model\nparams = list(heteroskedastic_model.parameters())\n\ntrain_fn(\n    heteroskedastic_model,\n    loss_heteroskedastic,\n    params,\n    x_lin[:, None],\n    y,\n    n_epochs=1000,\n    lr=0.01,\n)\n\nEpoch 0: loss 0.849\nEpoch 50: loss 0.023\nEpoch 100: loss -0.083\nEpoch 150: loss -0.334\nEpoch 200: loss -0.446\nEpoch 250: loss -0.476\nEpoch 300: loss -0.488\nEpoch 350: loss -0.508\nEpoch 400: loss -0.536\nEpoch 450: loss -0.547\nEpoch 500: loss -0.558\nEpoch 550: loss -0.565\nEpoch 600: loss -0.570\nEpoch 650: loss -0.569\nEpoch 700: loss -0.574\nEpoch 750: loss -0.577\nEpoch 800: loss -0.586\nEpoch 850: loss -0.589\nEpoch 900: loss -0.597\nEpoch 950: loss -0.602\n\n\n-0.6076287031173706\n\n\n\n# Plot the trained model\nwith torch.no_grad():\n    y_hat, log_noise_std = heteroskedastic_model(x_lin[:, None])\n    aleatoric_std = torch.exp(log_noise_std)\n\nplot_results(y_hat, aleatoric_std=aleatoric_std, model_name=\"Trained model\\n (Heteroskedastic)\")\n\n\n\n\n\n\nEpistemic Uncertainty: Bayesian NN with Laplace approximation\n\nMAP estimation\n\ndef negative_log_prior(model):\n    log_prior = 0.0\n\n    for param in model.parameters():\n        log_prior += torch.distributions.Normal(0, 1).log_prob(param).sum()\n    return -log_prior\n\n\ndef negative_log_likelihood(model, x, y, log_noise_std):\n    mu_hat = model(x).squeeze()\n    assert mu_hat.shape == y.shape\n    noise_std = torch.exp(log_noise_std).expand_as(mu_hat)\n    dist = torch.distributions.Normal(mu_hat, noise_std)\n    return -dist.log_prob(y).sum()\n\n\ndef negative_log_joint(model, x, y, log_noise_std):\n    return negative_log_likelihood(model, x, y, log_noise_std) + negative_log_prior(\n        model\n    )\n\n\ndef custom_loss_fn(model, x, y, params):\n    log_noise_std = params[\"log_noise_std\"]\n    return negative_log_joint(model, x, y, log_noise_std)\n\n\ntorch.manual_seed(3)\nlaplace_model = MeanEstimateNN().to(device)\nfixed_log_noise_std = torch.log(torch.tensor(0.2)).to(device)\nparams = {\n    \"log_noise_std\": fixed_log_noise_std,\n    \"nn_params\": list(laplace_model.parameters()),\n}\n\ntrain_fn(\n    laplace_model,\n    custom_loss_fn,\n    params,\n    x_lin[:, None],\n    y,\n    n_epochs=1000,\n    lr=0.01,\n)\n\nEpoch 0: loss 439.902\nEpoch 50: loss 138.376\nEpoch 100: loss 131.914\nEpoch 150: loss 131.173\nEpoch 200: loss 130.548\nEpoch 250: loss 126.990\nEpoch 300: loss 125.752\nEpoch 350: loss 125.163\nEpoch 400: loss 123.350\nEpoch 450: loss 122.710\nEpoch 500: loss 122.472\nEpoch 550: loss 122.251\nEpoch 600: loss 122.128\nEpoch 650: loss 122.113\nEpoch 700: loss 121.922\nEpoch 750: loss 121.868\nEpoch 800: loss 121.792\nEpoch 850: loss 121.794\nEpoch 900: loss 121.696\nEpoch 950: loss 121.726\n\n\n121.6390151977539\n\n\n\nwith torch.no_grad():\n    y_hat = laplace_model(x_lin[:, None]).squeeze()\n    aleatoric_std = torch.exp(fixed_log_noise_std)\n\nplot_results(y_hat, aleatoric_std=aleatoric_std, model_name=\"MAP estimate\\n (Homosdedastic fixed noise)\")\n\n\n\n\n\n\n\nWhat weighs to consider?\n\n\n\nGoal: Compute the Hessian of the negative log joint wrt the last layer weights\n\n\nChallenge: The negative log joint is a function of all the weights, not just the last layer weights\n\n\nAside on functools.partial\n\nThe functools module is for higher-order functions: functions that act on or return other functions. In general, any callable object can be treated as a function for the purposes of this module.\n\n\nprint(int(\"1001\", base=2), int(\"1001\", base=4), int(\"1001\"))\n\nfrom functools import partial\n\nbase_two = partial(int, base=2)\nbase_two.__doc__ = \"Convert base 2 string to an int.\"\n\nprint(base_two)\nprint(base_two.__doc__)\nprint(help(base_two))\nprint(base_two(\"1001\"))\n\n9 65 1001\nfunctools.partial(&lt;class 'int'&gt;, base=2)\nConvert base 2 string to an int.\nHelp on partial:\n\nfunctools.partial(&lt;class 'int'&gt;, base=2)\n    Convert base 2 string to an int.\n\nNone\n9\n\n\n\n\nA primer on functional calls to PyTorch\n\ntiny_model = torch.nn.Linear(3, 1)\ninput = torch.randn(2, 3)\ntarget = torch.randn(2, 1)\ntiny_model\n\nLinear(in_features=3, out_features=1, bias=True)\n\n\n\noutput = tiny_model(input)\noutput\n\ntensor([[ 0.0756],\n        [-0.2420]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\nloss_fn = torch.nn.MSELoss()\nloss = loss_fn(output, target)\nloss.backward()\n\ngrad_dict = {\"weight\": tiny_model.weight.grad, \"bias\": tiny_model.bias.grad}\ngrad_dict\n\n{'weight': tensor([[ 2.2469, -1.7073, -0.6623]]), 'bias': tensor([-1.9448])}\n\n\n\nparams = dict(tiny_model.named_parameters())\noutput = torch.func.functional_call(tiny_model, params, input)\noutput\n\ntensor([[ 0.0756],\n        [-0.2420]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\ndef custom_loss_fn(params):\n    output = torch.func.functional_call(tiny_model, params, input)\n    return loss_fn(output, target)\n\n\ntorch.func.grad(custom_loss_fn, argnums=0)(params)\n\n{'weight': tensor([[ 2.2469, -1.7073, -0.6623]], grad_fn=&lt;TBackward0&gt;),\n 'bias': tensor([-1.9448], grad_fn=&lt;ViewBackward0&gt;)}\n\n\nIt is also possible to get the gradients/hessian with respect to only a few weights.\n\ndef custom_loss_fn(partial_params, params):\n    params.update(partial_params)\n    output = torch.func.functional_call(tiny_model, params, input)\n    return loss_fn(output, target)\n\n\npartial_params = {\"bias\": params[\"bias\"]}\ntorch.func.grad(custom_loss_fn, argnums=0)(partial_params, params)\n\n{'bias': tensor([-1.9448], grad_fn=&lt;ViewBackward0&gt;)}\n\n\n\n\n\nLast layer Laplace approximation\n\ndef functional_negative_log_prior(partial_params):\n    partial_parameter_leaves = jtu.tree_leaves(partial_params)\n\n    log_prior = 0.0\n    for param in partial_parameter_leaves:\n        log_prior += torch.distributions.Normal(0, 1).log_prob(param).sum()\n    return -log_prior\n\n\ndef functional_negative_log_likelihood(\n    partial_params, params, model, x, y, log_noise_std\n):\n    params.update(partial_params)\n\n    mu_hat = torch.func.functional_call(model, params, x).squeeze()\n    assert mu_hat.shape == y.shape, f\"{mu_hat.shape} != {y.shape}\"\n\n    noise_std = torch.exp(log_noise_std).expand_as(mu_hat)\n    dist = torch.distributions.Normal(mu_hat, noise_std)\n    return -dist.log_prob(y).sum()\n\n\ndef functional_negative_log_joint(partial_params, params, model, x, y, log_noise_std):\n    return functional_negative_log_likelihood(\n        partial_params, params, model, x, y, log_noise_std\n    ) + functional_negative_log_prior(partial_params)\n\n\nparams = dict(laplace_model.named_parameters())\nparams.keys()\n\ndict_keys(['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n\n\n\npartial_params = {\"fc3.weight\": params[\"fc3.weight\"]}\npartial_params\n\n{'fc3.weight': Parameter containing:\n tensor([[ 1.3786e-12,  3.2083e-01,  8.6144e-01,  2.3610e-03, -1.3280e+00,\n           1.9577e-01,  8.8147e-01,  2.7229e-01, -4.8347e-05,  2.0937e-01]],\n        device='cuda:0', requires_grad=True)}\n\n\n\njtu.tree_leaves(partial_params)\n\n[Parameter containing:\n tensor([[ 1.3786e-12,  3.2083e-01,  8.6144e-01,  2.3610e-03, -1.3280e+00,\n           1.9577e-01,  8.8147e-01,  2.7229e-01, -4.8347e-05,  2.0937e-01]],\n        device='cuda:0', requires_grad=True)]\n\n\n\nprint(\"Full negative log prior\", negative_log_prior(laplace_model))\nprint(\n    \"Partial negative log prior\",\n    functional_negative_log_prior(partial_params),\n)\n\nFull negative log prior tensor(135.0255, device='cuda:0', grad_fn=&lt;NegBackward0&gt;)\nPartial negative log prior tensor(10.9603, device='cuda:0', grad_fn=&lt;NegBackward0&gt;)\n\n\n\nprint(\n    \"Full negative log likelihood\",\n    negative_log_likelihood(laplace_model, x_lin[:, None], y, fixed_log_noise_std),\n)\nprint(\n    \"Partial negative log likelihood\",\n    functional_negative_log_likelihood(\n        partial_params, params, laplace_model, x_lin[:, None], y, fixed_log_noise_std\n    ),\n)\n\nFull negative log likelihood tensor(-13.3980, device='cuda:0', grad_fn=&lt;NegBackward0&gt;)\nPartial negative log likelihood tensor(-13.3980, device='cuda:0', grad_fn=&lt;NegBackward0&gt;)\n\n\n\nprint(\n    \"Full negative log joint\",\n    negative_log_joint(laplace_model, x_lin[:, None], y, fixed_log_noise_std),\n)\nprint(\n    \"Partial negative log joint\",\n    functional_negative_log_joint(\n        partial_params, params, laplace_model, x_lin[:, None], y, fixed_log_noise_std\n    ),\n)\n\nFull negative log joint tensor(121.6275, device='cuda:0', grad_fn=&lt;AddBackward0&gt;)\nPartial negative log joint tensor(-2.4377, device='cuda:0', grad_fn=&lt;AddBackward0&gt;)\n\n\n\nmap_params = dict(laplace_model.named_parameters())\nlast_layer_params = {\"fc3.weight\": map_params[\"fc3.weight\"]}\n\npartial_func = partial(\n    functional_negative_log_joint,\n    params=map_params,\n    model=laplace_model,\n    x=x_lin[:, None],\n    y=y,\n    log_noise_std=fixed_log_noise_std,\n)\n\nH = torch.func.hessian(partial_func)(last_layer_params)[\"fc3.weight\"][\"fc3.weight\"]\nprint(H.shape)\n\nH = H[0, :, 0, :]\nprint(H.shape)\n\ntorch.Size([1, 10, 1, 10])\ntorch.Size([10, 10])\n\n\n\nimport seaborn as sns\n\nwith torch.no_grad():\n    cov = torch.inverse(H + 1e-3 * torch.eye(H.shape[0]).to(device))\n\nplt.figure(figsize=(7, 7))\nsns.heatmap(cov.cpu().numpy(), annot=True, fmt=\".2f\", cmap=\"viridis\")\nplt.gca().set_aspect(\"equal\", \"box\")\n\n\n\n\n\nlaplace_posterior = torch.distributions.MultivariateNormal(\n    last_layer_params[\"fc3.weight\"].ravel(), cov\n)\nlast_layer_weights_samples = laplace_posterior.sample((501,))[..., None]\nlast_layer_weights_samples.shape\n\ntorch.Size([501, 10, 1])\n\n\n\ndef forward_pass(last_layer_weight, params):\n    params.update({\"fc3.weight\": last_layer_weight.reshape(1, -1)})\n    return torch.func.functional_call(laplace_model, params, x_lin[:, None]).squeeze()\n\n\nforward_pass(last_layer_weights_samples[0], params).shape\n\ntorch.Size([100])\n\n\n\nmc_outputs = torch.vmap(lambda x: forward_pass(x, params))(last_layer_weights_samples)\nprint(mc_outputs.shape)\n\ntorch.Size([501, 100])\n\n\n\nmean_mc_outputs = mc_outputs.mean(0)\nstd_mc_outputs = mc_outputs.std(0)\nmean_mc_outputs.shape, std_mc_outputs.shape\n\n(torch.Size([100]), torch.Size([100]))\n\n\n\nwith torch.no_grad():\n    epistemic_std = std_mc_outputs\n    aleatoric_std = torch.exp(fixed_log_noise_std) + epistemic_std\n\n    plot_results(\n        mean_mc_outputs,\n        epistemic_std=epistemic_std,\n        aleatoric_std=aleatoric_std,\n        model_name=\"Laplace approximation\",\n    )\n\n\n\n\n\nwith torch.no_grad():\n    epistemic_std = std_mc_outputs\n    aleatoric_std = torch.exp(fixed_log_noise_std) + epistemic_std\n\n    plot_results(\n        mean_mc_outputs,\n        epistemic_std=None,\n        aleatoric_std=None,\n        model_name=\"Laplace approximation\",\n    )\n\n    for i in range(10):\n        plt.plot(\n            x_lin.cpu(),\n            mc_outputs[i].cpu(),\n            alpha=0.3,\n            color=\"C2\",\n            label=\"Laplace approximation\",\n        )"
  },
  {
    "objectID": "notebooks/mle-univariate.html",
    "href": "notebooks/mle-univariate.html",
    "title": "Probabilistic Machine Learning",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\n\nc1 = torch.distributions.Normal(80, 10)\nc2 = torch.distributions.Normal(70, 10)\nc3 = torch.distributions.Normal(90, 5)\n\n\n# Plot the distributions\nx = torch.linspace(0, 105, 1000)\nplt.plot(x, c1.log_prob(x).exp(), label='C1')\nplt.plot(x, c2.log_prob(x).exp(), label='C2')\nplt.plot(x, c3.log_prob(x).exp(), label='C3')\n# Fill the area under the curve\nplt.fill_between(x, c1.log_prob(x).exp(), alpha=0.2)\nplt.fill_between(x, c2.log_prob(x).exp(), alpha=0.2)\nplt.fill_between(x, c3.log_prob(x).exp(), alpha=0.2)\n\nplt.xlabel('Marks')\nplt.ylabel('Probability')\n\nplt.legend(title='Class')\nplt.savefig('../figures/mle/mle-example.pdf', bbox_inches='tight')\n\n# Vertical line at x = 85\nmarks = torch.tensor([82.])\nplt.axvline(marks.item(), color='k', linestyle='--', lw=2)\n# Draw horizontal line to show the probability at x = 85\nplt.hlines(c1.log_prob(marks).exp(), 0, marks.item(), color='C0', linestyle='--', lw=2)\nplt.hlines(c2.log_prob(marks).exp(), 0, marks.item(), color='C1', linestyle='--', lw=2)\nplt.hlines(c3.log_prob(marks).exp(), 0, marks.item(), color='C2', linestyle='--', lw=2)\nplt.savefig('../figures/mle/mle-example-2.pdf', bbox_inches='tight')\n\n\n\n\n\nobs = torch.tensor([20.0])\nsigma = torch.tensor([1.0])\n\n# Plot the likelihood\nmus = torch.linspace(0, 40, 1000)\nplt.plot(mus, torch.distributions.Normal(mus, sigma).log_prob(obs).exp())\nplt.xlabel(r'Parameter ($\\mu$)')\nplt.ylabel(r'Likelihood $p(x = 20|\\mu$)')\n\nText(0, 0.5, 'Likelihood $p(x = 20|\\\\mu$)')\n\n\nfindfont: Font family ['cursive'] not found. Falling back to DejaVu Sans.\nfindfont: Generic family 'cursive' not found because none of the following families were found: Apple Chancery, Textile, Zapf Chancery, Sand, Script MT, Felipa, Comic Neue, Comic Sans MS, cursive\n\n\n\n\n\n\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets\n\n\n# Interactive plot showing fitting normal distribution of varying mu to one data point\n\ndef plot_norm(mu):\n    mu = torch.tensor(mu)\n    sigma = torch.tensor(1.0)\n    x = torch.tensor(20.0)\n    n = torch.distributions.Normal(mu, sigma)\n    x_lin = torch.linspace(0, 40, 500)\n    y_lin = n.log_prob(x_lin).exp()\n    likelihood = n.log_prob(x).exp()\n    plt.plot(x_lin, y_lin, label=rf\"$\\mathcal{{N}}({mu.item():0.4f}, 1)$\")\n    plt.legend()\n    plt.title(f\"Likelihood={likelihood:.4f}\")\n    plt.ylim(0, 0.5)\n    plt.fill_between(x_lin, y_lin, alpha=0.2)\n    plt.axvline(x=x, color=\"black\", linestyle=\"--\")\n    plt.axhline(y=likelihood, color=\"black\", linestyle=\"--\")\n        \n#plot_norm(20)\ninteract(plot_norm, mu=(0, 30, 0.1))\n\n\n\n\n&lt;function __main__.plot_norm(mu)&gt;\n\n\n\n# Interactive plot showing fitting normal distribution of varying mu to one data point\n\ndef plot_norm_log(mu):\n    mu = torch.tensor(mu)\n    sigma = torch.tensor(1.0)\n    x = torch.tensor(20.0)\n    n = torch.distributions.Normal(mu, sigma)\n    x_lin = torch.linspace(0, 40, 500)\n    fig, ax = plt.subplots(nrows=2, sharex=True)\n    y_log_lin = n.log_prob(x_lin)\n    y_lin = y_log_lin.exp()\n    ll = n.log_prob(x)\n    likelihood = ll.exp()\n    ax[0].plot(x_lin, y_lin, label=rf\"$\\mathcal{{N}}({mu.item():0.4f}, 1)$\")\n    #plt.legend()\n    ax[0].set_title(f\"Likelihood={likelihood:.4f}\")\n    ax[0].set_ylim(0, 0.5)\n    ax[0].fill_between(x_lin, y_lin, alpha=0.2)\n\n    ax[1].plot(x_lin, y_log_lin, label=rf\"$\\mathcal{{N}}({mu.item():0.4f}, 1)$\")\n    ax[1].set_title(f\"Log Likelihood={ll:.4f}\")\n    ax[1].set_ylim(-500, 20)\n    \n    ax[0].axvline(x=x, color=\"black\", linestyle=\"--\")\n    ax[0].axhline(y=likelihood, color=\"black\", linestyle=\"--\")\n\n    ax[1].axvline(x=x, color=\"black\", linestyle=\"--\")\n    ax[1].axhline(y=ll, color=\"black\", linestyle=\"--\")\n        \n#plot_norm_log(10)\ninteract(plot_norm_log, mu=(0, 30, 0.1))\n\n\n\n\n&lt;function __main__.plot_norm_log(mu)&gt;\n\n\n\n# Plot the distributions\ndef plot_class(class_num):\n    x = torch.linspace(0, 105, 1000)\n    dist = [c1, c2, c3][class_num-1]\n    plt.plot(x, dist.log_prob(x).exp(), label=f'C{class_num}')\n    plt.fill_between(x, dist.log_prob(x).exp(), alpha=0.2)\n\n\n    plt.xlabel('Marks')\n    plt.ylabel('Probability')\n\n    #plt.legend(title='Class')\n    #plt.savefig('../figures/mle/mle-example.pdf', bbox_inches='tight')\n\n    # Vertical line at x = 82\n    marks = torch.tensor([82., 72.0])\n    for mark in marks:\n        plt.axvline(mark.item(), color='k', linestyle='--', lw=2)\n\n        plt.hlines(dist.log_prob(mark).exp(), 0, mark.item(), color='C0', linestyle='--', lw=2, label=f\"P({mark.item()}|Class ={dist.log_prob(mark).exp().item():0.4f}\")\n        #plt.hlines(c2.log_prob(mark).exp(), 0, mark.item(), color='C1', linestyle='--', lw=2)\n        #plt.hlines(c3.log_prob(mark).exp(), 0, mark.item(), color='C2', linestyle='--', lw=2)\n    #plt.savefig('../figures/mle/mle-example-2.pdf', bbox_inches='tight')\n    plt.legend()\n    #plt.savefig(\"..\")\n\nplot_class(1)\n\n\n\n\n\nplot_class(2)\n\n\n\n\n\nplot_class(3)\n\n\n\n\n\n#\ns1 = torch.tensor([82.0])\ns2 = torch.tensor([72.0])\n\np_s1_c1 = c1.log_prob(s1).exp()\np_s1_c2 = c2.log_prob(s1).exp()\np_s1_c3 = c3.log_prob(s1).exp()\n\np_s2_c1 = c1.log_prob(s2).exp()\np_s2_c2 = c2.log_prob(s2).exp()\np_s2_c3 = c3.log_prob(s2).exp()\n\n# Create dataframe\ndf = pd.DataFrame({\n    'Class': ['C1', 'C2', 'C3'],\n    'Student 1 (82)': [p_s1_c1.item(), p_s1_c2.item(), p_s1_c3.item()],\n    'Student 2 (72)': [p_s2_c1.item(), p_s2_c2.item(), p_s2_c3.item()]\n})\n\ndf = df.set_index('Class')\ndf\n\n\n\n\n\n\n\n\nStudent 1 (82)\nStudent 2 (72)\n\n\nClass\n\n\n\n\n\n\nC1\n0.039104\n0.028969\n\n\nC2\n0.019419\n0.039104\n\n\nC3\n0.022184\n0.000122\n\n\n\n\n\n\n\n\ndf.plot(kind='bar', rot=0)\n\n&lt;AxesSubplot:xlabel='Class'&gt;\n\n\n\n\n\n\n# Multiply the probabilities\ndf.aggregate('prod', axis=1)\n\nClass\nC1    0.001133\nC2    0.000759\nC3    0.000003\ndtype: float64\n\n\n\ndf.aggregate('prod', axis=1).plot(kind='bar', rot=0)\n\n&lt;AxesSubplot:xlabel='Class'&gt;\n\n\n\n\n\n\n# Create a slider to change s1 and s2 marks and plot the likelihood\n\n\ndef plot_likelihood(s1, s2, scale='log'):\n    s1 = torch.tensor([s1])\n    s2 = torch.tensor([s2])\n    p_s1_c1 = c1.log_prob(s1)\n    p_s1_c2 = c2.log_prob(s1)\n    p_s1_c3 = c3.log_prob(s1)\n\n    p_s2_c1 = c1.log_prob(s2)\n    p_s2_c2 = c2.log_prob(s2)\n    p_s2_c3 = c3.log_prob(s2)\n\n\n    # Create dataframe\n    df = pd.DataFrame({\n        'Class': ['C1', 'C2', 'C3'],\n        f'Student 1 ({s1.item()})': [p_s1_c1.item(), p_s1_c2.item(), p_s1_c3.item()],\n        f'Student 2 ({s2.item()})': [p_s2_c1.item(), p_s2_c2.item(), p_s2_c3.item()]\n    })\n    \n    \n\n    df = df.set_index('Class')\n    if scale!='log':\n        df  = df.apply(np.exp)\n    df.plot(kind='bar', rot=0)\n    plt.ylabel('Probability')\n    plt.xlabel('Class')\n\n    if scale=='log':\n        plt.ylabel('Log Probability')\n        #plt.yscale('log')\n\nplot_likelihood(80, 72, scale='linear')\n\n    \n\n\n\n\n\nplot_likelihood(80, 72, scale='log')\n\n\n\n\n\n# Interactive plot\ninteract(plot_likelihood, s1=(0, 100), s2=(0, 100), scale=['linear', 'log'])\n\n\n\n\n&lt;function __main__.plot_likelihood(s1, s2, scale='log')&gt;\n\n\n\n# Let us now consider some N points from a univariate Gaussian distribution with mean 0 and variance 1.\n\nN = 50\ntorch.manual_seed(2)\nsamples = torch.distributions.Normal(0, 1).sample((N,))\nsamples\nsamples.mean(), samples.std(correction=0)\n\n(tensor(-0.0089), tensor(1.0376))\n\n\n\nplt.scatter(samples, np.zeros_like(samples))\n\n&lt;matplotlib.collections.PathCollection at 0x7f93d6f98550&gt;\n\n\n\n\n\n\ndist = torch.distributions.Normal(0, 1)\ndist.log_prob(samples)\n\ntensor([-1.4606, -1.3390, -1.7694, -1.5346, -1.6616, -1.6006, -1.4780, -0.9260,\n        -1.3310, -0.9785, -1.0821, -0.9466, -1.4266, -1.2024, -0.9443, -1.0125,\n        -2.0547, -1.0241, -1.2785, -1.0576, -0.9194, -0.9202, -1.4861, -1.3115,\n        -1.0266, -1.0433, -0.9272, -4.7362, -0.9288, -1.5451, -0.9686, -2.4551,\n        -1.2115, -2.5932, -2.2046, -2.6290, -0.9199, -2.1755, -1.0937, -1.5588,\n        -1.3670, -2.4799, -1.0891, -1.0160, -2.8774, -1.2221, -1.2191, -0.9287,\n        -0.9790, -0.9227])\n\n\n\ndef ll(mu, sigma):\n    mu = torch.tensor(mu)\n    sigma = torch.tensor(sigma)\n\n    dist = torch.distributions.Normal(mu, sigma)\n    loglik = dist.log_prob(samples).sum()\n    return dist, loglik\n\ndef plot_normal(mu, sigma):\n    xs = torch.linspace(-5, 5, 100)\n    dist, loglik = ll(mu, sigma)\n    ys_log = dist.log_prob(xs)\n    plt.plot(xs, ys_log)\n\n    plt.scatter(samples, dist.log_prob(samples), color='C3', alpha=0.5)\n    plt.title(f'log likelihood: {loglik:.8f}')\n   \n\n\nplot_normal(0, 1)\n#plt.ylim(-1.7, -1.6)\n\n\n\n\n\ninteract(plot_normal, mu=(-3.0, 3.0), sigma=(0.1, 10))\n\n\n\n\n&lt;function __main__.plot_normal(mu, sigma)&gt;\n\n\n\ndef get_lls(mus, sigmas):\n\n    lls = torch.zeros((len(mus), len(sigmas)))\n    for i, mu in enumerate(mus):\n        for j, sigma in enumerate(sigmas):\n\n            lls[i, j] = ll(mu, sigma)[1]\n    return lls\n\nmus = torch.linspace(-1, 1, 100)\nsigmas = torch.linspace(0.1, 1.5, 100)\nlls = get_lls(mus, sigmas)\n\n/tmp/ipykernel_444491/3787141935.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  mu = torch.tensor(mu)\n/tmp/ipykernel_444491/3787141935.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  sigma = torch.tensor(sigma)\n\n\n\npd.DataFrame(lls.numpy(), index=mus.numpy(), columns=sigmas.numpy())\n\n\n\n\n\n\n\n\n0.100000\n0.114141\n0.128283\n0.142424\n0.156566\n0.170707\n0.184848\n0.198990\n0.213131\n0.227273\n...\n1.372727\n1.386869\n1.401010\n1.415151\n1.429293\n1.443434\n1.457576\n1.471717\n1.485859\n1.500000\n\n\n\n\n-1.000000\n-5077.877930\n-3888.119141\n-3070.949951\n-2485.914551\n-2052.976318\n-1723.822998\n-1467.891479\n-1265.083740\n-1101.745483\n-968.337524\n...\n-89.101242\n-89.059502\n-89.029251\n-89.009949\n-89.001060\n-89.002075\n-89.012505\n-89.031929\n-89.059898\n-89.096008\n\n\n-0.979798\n-4978.790039\n-3812.062744\n-3010.738281\n-2437.065918\n-2012.553467\n-1689.820068\n-1438.892090\n-1240.059692\n-1079.932007\n-949.154175\n...\n-88.575401\n-88.544327\n-88.524429\n-88.515175\n-88.516029\n-88.526489\n-88.546104\n-88.574448\n-88.611084\n-88.655617\n\n\n-0.959596\n-4881.743652\n-3737.573975\n-2951.766602\n-2389.223145\n-1972.963379\n-1656.517456\n-1410.490112\n-1215.551025\n-1058.567871\n-930.365845\n...\n-88.060394\n-88.039780\n-88.030014\n-88.030586\n-88.040977\n-88.060699\n-88.089317\n-88.126389\n-88.171516\n-88.224297\n\n\n-0.939394\n-4786.737305\n-3664.650146\n-2894.034424\n-2342.386963\n-1934.205566\n-1623.915161\n-1382.685181\n-1191.557739\n-1037.652710\n-911.972656\n...\n-87.556213\n-87.545830\n-87.545990\n-87.556183\n-87.575905\n-87.604698\n-87.642128\n-87.687759\n-87.741196\n-87.802048\n\n\n-0.919192\n-4693.770996\n-3593.292969\n-2837.542480\n-2296.556152\n-1896.280029\n-1592.012939\n-1355.477539\n-1168.079590\n-1017.187012\n-893.974426\n...\n-87.062874\n-87.062485\n-87.072350\n-87.091965\n-87.120834\n-87.158508\n-87.204544\n-87.258545\n-87.320107\n-87.388863\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n0.919192\n-4775.875488\n-3656.313477\n-2887.434814\n-2337.032471\n-1929.774536\n-1620.187744\n-1379.506714\n-1188.814819\n-1035.261719\n-909.869873\n...\n-87.498581\n-87.489357\n-87.490654\n-87.501953\n-87.522743\n-87.552574\n-87.591011\n-87.637611\n-87.691994\n-87.753777\n\n\n0.939394\n-4870.645508\n-3729.055664\n-2945.022949\n-2383.752197\n-1968.436035\n-1652.709229\n-1407.242188\n-1212.748413\n-1056.124878\n-928.217346\n...\n-88.001511\n-87.982071\n-87.973473\n-87.975166\n-87.986656\n-88.007431\n-88.037086\n-88.075157\n-88.121246\n-88.174980\n\n\n0.959596\n-4967.457520\n-3803.364014\n-3003.851562\n-2431.478760\n-2007.930176\n-1685.930908\n-1435.575439\n-1237.197510\n-1077.437134\n-946.959961\n...\n-88.515259\n-88.485413\n-88.466705\n-88.458580\n-88.460556\n-88.472092\n-88.492760\n-88.522125\n-88.559746\n-88.605247\n\n\n0.979798\n-5066.307617\n-3879.238281\n-3063.919678\n-2480.210938\n-2048.256348\n-1719.852661\n-1464.505371\n-1262.161865\n-1099.198608\n-966.097595\n...\n-89.039841\n-88.999352\n-88.970322\n-88.952187\n-88.944420\n-88.946533\n-88.958046\n-88.978508\n-89.007492\n-89.044586\n\n\n1.000000\n-5167.200195\n-3956.678955\n-3125.228271\n-2529.948730\n-2089.415527\n-1754.474854\n-1494.032837\n-1287.641602\n-1121.409302\n-985.630249\n...\n-89.575256\n-89.523895\n-89.484329\n-89.455971\n-89.438293\n-89.430779\n-89.432945\n-89.444313\n-89.464470\n-89.492989\n\n\n\n\n100 rows × 100 columns\n\n\n\n\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\ndef plot_lls(mus, sigmas, lls):\n    fig, ax1 = plt.subplots(figsize=(8, 6))\n    \n    X, Y = np.meshgrid(mus, sigmas)\n    \n    max_indices = np.unravel_index(np.argmax(lls), lls.shape)\n    max_mu = mus[max_indices[1]]\n    max_sigma = sigmas[max_indices[0]]\n    max_loglik = lls[max_indices]\n\n    # Define levels with increasing granularity\n    levels_low = np.linspace(lls.min(), max_loglik, 20)\n    levels_high = np.linspace(max_loglik + 0.001, lls.max(), 10)  # Adding a small value to prevent duplicates\n    levels = levels_low\n    \n    # Plot the contour filled plot\n    contour = ax1.contourf(X, Y, lls.T, levels=levels, cmap='magma')\n    \n    # Plot the contour lines\n    contour_lines = ax1.contour(X, Y, lls.T, levels=levels, colors='black', linewidths=0.5, alpha=0.6)\n    \n    # Add contour labels\n    ax1.clabel(contour_lines, inline=True, fontsize=10, colors='black', fmt='%1.2f')\n    \n    ax1.set_xlabel('Mu')\n    ax1.set_ylabel('Sigma')\n    ax1.set_title('Contour Plot of Log Likelihood')\n    \n    # Add maximum log likelihood point as scatter on the contour plot\n    ax1.scatter([max_mu], [max_sigma], color='red', marker='o', label='Maximum Log Likelihood')\n    ax1.annotate(f'Max LL: {max_loglik:.2f}', (max_mu, max_sigma), textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=10, color='red', bbox=dict(facecolor='white', alpha=0.8, edgecolor='none', boxstyle='round,pad=0.3'))\n\n    ax1.axvline(max_mu, color='red', linestyle='--', alpha=0.5)\n    ax1.axhline(max_sigma, color='red', linestyle='--', alpha=0.5)\n    \n    # Create colorbar outside the plot\n    divider = make_axes_locatable(ax1)\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n    cbar = plt.colorbar(contour, cax=cax)\n    cbar.set_label('Log Likelihood', rotation=270, labelpad=15)\n    \n    plt.tight_layout()\n    plt.show()\n\nplot_lls(mus, sigmas, lls)\n\n/tmp/ipykernel_444491/1128417763.py:44: UserWarning: This figure was using constrained_layout, but that is incompatible with subplots_adjust and/or tight_layout; disabling constrained_layout.\n  plt.tight_layout()\n\n\n\n\n\n\nsamples.mean(), samples.std(correction=0)\n\n(tensor(-0.0089), tensor(1.0376))\n\n\n\nmus = torch.linspace(-0.4, 0.4, 200)\nsigmas = torch.linspace(0.5, 1.0,200)\nlls = get_lls(mus, sigmas)\nplot_lls(mus, sigmas, lls)\n\n/tmp/ipykernel_444491/3787141935.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  mu = torch.tensor(mu)\n/tmp/ipykernel_444491/3787141935.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  sigma = torch.tensor(sigma)\n/tmp/ipykernel_444491/1128417763.py:44: UserWarning: This figure was using constrained_layout, but that is incompatible with subplots_adjust and/or tight_layout; disabling constrained_layout.\n  plt.tight_layout()"
  },
  {
    "objectID": "notebooks/log-likelihood-linreg.html",
    "href": "notebooks/log-likelihood-linreg.html",
    "title": "Probabilistic Machine Learning",
    "section": "",
    "text": "import torch\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\n\nx = torch.linspace(-1, 1, 100)\nf = lambda x: 3*x + 1 \n\nnoise = torch.distributions.Normal(0, 0.1).sample((100,))\ny = f(x) + noise\n\n\nplt.scatter(x, y, marker='x', c='k', s=20, label=\"Noisy data\")\nplt.plot(x, f(x), c='k', label=\"True function\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fc90a452970&gt;\n\n\n\n\n\n\ndef forward(x, theta):\n    return x*theta[1] + theta[0]\n\n\ndef nll(theta):\n    mu = forward(x, theta)\n    sigma = torch.tensor(1.0)\n    dist = torch.distributions.normal.Normal(mu, sigma)\n    return -dist.log_prob(y).sum()\n\n\nnll(torch.zeros(2)), nll(torch.tensor([1., 3.]))\n\n(tensor(297.3265), tensor(92.3937))\n\n\n\n# Create a grid of theta[0] and theta[1] values\ntheta0_values = torch.linspace(-4, 4, 100)\ntheta1_values = torch.linspace(-4, 4, 100)\ntheta0_mesh, theta1_mesh = torch.meshgrid(theta0_values, theta1_values)\nnll_values = torch.zeros_like(theta0_mesh)\n\n# Calculate negative log-likelihood values for each combination of theta[0] and theta[1]\nfor i in range(len(theta0_values)):\n    for j in range(len(theta1_values)):\n        nll_values[i, j] = nll([theta0_values[i], theta1_values[j]])\n\n# Create a contour plot\nplt.figure(figsize=(8, 6))\ncontour = plt.contourf(theta0_mesh, theta1_mesh, nll_values, levels=20, cmap='viridis')\nplt.colorbar(contour)\nplt.xlabel(r'$\\theta_0$')\nplt.ylabel(r'$\\theta_1$')\n#plt.title('Contour Plot of Negative Log-Likelihood')\n\n# Adding contour level labels\ncontour_labels = plt.contour(theta0_mesh, theta1_mesh, nll_values, levels=20, colors='black', linewidths=0.5)\nplt.clabel(contour_labels, inline=True, fontsize=8, fmt='%1.1f')\n\n# Find and mark the minimum\nmin_indices = torch.argmin(nll_values)\nmin_theta0 = theta0_mesh.flatten()[min_indices]\nmin_theta1 = theta1_mesh.flatten()[min_indices]\nplt.scatter(min_theta0, min_theta1, color='red', marker='x', label='Minima')\n\n# Draw lines from the minimum point to the axes\nplt.axhline(min_theta1, color='gray', linestyle='--')\nplt.axvline(min_theta0, color='gray', linestyle='--')\n\n# Add labels to the lines\nplt.text(min_theta0, 4.2, f'{min_theta0:.2f}', color='gray', fontsize=10)\nplt.text(-4.5, min_theta1, f'{min_theta1:.2f}', color='gray', fontsize=10)\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fc90a276d60&gt;\n\n\n\n\n\n\n\ndef plot_theta(theta):\n    plt.figure(figsize=(8, 4))\n\n    # Left-hand side plot (contour plot)\n    plt.subplot(1, 2, 1)\n    contour = plt.contourf(theta0_mesh, theta1_mesh, nll_values, levels=20, cmap='viridis')\n    # Colorbar\n    plt.colorbar(contour)\n    plt.scatter(theta[0], theta[1], color='red', marker='x')\n    plt.axhline(theta[1], color='gray', linestyle='--')\n    plt.axvline(theta[0], color='gray', linestyle='--')\n    plt.xlabel(r'$\\theta_0$')\n    plt.ylabel(r'$\\theta_1$')\n    plt.title('Contour Plot of Negative Log-Likelihood')\n\n    # Right-hand side plot (data fit)\n    plt.subplot(1, 2, 2)\n    plt.scatter(x, y, label='Data')\n    plt.plot(x, theta[0] + theta[1] * x, color='red', label='Fit')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title('Data Fit')\n    plt.legend()\n\n    #plt.tight_layout()\n    plt.show()\n\n# Generate a random theta vector\nrandom_theta = torch.randn(2)\nprint(random_theta)\n\nimport ipywidgets as widgets\nfrom ipywidgets import interact\n\n\n@interact(theta0=(-5, 5, 0.1), theta1=(-5, 5, 0.1))\ndef plot_interactive(theta0=random_theta[0], theta1=random_theta[1]):\n    theta = [theta0, theta1]\n    plot_theta(theta)\n\ntensor([-0.9771,  0.4177])\n\n\n\n\n\n\n## Logistic regression\n\nx = torch.linspace(-5, 5, 100)\n\ntrue_theta = torch.tensor([-10.0, 4.0])\np = torch.sigmoid(true_theta[0] + true_theta[1] * x)\n\nplt.plot(x, p, label='p(x)')\n\n\n\n\n\ny = torch.distributions.Bernoulli(probs=p).sample()\ny\n\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n        0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n\n\n\nplt.plot(x, y, 'o')\n\n\n\n\n\ndef nll_lr(theta):\n    p = torch.sigmoid(theta[0]+ theta[1]*x)\n    dist = torch.distributions.Bernoulli(probs=p)\n    return -dist.log_prob(y).sum()\n\ndef nll_lr_logits(theta):\n    logits = theta[0]+ theta[1]*x\n    dist = torch.distributions.Bernoulli(logits=logits)\n    return -dist.log_prob(y).sum()\n\n\nnll_lr(torch.tensor([1.0, 1.0])), nll_lr(true_theta), nll_lr_logits(torch.tensor([1.0, 1.0])), nll_lr_logits(true_theta), \n\n(tensor(78.3085), tensor(6.8952), tensor(78.3085), tensor(6.8952))\n\n\n\n# Create a grid of theta[0] and theta[1] values\ntheta0_values = torch.linspace(-15, 5, 100)\ntheta1_values = torch.linspace(-2, 8, 100)\ntheta0_mesh, theta1_mesh = torch.meshgrid(theta0_values, theta1_values)\nnll_values = torch.zeros_like(theta0_mesh)\n\n# Calculate negative log-likelihood values for each combination of theta[0] and theta[1]\nfor i in range(len(theta0_values)):\n    for j in range(len(theta1_values)):\n        nll_values[i, j] = nll_lr([theta0_values[i], theta1_values[j]])\n\n# Create a contour plot\nplt.figure(figsize=(8, 6))\ncontour = plt.contourf(theta0_mesh, theta1_mesh, nll_values, levels=20, cmap='viridis')\nplt.colorbar(contour)\nplt.scatter(true_theta[0], true_theta[1], color='red', marker='x', label='True Theta')\nplt.xlabel(r'$\\theta_0$')\nplt.ylabel(r'$\\theta_1$')\nplt.title('Contour Plot of Negative Log-Likelihood (Logistic Regression)')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fc90358c6a0&gt;\n\n\n\n\n\n\ndef decision_boundary(theta):\n    return - (theta[0] + theta[1] * x) / theta[1]\n\n# Create a given theta vector\ngiven_theta = torch.tensor([-10.0, 4.0])\n\n# Plot data and decision boundary for the given theta\nplt.figure(figsize=(8, 4))\n\n# Left-hand side plot (data and decision boundary)\nplt.subplot(1, 2, 1)\nplt.scatter(x, y, label='Data')\nplt.plot(x, decision_boundary(given_theta), color='red', label='Decision Boundary')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Logistic Regression: Data and Decision Boundary')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fc90c4838e0&gt;"
  },
  {
    "objectID": "notebooks/MAP.html",
    "href": "notebooks/MAP.html",
    "title": "Setting Up & Imports",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\nimport logging\nlogging.getLogger('matplotlib.font_manager').disabled = True\n\nfrom ipywidgets import interact, FloatSlider, IntSlider\n\n\nCoin Toss Problem\n\ndef plot_beta(alpha, beta):\n    dist = torch.distributions.Beta(concentration1=alpha, concentration0=beta)\n    xs = torch.linspace(0, 1, 500)\n    ys = dist.log_prob(xs).exp()\n    plt.plot(xs, ys, color=\"C0\")\n    plt.grid()\n    plt.xlabel(r\"$\\theta$\")\n    plt.ylabel(r\"$p(\\theta)$\")\n    plt.title(\"Beta Distribution\")\n    plt.show()\n\n\ninteract(\n    plot_beta,\n    alpha=FloatSlider(min=1, max=11, step=0.5, value=1),\n    beta=FloatSlider(min=1, max=11, step=0.5, value=1),\n)\n\n\n\n\n&lt;function __main__.plot_beta(alpha, beta)&gt;\n\n\n\ncombinations = [[1, 1], [5, 1], [1, 3], [2, 2], [2, 5]]\nfig, ax = plt.subplots(1, 1)\nfor alpha, beta in combinations:\n    dist = torch.distributions.Beta(concentration1=alpha, concentration0=beta)\n    xs = torch.linspace(0, 1, 500)\n    ys = dist.log_prob(xs).exp()\n    ax.plot(xs, ys, label=rf\"($\\alpha$={alpha}, $\\beta$={beta})\")\nax.legend()\nax.grid()\nax.set_xlabel(r\"$\\theta$\")\nax.set_ylabel(r\"$p(\\theta)$\")\nax.set_title(\"Beta Distribution\")\nax.set_ylim(0, 3)\nplt.savefig(\"../figures/map/beta_distribution.pdf\", bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\nalpha = 11\nbeta = 11\nbernoulli_theta = 0.5\nn_1 = 9\nn_0 = 1\n\ndist = torch.distributions.Beta(concentration1=alpha, concentration0=beta)\nxs = torch.linspace(0, 1, 500)\nys = dist.log_prob(xs).exp()\nplt.plot(xs, ys, label=\"Beta Prior\")\n\nsample_size = n_1 + n_0\nplt.title(f\"H: {n_1}, T: {n_0}\")\n\nmle_estimate = n_1 / (sample_size)  # samples.mean()\nplt.axvline(mle_estimate, color=\"k\", linestyle=\"--\", label=\"MLE\")\n\nmap_estimate = (n_1 + alpha - 1) / (sample_size + alpha + beta - 2)\nplt.axvline(map_estimate, color=\"r\", linestyle=\"-.\", label=\"MAP\")\n\nplt.axvline(alpha / (alpha + beta), color=\"g\", linestyle=\":\", label=\"Prior Mean\")\n\nplt.grid()\nplt.xlabel(r\"$\\theta$\")\nplt.ylabel(r\"$p(\\theta)$\")\nplt.legend(bbox_to_anchor=(1.25, 1), borderaxespad=0)\nplt.savefig(\"../figures/map/coin_toss_prior_mle_map.pdf\", bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\ndef plot_beta_all(alpha, beta, bernoulli_theta=0.5, sample_size=10):\n    torch.manual_seed(42)\n\n    dist = torch.distributions.Beta(concentration1=alpha, concentration0=beta)\n    xs = torch.linspace(0, 1, 500)\n    ys = dist.log_prob(xs).exp()\n    plt.plot(xs, ys, label=\"Beta Prior\")\n\n    samples = torch.empty(sample_size)\n    for s_num in range(sample_size):\n        dist = torch.distributions.Bernoulli(probs=bernoulli_theta)\n        samples[s_num] = dist.sample()\n    n_1 = int(samples.sum())\n    n_0 = int(sample_size - n_1)\n    plt.title(f\"H: {n_1}, T: {n_0}\")\n\n    mle_estimate = n_1 / (sample_size)  # samples.mean()\n    plt.axvline(mle_estimate, color=\"k\", linestyle=\"--\", label=\"MLE\")\n\n    map_estimate = (n_1 + alpha - 1) / (sample_size + alpha + beta - 2)\n    plt.axvline(map_estimate, color=\"r\", linestyle=\"-.\", label=\"MAP\")\n\n    plt.axvline(alpha / (alpha + beta), color=\"g\", linestyle=\":\", label=\"Prior Mean\")\n\n    plt.grid()\n    plt.xlabel(r\"$\\theta$\")\n    plt.ylabel(r\"$p(\\theta)$\")\n    plt.legend(bbox_to_anchor=(1.25, 1), borderaxespad=0)\n    plt.show()\n\n\ninteract(\n    plot_beta_all,\n    alpha=FloatSlider(min=1, max=51, step=1, value=11),\n    beta=FloatSlider(min=1, max=51, step=1, value=11),\n    bernoulli_theta=FloatSlider(min=0, max=1, step=0.1, value=0.5),\n    sample_size=IntSlider(min=10, max=1000, step=10, value=10),\n)\n\n\n\n\n&lt;function __main__.plot_beta_all(alpha, beta, bernoulli_theta=0.5, sample_size=10)&gt;\n\n\n\n\nLinear Regression\n\nn_data = 20\nx = torch.linspace(-1, 1, n_data)\nslope = 2\nintercept = 1\nf = lambda x: slope * x + intercept\n\nnoise = torch.distributions.Normal(0, 1).sample((n_data,))\ny = f(x) + noise\n\nplt.figure()\nplt.scatter(x, y, marker=\"x\", c=\"k\", s=20, label=\"Noisy data\")\nplt.plot(x, f(x), c=\"k\", label=\"True function\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nline_str = f\"{slope} x + {intercept}\"\nplt.title(r\"$y_{true} = $\" + line_str)\nplt.savefig(f\"../figures/map/linreg_data-{n_data}.pdf\", bbox_inches=\"tight\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f2b9d6409d0&gt;\n\n\n\n\n\n\ndef nll(theta):\n    mu = theta[0] + theta[1] * x\n    sigma = torch.tensor(1.0)\n    dist = torch.distributions.normal.Normal(mu, sigma)\n    return -dist.log_prob(y).sum()\n\n\ndef neg_log_prior(theta):\n    prior_mean = torch.tensor([0.0, 0.0])  # Prior mean for slope and intercept\n    prior_std = torch.tensor(\n        [0.1, 0.1]\n    )  # Prior standard deviation for slope and intercept\n    dist = torch.distributions.normal.Normal(prior_mean, prior_std)\n    return -dist.log_prob(theta).sum()\n\n\ndef neg_log_joint(theta):\n    return nll(theta) + neg_log_prior(theta)\n\n\n# Create a grid of theta[0] and theta[1] values\ntheta0_values = torch.linspace(-4, 4, 101)\ntheta1_values = torch.linspace(-4, 4, 101)\ntheta0_mesh, theta1_mesh = torch.meshgrid(theta0_values, theta1_values)\nnll_values = torch.zeros_like(theta0_mesh)\nnlp_values = torch.zeros_like(theta0_mesh)\nnlj_values = torch.zeros_like(theta0_mesh)\n\nfor i in range(len(theta0_values)):\n    for j in range(len(theta1_values)):\n        theta_current = torch.tensor([theta0_values[i], theta1_values[j]])\n        nll_values[i, j] = nll(theta_current)\n        nlp_values[i, j] = neg_log_prior(theta_current)\n        nlj_values[i, j] = neg_log_joint(theta_current)\n\n\ndef plot_contour_with_minima(values, theta0_values, theta1_values, title, ax, cbar_ax):\n    # plt.figure(figsize=(6, 6))\n\n    contour = ax.contourf(theta0_mesh, theta1_mesh, values, levels=20, cmap=\"viridis\")\n    ax.set_xlabel(r\"$\\theta_0$\")\n    ax.set_ylabel(r\"$\\theta_1$\")\n    ax.set_title(title)\n\n    ax.set_aspect(\"equal\", adjustable=\"box\")  # Set aspect ratio to be equal\n\n    # mark the minimum\n    min_idx = np.unravel_index(np.argmin(values), values.shape)\n    t0_min = theta0_values[min_idx[0]]\n    t1_min = theta1_values[min_idx[1]]\n    ax.scatter(\n        t0_min,\n        t1_min,\n        marker=\"x\",\n        color=\"red\",\n        s=100,\n    )\n    ax.annotate(\n        f\"({t0_min:.2f}, {t1_min:.2f})\",\n        (t0_min, t1_min),\n        xytext=(t0_min - 0.5, t1_min - 0.8),\n        color=\"red\",\n        # set size of text\n        size=12,\n    )\n\n    fig.colorbar(contour, ax=ax, cax=cbar_ax)\n    # plt.tight_layout()\n    # plt.show()\n    # return contour\n\n\n# Usage example\nfig, ax = plt.subplots(\n    1, 7, figsize=(15, 3), gridspec_kw={\"width_ratios\": [10, 10, 1, 10, 1, 10, 1]}\n)\ni = 0\n\n\ndef get_next_ax():\n    global i\n    i += 1\n    return ax[i - 1]\n\n\ndata_ax = get_next_ax()\ndata_ax.scatter(x, y, marker=\"x\", c=\"k\", s=20, label=\"Noisy data\")\ndata_ax.plot(x, f(x), c=\"k\", label=\"True function\")\ndata_ax.set_xlabel(\"x\")\ndata_ax.set_ylabel(\"y\")\nline_str = f\"{slope} x + {intercept}\"\ndata_ax.set_title(r\"$y_{true} = $\" + line_str)\ndata_ax.legend()\n\ncontour = plot_contour_with_minima(\n    nll_values,\n    theta0_values,\n    theta1_values,\n    \"Negative Log-Likelihood\",\n    get_next_ax(),\n    get_next_ax(),\n)\ncontour = plot_contour_with_minima(\n    nlp_values,\n    theta0_values,\n    theta1_values,\n    \"Negative Log-Prior\",\n    get_next_ax(),\n    get_next_ax(),\n)\ncontour = plot_contour_with_minima(\n    nlj_values,\n    theta0_values,\n    theta1_values,\n    \"Negative Log-Joint\",\n    get_next_ax(),\n    get_next_ax(),\n)"
  },
  {
    "objectID": "notebooks/sampling-normal.html",
    "href": "notebooks/sampling-normal.html",
    "title": "Probabilistic Machine Learning",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport arviz as az\n%matplotlib inline\n# Retina display\n%config InlineBackend.figure_format = 'retina'\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\n\nN = 5000\nU = torch.rand(N, 2)\nU1 = U[:, 0]\nU2 = U[:, 1]\n\nR = torch.sqrt(-2 * torch.log(U1))\ntheta = 2 * np.pi * U2\n\nX = R * torch.cos(theta)\nY = R * torch.sin(theta)\n\naz.plot_kde(X.numpy(), label='X', plot_kwargs={'color': 'C0'})\naz.plot_kde(Y.numpy(), label='Y', plot_kwargs={'color': 'C1'})\n\n# Plot true density\nx = torch.linspace(-4, 4, 100)\nnorm = torch.distributions.Normal(0, 1)\nplt.plot(x, norm.log_prob(x).exp().numpy(), label='True density', color='C2')\n\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f52ce18d2e0&gt;\n\n\n\n\n\n\n### Multivariate Sampling\n\n\n\nfrom scipy.stats import gaussian_kde\n\ntrue_mean = torch.tensor([1., -1.])\ntrue_cov = torch.tensor([[1., 0.5], [0.5, 1.]])\ntrue_dist = torch.distributions.MultivariateNormal(true_mean, true_cov)\n\nsamples = true_dist.sample((N,))\n\n# Generate samples from the true distribution\nN = 4000  # Number of samples\nsamples = true_dist.sample((N,))\n\nsample_data = samples.numpy()\n\n# Calculate KDE using scipy's gaussian_kde\nsns.jointplot(x=sample_data[:, 0], y=sample_data[:, 1], kind=\"kde\", space=0, color='C0')\n\n\n\n\n\n# Find the cholesky decomposition of the covariance matrix\nL = torch.cholesky(true_cov)\nprint(L)\n\ntensor([[1.0000, 0.0000],\n        [0.5000, 0.8660]])\n\n\n\nL@L.T\n\ntensor([[1.0000, 0.5000],\n        [0.5000, 1.0000]])\n\n\n\nX\n\ntensor([ 0.6383,  0.1196, -0.6610,  ..., -1.2854, -0.0679, -0.8292])\n\n\n\nY.shape\n\ntorch.Size([5000])\n\n\n\nZ_I = torch.stack([X, Y], dim=1)\n\nZ_mu_sigma = Z_I @ L.T + true_mean\n\n\nZ_mu_sigma.shape\n\ntorch.Size([5000, 2])\n\n\n\nsns.jointplot(x=Z_mu_sigma.numpy()[:, 0], y=Z_mu_sigma.numpy()[:, 1], kind=\"kde\", space=0, color='C0')\n\n\n\n\n\nnp.cov(Z_mu_sigma.numpy().T)\n\narray([[1.05325545, 0.51403749],\n       [0.51403749, 1.01457316]])\n\n\n\nnp.mean(Z_mu_sigma.numpy(), axis=0)\n\narray([ 0.9873801, -0.992024 ], dtype=float32)"
  },
  {
    "objectID": "notebooks/lin_reg_slider.html",
    "href": "notebooks/lin_reg_slider.html",
    "title": "Probabilistic Machine Learning",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output\nfrom matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n%matplotlib inline\n# Retina display\n%config InlineBackend.figure_format = 'retina'\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\n\nplt.rcParams[\"font.family\"] = \"Arial\"\n\n\nX = np.array([1, 2, 3, 4, 5])\ny = np.array([2, 3, 4, 4.5, 5])\n\n\ndef compute_cost(theta0, theta1):\n    y_pred = theta0 + theta1 * X\n    error = y_pred - y\n    cost = -0.5 * np.sum(error ** 2)\n    return cost\n\n\ntheta0_vals = np.linspace(-2, 4, 100)\ntheta1_vals = np.linspace(-2, 4, 100)\ntheta0_grid, theta1_grid = np.meshgrid(theta0_vals, theta1_vals)\ncost_grid = np.zeros_like(theta0_grid)\n\nfor i in range(len(theta0_vals)):\n    for j in range(len(theta1_vals)):\n        cost_grid[i, j] = compute_cost(theta0_vals[i], theta1_vals[j])\n\ntheta0_slider = widgets.FloatSlider(\n    value=0, min=-2, max=4, step=0.1, description='Theta0:')\ntheta1_slider = widgets.FloatSlider(\n    value=0, min=-2, max=4, step=0.1, description='Theta1:')\n\nfigure_container = widgets.Output()\n\n\ndef update_figure(change):\n    with figure_container:\n        clear_output(wait=True)\n        plt.figure(figsize=(12, 5))\n\n        plt.subplot(1, 2, 1)\n        plt.contourf(theta0_grid, theta1_grid, cost_grid,\n                     levels=20, cmap='viridis')\n        plt.colorbar(label='Cost')\n\n        true_theta0 = 1.0\n        true_theta1 = 1.0\n        plt.scatter([true_theta0], [true_theta1], color='blue',\n                    marker='o', label='True Values')\n        plt.scatter([theta0_slider.value], [theta1_slider.value],\n                    color='red', marker='x', label='Current Values')\n        plt.xlabel('Theta0')\n        plt.ylabel('Theta1')\n        plt.title('Log Likelihood Contour Plot')\n        plt.legend()\n\n        plt.subplot(1, 2, 2)\n        plt.scatter(X, y, label='Data')\n        plt.plot(X, theta0_slider.value + theta1_slider.value *\n                 X, color='red', label='Line Fit')\n        plt.xlabel('X')\n        plt.ylabel('y')\n        plt.title('Linear Regression Line Fit')\n        plt.legend()\n\n        # plt.tight_layout()\n        plt.show()\n\n\ntheta0_slider.observe(update_figure, 'value')\ntheta1_slider.observe(update_figure, 'value')\n\nupdate_figure(None)\n\ndisplay(widgets.HBox(\n    [figure_container, widgets.VBox([theta0_slider, theta1_slider])]))\n\n\n\n\n\n# plt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.contourf(theta0_grid, theta1_grid, cost_grid, levels=20, cmap='viridis')\nplt.colorbar(label='Cost')\n\ntrue_theta0 = 1.0\ntrue_theta1 = 1.0\nplt.scatter([true_theta0], [true_theta1], color='blue',\n            marker='o', label='True Values')\nplt.scatter([0.5], [0.2], color='red', marker='x', label='Current Values')\nplt.xlabel('Theta0')\nplt.ylabel('Theta1')\nplt.title('Log Likelihood Contour Plot')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.scatter(X, y, label='Data')\nplt.plot(X, 0.5 + 0.2 * X, color='red', label='Line Fit')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Linear Regression Line Fit')\nplt.legend()\n\n# plt.tight_layout()\nplt.savefig('figures/mle/lin_reg_slider_1.pdf')"
  },
  {
    "objectID": "notebooks/siren.html",
    "href": "notebooks/siren.html",
    "title": "Hypernetwork",
    "section": "",
    "text": "import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Remove all the warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set env CUDA_LAUNCH_BLOCKING=1\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n\ntry:\n    from einops import rearrange\nexcept ImportError:\n    %pip install einops\n    from einops import rearrange\n\n\nif os.path.exists('dog.jpg'):\n    print('dog.jpg exists')\nelse:\n    !wget https://segment-anything.com/assets/gallery/AdobeStock_94274587_welsh_corgi_pembroke_CD.jpg -O dog.jpg\n\ndog.jpg exists\n\n\n\n# Read in a image from torchvision\nimg = torchvision.io.read_image(\"dog.jpg\")\nprint(img.shape)\n\ntorch.Size([3, 1365, 2048])\n\n\n\n# Show the image\nplt.imshow(rearrange(img, 'c h w -&gt; h w c').numpy())\n\n&lt;matplotlib.image.AxesImage at 0x7fc2316addf0&gt;\n\n\n\n\n\n\n# Use sklearn to normalize the image and store the transform to be used later\nfrom sklearn import preprocessing\n\nscaler_img = preprocessing.MinMaxScaler().fit(img.reshape(-1, 1))\nscaler_img\n\nMinMaxScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MinMaxScalerMinMaxScaler()\n\n\n\nimg_scaled = scaler_img.transform(img.reshape(-1, 1)).reshape(img.shape)\nimg_scaled.shape\n\nimg_scaled = torch.tensor(img_scaled)\n\n\nimg_scaled.shape\n\ntorch.Size([3, 1365, 2048])\n\n\n\nimg_scaled = img_scaled.to(device)\nimg_scaled\n\ntensor([[[0.3098, 0.3137, 0.3137,  ..., 0.2941, 0.2941, 0.2980],\n         [0.3098, 0.3137, 0.3137,  ..., 0.2941, 0.2941, 0.2980],\n         [0.3098, 0.3137, 0.3137,  ..., 0.2941, 0.2941, 0.2980],\n         ...,\n         [0.4745, 0.4745, 0.4784,  ..., 0.3804, 0.3765, 0.3765],\n         [0.4745, 0.4745, 0.4784,  ..., 0.3804, 0.3804, 0.3765],\n         [0.4745, 0.4745, 0.4784,  ..., 0.3843, 0.3804, 0.3804]],\n\n        [[0.2039, 0.2078, 0.2078,  ..., 0.2157, 0.2157, 0.2118],\n         [0.2039, 0.2078, 0.2078,  ..., 0.2157, 0.2157, 0.2118],\n         [0.2039, 0.2078, 0.2078,  ..., 0.2157, 0.2157, 0.2118],\n         ...,\n         [0.4039, 0.4039, 0.4078,  ..., 0.3216, 0.3176, 0.3176],\n         [0.4039, 0.4039, 0.4078,  ..., 0.3216, 0.3216, 0.3176],\n         [0.4039, 0.4039, 0.4078,  ..., 0.3255, 0.3216, 0.3216]],\n\n        [[0.1373, 0.1412, 0.1412,  ..., 0.1176, 0.1176, 0.1176],\n         [0.1373, 0.1412, 0.1412,  ..., 0.1176, 0.1176, 0.1176],\n         [0.1373, 0.1412, 0.1412,  ..., 0.1176, 0.1176, 0.1176],\n         ...,\n         [0.1451, 0.1451, 0.1490,  ..., 0.1686, 0.1647, 0.1647],\n         [0.1451, 0.1451, 0.1490,  ..., 0.1686, 0.1686, 0.1647],\n         [0.1451, 0.1451, 0.1490,  ..., 0.1725, 0.1686, 0.1686]]],\n       device='cuda:0', dtype=torch.float64)\n\n\n\ncrop = torchvision.transforms.functional.crop(img_scaled.cpu(), 600, 750, 400, 400)\ncrop.shape\n\ntorch.Size([3, 400, 400])\n\n\n\n# Plot the crop using matplotlib and using torch.einsum to convert the image \n# from C, H, W to H, W, C\nplt.imshow(rearrange(crop, 'c h w -&gt; h w c').cpu().numpy())\n\n&lt;matplotlib.image.AxesImage at 0x7fc4461d7700&gt;\n\n\n\n\n\n\ncrop = crop.to(device)\n\n\n# Get the dimensions of the image tensor\nnum_channels, height, width = crop.shape\nprint(num_channels, height, width)\n\n3 400 400\n\n\nLet us now write a function to generate the coordinate inputs. We want to first have changes in the y coordinate and then in the x coordinate. This is equivlent to lower bit changes first and then higher bit changes.\n\nnum_channels, height, width = 2, 3, 4\n\n    \n# Create a 2D grid of (x,y) coordinates\nw_coords = torch.arange(width).repeat(height, 1)\nh_coords = torch.arange(height).repeat(width, 1).t()\nw_coords = w_coords.reshape(-1)\nh_coords = h_coords.reshape(-1)\n\n# Combine the x and y coordinates into a single tensor\nX = torch.stack([h_coords, w_coords], dim=1).float()\n\n\nX\n\ntensor([[0., 0.],\n        [0., 1.],\n        [0., 2.],\n        [0., 3.],\n        [1., 0.],\n        [1., 1.],\n        [1., 2.],\n        [1., 3.],\n        [2., 0.],\n        [2., 1.],\n        [2., 2.],\n        [2., 3.]])\n\n\n\ndef create_coordinate_map(img):\n    \"\"\"\n    img: torch.Tensor of shape (num_channels, height, width)\n    \n    return: tuple of torch.Tensor of shape (height * width, 2) and torch.Tensor of shape (height * width, num_channels)\n    \"\"\"\n    \n    num_channels, height, width = img.shape\n    \n    # Create a 2D grid of (x,y) coordinates (h, w)\n    # width values change faster than height values\n    w_coords = torch.arange(width).repeat(height, 1)\n    h_coords = torch.arange(height).repeat(width, 1).t()\n    w_coords = w_coords.reshape(-1)\n    h_coords = h_coords.reshape(-1)\n\n    # Combine the x and y coordinates into a single tensor\n    X = torch.stack([h_coords, w_coords], dim=1).float()\n\n    # Move X to GPU if available\n    X = X.to(device)\n\n    # Reshape the image to (h * w, num_channels)\n    Y = rearrange(img, 'c h w -&gt; (h w) c').float()\n    return X, Y\n\n\ndog_X, dog_Y = create_coordinate_map(crop)\n\ndog_X.shape, dog_Y.shape\n\n(torch.Size([160000, 2]), torch.Size([160000, 3]))\n\n\n\ndog_X\n\ntensor([[  0.,   0.],\n        [  0.,   1.],\n        [  0.,   2.],\n        ...,\n        [399., 397.],\n        [399., 398.],\n        [399., 399.]], device='cuda:0')\n\n\n\n# MinMaxScaler from -1 to 1\nscaler_X = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(dog_X.cpu())\n\n# Scale the X coordinates\ndog_X_scaled = scaler_X.transform(dog_X.cpu())\n\n# Move the scaled X coordinates to the GPU\ndog_X_scaled = torch.tensor(dog_X_scaled).to(device)\n\n# Set to dtype float32\ndog_X_scaled = dog_X_scaled.float()\n\n\ndog_X_scaled.shape, dog_Y.shape\n\n(torch.Size([160000, 2]), torch.Size([160000, 3]))\n\n\n\ndog_X[:2], dog_X_scaled[:2], dog_Y[:2]\n\n(tensor([[0., 0.],\n         [0., 1.]], device='cuda:0'),\n tensor([[-1.0000, -1.0000],\n         [-1.0000, -0.9950]], device='cuda:0'),\n tensor([[0.7686, 0.6941, 0.4745],\n         [0.7686, 0.6941, 0.4745]], device='cuda:0'))\n\n\n\n# Create a MLP with 5 hidden layers with 256 neurons each and ReLU activations.\n# Input is (x, y) and output is (r, g, b) or (g) for grayscale\n\ns = 128\n\nclass NN(nn.Module):\n    def _init_siren(self, activation_scale):\n        self.fc1.weight.data.uniform_(-1/self.fc1.in_features, 1/self.fc1.in_features)\n        for layers in [self.fc2, self.fc3, self.fc4, self.fc5]:\n            layers.weight.data.uniform_(-np.sqrt(6/self.fc2.in_features)/activation_scale, \n                                        np.sqrt(6/self.fc2.in_features)/activation_scale)\n        \n    def __init__(self, activation=torch.sin, n_out=1, activation_scale=1.0):\n        super().__init__()\n        self.activation = activation    \n        self.activation_scale = activation_scale\n        self.fc1 = nn.Linear(2, s)\n        self.fc2 = nn.Linear(s, s)\n        self.fc3 = nn.Linear(s, s)\n        self.fc4 = nn.Linear(s, s)\n        self.fc5 = nn.Linear(s, n_out) #gray scale image (1) or RGB (3)\n        if self.activation == torch.sin:\n            # init weights and biases for sine activation\n            self._init_siren(activation_scale=self.activation_scale)\n\n    def forward(self, x):\n        x = self.activation(self.activation_scale*self.fc1(x))\n        x = self.activation(self.activation_scale*self.fc2(x))\n        x = self.activation(self.activation_scale*self.fc3(x))\n        x = self.activation(self.activation_scale*self.fc4(x))\n        return self.fc5(x)\n\n\n# Shuffle data\n\n# shuffled index\nsh_index = torch.randperm(dog_X_scaled.shape[0])\n\n# Shuffle the dataset\ndog_X_sh = dog_X_scaled[sh_index]\ndog_Y_sh = dog_Y[sh_index]\n\n\nnns = {}\nnns[\"dog\"] = {}\nnns[\"dog\"][\"relu\"] = NN(activation=torch.relu, n_out=3).to(device)\nnns[\"dog\"][\"sin\"] = NN(activation=torch.sin, n_out=3, activation_scale=30.0).to(device)\n\n\nnns[\"dog\"][\"relu\"](dog_X_sh).shape, nns[\"dog\"][\"sin\"](dog_X_sh).shape\n\n(torch.Size([160000, 3]), torch.Size([160000, 3]))\n\n\n\nn_iter = 2200\n\n\ndef train(net, lr, X, Y, epochs, verbose=True):\n    \"\"\"\n    net: torch.nn.Module\n    lr: float\n    X: torch.Tensor of shape (num_samples, 2)\n    Y: torch.Tensor of shape (num_samples, 3)\n    \"\"\"\n\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n    for epoch in range(epochs):\n        optimizer.zero_grad()\n        outputs = net(X)\n        \n        \n        loss = criterion(outputs, Y)\n        loss.backward()\n        optimizer.step()\n        if verbose and epoch % 100 == 0:\n            print(f\"Epoch {epoch} loss: {loss.item():.6f}\")\n    return loss.item()\n\n\ntrain(nns[\"dog\"][\"relu\"], lr=3e-4, X=dog_X_sh, Y=dog_Y_sh, epochs=n_iter)\n\nEpoch 0 loss: 0.362823\nEpoch 100 loss: 0.030849\nEpoch 200 loss: 0.027654\nEpoch 300 loss: 0.024760\nEpoch 400 loss: 0.021445\nEpoch 500 loss: 0.017849\nEpoch 600 loss: 0.015446\nEpoch 700 loss: 0.013979\nEpoch 800 loss: 0.013087\nEpoch 900 loss: 0.013066\nEpoch 1000 loss: 0.011910\nEpoch 1100 loss: 0.011552\nEpoch 1200 loss: 0.011221\nEpoch 1300 loss: 0.010859\nEpoch 1400 loss: 0.010564\nEpoch 1500 loss: 0.010345\nEpoch 1600 loss: 0.010518\nEpoch 1700 loss: 0.009886\nEpoch 1800 loss: 0.009697\nEpoch 1900 loss: 0.009528\nEpoch 2000 loss: 0.009371\nEpoch 2100 loss: 0.009229\n\n\n0.009103643707931042\n\n\n\ndef plot_reconstructed_and_original_image(original_img, net, X, title=\"\"):\n    \"\"\"\n    net: torch.nn.Module\n    X: torch.Tensor of shape (num_samples, 2)\n    Y: torch.Tensor of shape (num_samples, 3)\n    \"\"\"\n    num_channels, height, width = original_img.shape\n    net.eval()\n    with torch.no_grad():\n        outputs = net(X)\n        outputs = outputs.reshape(height, width, num_channels)\n        #outputs = outputs.permute(1, 2, 0)\n    fig = plt.figure(figsize=(6, 4))\n    gs = gridspec.GridSpec(1, 2, width_ratios=[1, 1])\n\n    ax0 = plt.subplot(gs[0])\n    ax1 = plt.subplot(gs[1])\n\n    ax0.imshow(outputs.cpu())\n    ax0.set_title(\"Reconstructed Image\")\n    \n\n    ax1.imshow(original_img.cpu().permute(1, 2, 0))\n    ax1.set_title(\"Original Image\")\n    \n    for a in [ax0, ax1]:\n        a.axis(\"off\")\n\n\n    fig.suptitle(title, y=0.9)\n    plt.tight_layout()\n\nplot_reconstructed_and_original_image(crop, nns[\"dog\"][\"relu\"], dog_X_scaled, title=\"ReLU\")\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\nimgs_dog_sin = train(nns[\"dog\"][\"sin\"], lr=3e-4, X=dog_X_sh, Y=dog_Y_sh, epochs=n_iter)\n\nEpoch 0 loss: 0.409131\nEpoch 100 loss: 0.003556\nEpoch 200 loss: 0.002283\nEpoch 300 loss: 0.001715\nEpoch 400 loss: 0.001379\nEpoch 500 loss: 0.001163\nEpoch 600 loss: 0.000990\nEpoch 700 loss: 0.000902\nEpoch 800 loss: 0.000778\nEpoch 900 loss: 0.000713\nEpoch 1000 loss: 0.000647\nEpoch 1100 loss: 0.000597\nEpoch 1200 loss: 0.000548\nEpoch 1300 loss: 0.000509\nEpoch 1400 loss: 0.000494\nEpoch 1500 loss: 0.000448\nEpoch 1600 loss: 0.000423\nEpoch 1700 loss: 0.000401\nEpoch 1800 loss: 0.000386\nEpoch 1900 loss: 0.000363\nEpoch 2000 loss: 0.000342\nEpoch 2100 loss: 0.000335\n\n\n\nplot_reconstructed_and_original_image(crop, nns[\"dog\"][\"sin\"], dog_X_scaled, title=\"Sine\")\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\ntest_error_all_data = {}\ntest_error_all_data[\"sin\"] = torch.nn.MSELoss()(nns[\"dog\"][\"sin\"](dog_X_scaled), dog_Y).item()\ntest_error_all_data[\"relu\"] = torch.nn.MSELoss()(nns[\"dog\"][\"relu\"](dog_X_scaled), dog_Y).item()\n\ntest_error_all_data\n\n{'sin': 0.00031630051671527326, 'relu': 0.009100575000047684}\n\n\n\ncontext_lengths = [5, 10, 100, 1000, 10000, len(dog_X)]\n\n\n# Now, reconstruct the image using a sine activation function with varying number of context points (subsampled from the original image)\n\ntest_nets_sirens = {}\n\n\n\nprint(dog_X_sh.shape, dog_Y_sh.shape)\nloss_context = {\"train\": {}, \"test\": {}}\nfor num_context in context_lengths:\n    print(\"=\"*50)\n    print(f\"Number of context points: {num_context}\")\n    test_nets_sirens[num_context] = NN(activation=torch.sin, n_out=3, activation_scale=30.0).to(device)\n    loss_context[\"train\"][num_context] = train(test_nets_sirens[num_context],\n                                               lr=3e-4, X=dog_X_sh[:num_context], \n                                               Y=dog_Y_sh[:num_context], epochs=n_iter, verbose=False)\n    # Find the loss on the test set\n    with torch.no_grad():\n        loss_context[\"test\"][num_context] = nn.MSELoss()(test_nets_sirens[num_context](dog_X_scaled), dog_Y).item()\n    print(f\"Test loss: {loss_context['test'][num_context]:.6f}\")\n\ntorch.Size([160000, 2]) torch.Size([160000, 3])\n==================================================\nNumber of context points: 5\nTest loss: 0.226195\n==================================================\nNumber of context points: 10\nTest loss: 0.043449\n==================================================\nNumber of context points: 100\nTest loss: 0.029126\n==================================================\nNumber of context points: 1000\nTest loss: 0.013547\n==================================================\nNumber of context points: 10000\nTest loss: 0.003966\n==================================================\nNumber of context points: 160000\nTest loss: 0.000310\n\n\n\n# Plot the test loss vs number of context points\nseries = pd.Series(loss_context[\"test\"])\nseries.plot(kind='bar')\nplt.xlabel(\"Number of context points\")\nplt.ylabel(\"Test loss\")\n\nText(0, 0.5, 'Test loss')\n\n\n\n\n\n\n# Plot the reconstructed image for each number of context points\n\nfor num_context in context_lengths:\n    print(f\"Number of context points: {num_context}\")\n    plot_reconstructed_and_original_image(crop, test_nets_sirens[num_context], \n                                          dog_X_scaled, title=f\"Number of context points: {num_context}\")\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\nNumber of context points: 5\nNumber of context points: 10\nNumber of context points: 100\nNumber of context points: 1000\nNumber of context points: 10000\nNumber of context points: 160000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n### Comparison of network size (number of floats) v/s the image size\n\nimg_size = crop.shape[1]*crop.shape[2]\nnw_size = torch.sum(torch.tensor([p.numel() for p in nns[\"dog\"][\"sin\"].parameters()]))\n\nprint(f\"Image size: {img_size}\")\nprint(f\"Network size: {nw_size}\")\n\nImage size: 160000\nNetwork size: 50307\n\n\n\ntry:\n    from tabulate import tabulate\nexcept:\n    %pip install tabulate\n    from tabulate import tabulate\n\nmodel = nns[\"dog\"][\"sin\"]\n\ntable_data = []\n\ntotal_params = 0\nstart = 0\nstart_end_mapping = {}\nfor name, param in model.named_parameters():\n    param_count = torch.prod(torch.tensor(param.shape)).item()\n    total_params += param_count\n    end = total_params\n    table_data.append([name, param.shape, param_count, start, end])\n    start_end_mapping[name] = (start, end)\n    start = end\n\nprint(tabulate(table_data, headers=[\"Layer Name\", \"Shape\", \"Parameter Count\", \"Start Index\", \"End Index\"]))\nprint(f\"Total number of parameters: {total_params}\")\n\nLayer Name    Shape                     Parameter Count    Start Index    End Index\n------------  ----------------------  -----------------  -------------  -----------\nfc1.weight    torch.Size([128, 2])                  256              0          256\nfc1.bias      torch.Size([128])                     128            256          384\nfc2.weight    torch.Size([128, 128])              16384            384        16768\nfc2.bias      torch.Size([128])                     128          16768        16896\nfc3.weight    torch.Size([128, 128])              16384          16896        33280\nfc3.bias      torch.Size([128])                     128          33280        33408\nfc4.weight    torch.Size([128, 128])              16384          33408        49792\nfc4.bias      torch.Size([128])                     128          49792        49920\nfc5.weight    torch.Size([3, 128])                  384          49920        50304\nfc5.bias      torch.Size([3])                         3          50304        50307\nTotal number of parameters: 50307\n\n\n\nstart_end_mapping\n\n{'fc1.weight': (0, 256),\n 'fc1.bias': (256, 384),\n 'fc2.weight': (384, 16768),\n 'fc2.bias': (16768, 16896),\n 'fc3.weight': (16896, 33280),\n 'fc3.bias': (33280, 33408),\n 'fc4.weight': (33408, 49792),\n 'fc4.bias': (49792, 49920),\n 'fc5.weight': (49920, 50304),\n 'fc5.bias': (50304, 50307)}\n\n\n\nInput: (x, y, R, G, B)\n\n\nOutput: Our Hypernetwork should have the output equal to the number of parameters in the main network.\n\nclass HyperNet(nn.Module):\n    def __init__(self, num_layers=5, num_neurons=256, activation=torch.sin, n_out=3):\n        super().__init__()\n        self.activation = activation\n        self.n_out = total_params\n        self.fc1 = nn.Linear(5, 512)\n        self.fc2 = nn.Linear(512, 512)\n        self.fc3 = nn.Linear(512, total_params)\n\n    def forward(self, x):\n        x = self.activation(self.fc1(x))\n        x = self.activation(self.fc2(x))\n        return self.fc3(x)\n\n\nhp = HyperNet().to(device)\n\nout_hp = hp(torch.rand(10, 5).to(device))\nprint(out_hp.shape)\n\nweights_flattened  = out_hp.mean(dim=0)\nprint(weights_flattened.shape)\n\ntorch.Size([10, 50307])\ntorch.Size([50307])\n\n\n\n# Set the weights of the model using start_end_mapping\n\nmodel = nns[\"dog\"][\"sin\"]\n\nfor name, param in model.named_parameters():\n    start, end = start_end_mapping[name]\n    param.data = weights_flattened[start:end].reshape(param.shape)\n\n\n\nUsing our homegrown library (Astra)\n\nFlattening and unflattening the weights\nEasily creating SIREN models\nEasily creating training loops\n\n\nfrom astra.torch.utils import ravel_pytree\nflat_weights, unravel_fn = ravel_pytree(dict(model.named_parameters()))\nprint(flat_weights.shape)\nprint(unravel_fn(flat_weights))\n\ntorch.Size([50307])\n{'fc1.weight': tensor([[ 7.2303e-02,  1.8475e-01],\n        [-6.9180e-03,  1.2231e-02],\n        [-1.9431e-01,  1.1563e-01],\n        [ 3.7423e-02,  3.5960e-02],\n        [ 3.5826e-02,  9.0268e-02],\n        [ 7.2428e-02,  2.3853e-02],\n        [ 6.7613e-02, -1.1572e-01],\n        [-1.8519e-01, -2.0865e-02],\n        [ 2.3674e-01, -5.3692e-02],\n        [ 1.3188e-01, -4.3248e-02],\n        [-1.7791e-01, -3.5844e-01],\n        [ 1.1637e-01,  1.0213e-01],\n        [ 3.6082e-02, -1.2981e-01],\n        [-8.9860e-02,  3.0743e-01],\n        [ 9.0076e-02, -1.3024e-01],\n        [ 2.0144e-01, -1.3623e-01],\n        [ 1.0967e-01,  1.1245e-01],\n        [ 2.1241e-02,  3.6728e-02],\n        [-7.0298e-02,  1.7884e-01],\n        [-7.9913e-02, -9.6210e-03],\n        [-1.2231e-03, -6.7778e-02],\n        [-6.1415e-02,  1.7982e-01],\n        [ 8.7729e-02,  7.6696e-02],\n        [ 1.1200e-02, -1.8881e-01],\n        [-6.9393e-02,  1.3919e-01],\n        [ 1.9506e-01,  1.5109e-01],\n        [ 1.0525e-01,  1.6728e-02],\n        [-7.1199e-02, -5.0646e-02],\n        [-3.4556e-02, -3.8549e-02],\n        [-6.5598e-02,  1.4262e-01],\n        [-6.3033e-02,  4.6765e-02],\n        [ 1.6343e-01, -1.1612e-01],\n        [ 5.3650e-02,  2.0001e-02],\n        [ 4.4351e-02, -1.2483e-01],\n        [-6.9576e-02,  1.9289e-02],\n        [ 2.1541e-01, -1.4095e-01],\n        [-4.7398e-02, -1.5326e-01],\n        [-5.5390e-02, -7.6401e-03],\n        [ 1.1270e-01, -3.3292e-02],\n        [ 2.6212e-01, -1.9070e-02],\n        [-2.2660e-02, -5.4961e-02],\n        [ 1.2587e-02,  1.1614e-01],\n        [-1.7161e-02, -7.1058e-02],\n        [ 2.9235e-02, -1.1581e-01],\n        [-1.4140e-02, -6.8351e-02],\n        [ 1.7271e-02, -1.7049e-01],\n        [-5.7903e-02,  1.6249e-01],\n        [-2.7371e-01, -1.5921e-01],\n        [ 1.2445e-01, -3.7183e-02],\n        [ 1.4074e-02, -2.4972e-01],\n        [-1.4386e-01,  1.5713e-01],\n        [ 1.6494e-01,  9.7524e-02],\n        [-1.6054e-01,  6.0998e-02],\n        [ 2.8658e-02,  3.1216e-02],\n        [-2.8942e-01,  1.5718e-01],\n        [ 2.3916e-01,  1.0318e-01],\n        [-3.3868e-02, -3.0468e-01],\n        [ 6.7633e-03,  6.7953e-02],\n        [ 3.2195e-01, -9.9608e-02],\n        [ 8.3436e-02,  4.2797e-02],\n        [-1.0098e-01,  7.9213e-02],\n        [-1.1259e-01,  3.0771e-02],\n        [ 6.9695e-02,  5.9238e-02],\n        [ 2.1898e-01, -2.5084e-01],\n        [ 8.4120e-02,  3.9265e-02],\n        [-4.4006e-02, -5.9050e-03],\n        [-1.8446e-01,  1.7159e-01],\n        [ 1.2318e-02, -1.6082e-02],\n        [-1.2422e-01, -1.2124e-01],\n        [ 1.5859e-01, -6.5549e-02],\n        [-6.1395e-02,  1.4970e-01],\n        [-1.4709e-01,  3.1681e-02],\n        [ 3.6000e-02, -1.8523e-02],\n        [ 1.4831e-01, -1.5797e-01],\n        [ 1.8725e-01,  3.8700e-02],\n        [-1.8377e-01, -5.8808e-02],\n        [ 1.0143e-02, -1.4246e-01],\n        [-1.4042e-01, -5.2038e-05],\n        [ 2.3339e-01,  8.5529e-02],\n        [-1.0926e-01,  9.3398e-03],\n        [ 2.6608e-02,  4.7951e-02],\n        [-1.4424e-01, -2.1638e-01],\n        [-2.6549e-02,  2.2673e-01],\n        [ 6.6978e-02,  2.1663e-01],\n        [ 1.6812e-01, -1.4362e-01],\n        [ 1.8123e-01, -1.0973e-02],\n        [-9.9815e-03, -6.4446e-02],\n        [ 1.0362e-02, -1.1157e-01],\n        [ 1.9818e-02,  3.1765e-03],\n        [-4.1822e-02,  2.9222e-02],\n        [ 3.5510e-04, -9.5421e-02],\n        [ 4.0710e-02, -3.6539e-03],\n        [ 8.6873e-02,  2.0872e-01],\n        [-1.2687e-01,  1.7474e-01],\n        [ 1.4681e-01,  3.6409e-02],\n        [ 6.9398e-02,  4.4749e-02],\n        [-3.4701e-01, -3.8960e-02],\n        [ 2.0324e-01,  6.4354e-02],\n        [-2.0250e-01, -7.3486e-02],\n        [ 2.5662e-01, -6.7429e-02],\n        [ 9.0947e-03,  6.0853e-03],\n        [-1.3005e-01, -1.3669e-01],\n        [-1.4868e-01, -4.5554e-03],\n        [ 1.5069e-01,  8.6696e-02],\n        [ 4.6468e-02,  6.9869e-02],\n        [-1.2077e-01, -3.3872e-03],\n        [-5.2897e-03, -1.9553e-01],\n        [ 1.2461e-01, -4.0207e-03],\n        [-1.0743e-01,  1.4018e-01],\n        [ 2.1903e-01, -1.1876e-01],\n        [ 3.3978e-02, -5.6171e-03],\n        [-1.6130e-01, -7.7339e-02],\n        [ 1.7324e-01, -9.7195e-02],\n        [ 4.2971e-02,  1.0253e-01],\n        [-1.0369e-02, -4.1130e-02],\n        [-1.1557e-01, -1.0589e-01],\n        [ 1.8067e-01,  1.6982e-01],\n        [ 4.1407e-02, -2.6240e-02],\n        [ 6.1280e-02,  2.0380e-01],\n        [ 2.0553e-02, -1.8692e-01],\n        [ 5.4052e-02,  7.0528e-03],\n        [ 6.3718e-02,  1.0462e-01],\n        [ 6.9697e-02, -7.6473e-02],\n        [ 7.3403e-02,  7.8244e-02],\n        [-1.3281e-01, -7.3169e-04],\n        [ 1.2163e-01, -1.2142e-01],\n        [-1.1956e-01,  2.6188e-02],\n        [-2.0094e-01, -3.3400e-03]], device='cuda:0',\n       grad_fn=&lt;ReshapeAliasBackward0&gt;), 'fc1.bias': tensor([ 1.3937e-01,  9.1031e-02,  1.1908e-01, -5.2545e-02, -9.3397e-02,\n        -2.5996e-03,  5.6113e-02,  8.7695e-02,  9.4380e-02,  5.8306e-02,\n        -5.8491e-02, -9.4788e-02, -1.0383e-01,  1.8825e-01,  6.4387e-02,\n        -6.8692e-02, -5.7511e-02, -1.0626e-01,  1.8366e-01,  9.3314e-02,\n        -1.5770e-01, -1.1771e-01,  1.8512e-02, -1.0166e-01,  1.1778e-01,\n         6.4702e-03,  2.0301e-01, -2.4718e-02,  1.1473e-02, -5.9337e-02,\n         4.5290e-02, -2.7956e-04,  7.4153e-02,  7.4455e-02,  1.1620e-01,\n         9.1790e-02, -2.6550e-01,  4.0917e-02, -4.9924e-02,  3.5913e-01,\n         2.3945e-01, -1.3163e-01,  7.0677e-02,  4.3314e-02,  5.4965e-02,\n        -3.8266e-02,  1.1958e-01,  1.9800e-02, -3.2567e-02, -7.0675e-02,\n         7.4658e-02,  1.3528e-01,  6.7624e-02,  1.3752e-01,  1.4267e-01,\n         8.1409e-02,  1.5277e-01, -9.4721e-02,  9.5535e-03, -2.7910e-02,\n        -6.1356e-02,  9.8481e-02,  1.7242e-01, -2.3863e-02, -8.4674e-02,\n        -5.6868e-03,  1.4529e-01, -1.4019e-01, -2.7921e-02,  2.1329e-01,\n        -4.0568e-02, -1.3940e-01,  3.9279e-02,  3.4861e-02,  1.6734e-01,\n        -2.2712e-02,  1.0885e-01,  1.1623e-01,  1.6150e-01, -1.8630e-02,\n        -7.1212e-02, -1.4402e-01,  1.5908e-01, -2.2806e-01, -4.3496e-02,\n         1.4644e-01, -2.2250e-02, -5.0974e-02, -1.5041e-01, -1.0052e-01,\n        -3.0135e-02,  7.5585e-02, -5.2927e-02, -1.2286e-01,  1.0296e-02,\n         1.0396e-01, -1.3909e-01,  1.1614e-02,  1.0974e-03,  2.1793e-01,\n        -2.5740e-02, -9.2893e-02,  4.9473e-02, -1.8746e-01,  4.2226e-02,\n         1.7077e-01,  1.5371e-01,  1.2752e-01, -7.0501e-02, -1.9778e-03,\n         4.9510e-02,  2.5706e-02, -5.7692e-02,  2.4992e-01, -1.6351e-01,\n         6.7651e-02, -7.1454e-02, -1.0196e-01,  6.8088e-02, -1.8465e-01,\n         5.1298e-03,  1.0726e-02,  6.1477e-03,  2.7650e-01, -1.9581e-02,\n        -1.2533e-01,  1.0733e-01, -3.6380e-02], device='cuda:0',\n       grad_fn=&lt;ReshapeAliasBackward0&gt;), 'fc2.weight': tensor([[ 0.1626,  0.1576,  0.0090,  ...,  0.0867, -0.0421, -0.0025],\n        [ 0.1144, -0.0025, -0.1852,  ..., -0.1083, -0.1402, -0.0354],\n        [-0.1914, -0.0494, -0.0735,  ..., -0.0817,  0.0393,  0.0877],\n        ...,\n        [-0.0717, -0.0291,  0.0692,  ..., -0.0523, -0.0864, -0.0602],\n        [ 0.1178, -0.1181,  0.0901,  ...,  0.0402,  0.1107,  0.0868],\n        [ 0.1341,  0.0799,  0.1080,  ...,  0.0148,  0.0278,  0.1365]],\n       device='cuda:0', grad_fn=&lt;ReshapeAliasBackward0&gt;), 'fc2.bias': tensor([ 2.6667e-02, -4.5061e-02, -1.3379e-01, -4.2892e-02, -1.9645e-01,\n        -2.3817e-01,  1.0494e-02,  2.8197e-01,  1.5406e-02, -2.4376e-02,\n         1.7561e-01, -1.4642e-01, -1.0797e-01,  3.6716e-02, -2.2550e-02,\n         1.9913e-02, -3.9965e-02,  7.6919e-02,  1.1498e-01, -6.5787e-02,\n        -8.4971e-02,  1.2668e-01,  2.0365e-01,  1.7946e-01, -1.2953e-01,\n         1.9260e-01, -9.6302e-02, -7.5193e-02,  6.4110e-02, -1.9202e-02,\n         1.5212e-01,  1.6503e-01,  1.4749e-01, -7.5665e-02, -2.3061e-01,\n         1.2124e-01,  8.7380e-02, -6.8148e-02,  1.9631e-02,  4.9709e-02,\n         5.2993e-02,  1.6483e-01,  1.1459e-01, -3.6108e-02,  1.9317e-01,\n        -4.4186e-02, -2.4532e-02, -2.2075e-01,  5.7838e-03,  1.9950e-01,\n         9.2932e-02, -9.3798e-02, -3.0699e-02,  3.2803e-02,  1.3139e-01,\n        -1.0147e-01, -7.2373e-02,  4.8918e-02,  8.4615e-02, -1.0287e-01,\n        -4.7278e-03,  1.2601e-01, -5.6927e-02,  2.2257e-01, -6.5842e-02,\n        -1.2036e-01,  9.9820e-02,  2.4243e-02, -1.0320e-02,  8.8519e-02,\n         1.2967e-02,  4.3215e-02, -1.6222e-01, -4.6659e-02, -5.8839e-02,\n         2.0254e-02,  1.3470e-01,  2.6904e-01,  8.3832e-02,  1.4001e-01,\n        -5.6560e-02, -1.7141e-01,  3.0431e-02,  3.0826e-02,  1.1963e-01,\n        -2.8436e-02, -4.2731e-02,  7.2354e-02,  1.9034e-02, -7.6484e-02,\n         5.2238e-02, -1.0509e-01,  9.1314e-02,  4.5333e-02,  2.1825e-01,\n        -1.6899e-01, -2.5924e-03, -6.3096e-02, -1.6934e-01, -1.1294e-01,\n        -8.2351e-02,  1.5471e-01, -6.3658e-02,  1.9206e-02,  7.1285e-02,\n        -1.9484e-01,  1.1433e-01,  2.1561e-01, -1.2322e-01,  5.6644e-02,\n         7.1328e-02, -1.2183e-01, -8.9327e-02,  1.0203e-02,  8.1200e-04,\n        -7.3808e-02, -2.0901e-01, -4.5195e-02, -1.3261e-04, -5.7953e-02,\n         6.8481e-02, -1.2712e-01, -1.2984e-01,  1.4874e-02,  4.8432e-02,\n         7.5876e-02, -3.8336e-02,  1.6079e-02], device='cuda:0',\n       grad_fn=&lt;ReshapeAliasBackward0&gt;), 'fc3.weight': tensor([[ 0.0396, -0.1677,  0.0536,  ...,  0.0329,  0.1299,  0.0670],\n        [-0.2666, -0.1038, -0.2126,  ...,  0.0804, -0.1728,  0.1918],\n        [ 0.1665,  0.0134, -0.0402,  ..., -0.0687,  0.1080,  0.0771],\n        ...,\n        [ 0.1816, -0.0331,  0.0454,  ..., -0.0926,  0.1632,  0.0675],\n        [-0.0477,  0.2884,  0.0700,  ..., -0.0688, -0.2281,  0.1430],\n        [-0.1463,  0.1891, -0.1439,  ...,  0.0600, -0.0703, -0.1880]],\n       device='cuda:0', grad_fn=&lt;ReshapeAliasBackward0&gt;), 'fc3.bias': tensor([-1.6453e-01,  7.5851e-02,  3.7721e-02, -9.4238e-03, -7.1286e-02,\n         2.2447e-01,  3.8425e-02,  1.0008e-01, -1.5634e-01,  8.8691e-02,\n         1.2365e-02,  1.4615e-01, -5.4554e-02, -4.8148e-02, -6.4282e-02,\n         1.0527e-02, -3.6634e-02,  1.5034e-02, -9.3311e-03,  1.7128e-01,\n        -1.8236e-01,  1.3478e-01,  1.7017e-01, -7.1820e-02,  1.4992e-01,\n        -4.0661e-01,  9.0499e-02, -3.6053e-01,  2.5526e-01, -8.1079e-02,\n        -7.7242e-02,  8.8858e-02, -1.4380e-01,  2.5731e-01,  8.9234e-03,\n         5.1684e-02,  4.9633e-02,  1.3959e-01, -1.2853e-01, -7.9164e-02,\n         6.9847e-02,  1.0135e-01, -1.1189e-01, -3.8928e-02, -2.0292e-02,\n         1.1789e-02,  1.4188e-01, -1.8961e-01, -2.3442e-02, -2.2014e-01,\n        -1.2734e-01,  2.7420e-03,  1.3705e-02, -2.0665e-01, -5.5958e-03,\n        -7.3088e-02,  2.3228e-02, -3.1873e-01, -4.0341e-02, -1.1505e-01,\n         3.2668e-02,  8.3553e-02,  8.1658e-03, -1.6962e-01, -1.2170e-01,\n         1.0409e-01, -2.6209e-01, -9.5062e-02,  8.3047e-02,  1.3676e-02,\n        -1.9911e-02, -2.3725e-01, -1.3274e-01, -9.1445e-03,  1.6600e-01,\n        -1.8830e-02,  9.9199e-03,  1.3318e-01,  8.1056e-02, -1.8402e-02,\n        -5.7780e-05,  1.6435e-01,  1.0902e-01,  3.4496e-02, -1.5712e-01,\n        -9.2133e-03,  1.7642e-01,  9.4327e-02, -7.3375e-02, -1.2195e-02,\n         4.0908e-02, -1.3142e-01,  6.7244e-02,  2.5039e-02, -4.3484e-02,\n        -8.8210e-02, -1.8980e-02, -2.9393e-02,  2.2965e-01,  6.2888e-02,\n         1.8778e-01,  2.0747e-01, -2.7634e-01,  8.7129e-02,  1.3946e-01,\n        -2.0265e-01,  2.0881e-01,  1.2091e-01,  3.0196e-02,  2.2240e-02,\n         1.3639e-01,  1.6089e-01, -8.9004e-02, -3.3855e-03,  5.0107e-02,\n         1.5593e-01,  6.7469e-02,  6.8105e-02,  8.7053e-02,  2.0969e-01,\n         1.1700e-01, -1.6770e-02,  4.2229e-02,  4.6535e-02, -9.8295e-02,\n         1.9084e-01, -4.1850e-02, -4.6306e-02], device='cuda:0',\n       grad_fn=&lt;ReshapeAliasBackward0&gt;), 'fc4.weight': tensor([[ 0.2830, -0.1100, -0.1323,  ..., -0.3271,  0.0813, -0.0061],\n        [ 0.2710,  0.3222,  0.0848,  ...,  0.2513, -0.0644, -0.2402],\n        [-0.0105,  0.0584,  0.0086,  ...,  0.0149,  0.0471,  0.1521],\n        ...,\n        [ 0.1835,  0.0269, -0.0054,  ..., -0.2250,  0.0190, -0.0436],\n        [ 0.0104, -0.1079,  0.0619,  ..., -0.0457, -0.1928,  0.1140],\n        [ 0.0292,  0.1185,  0.0900,  ...,  0.0677,  0.1507, -0.2381]],\n       device='cuda:0', grad_fn=&lt;ReshapeAliasBackward0&gt;), 'fc4.bias': tensor([-1.6619e-01, -1.1489e-01, -1.0995e-02, -1.8886e-01,  8.1446e-02,\n        -1.7253e-01,  4.6207e-02, -1.2412e-01,  2.5603e-01, -2.4989e-01,\n         1.0106e-01,  1.4181e-01,  1.1908e-01, -1.3170e-01, -1.6666e-02,\n         6.9981e-02, -9.4864e-02,  2.9104e-02,  3.3937e-02,  8.8687e-02,\n        -2.1460e-01,  1.7662e-02, -7.4605e-02, -2.8766e-01,  1.4387e-03,\n         8.8849e-02, -2.6133e-01,  2.3944e-01,  4.2308e-01, -2.2386e-02,\n        -3.5002e-02,  1.5050e-01, -1.8755e-01,  4.0275e-02,  5.0206e-02,\n         2.8535e-02,  1.5281e-01,  1.5107e-01,  1.1838e-02, -1.9254e-01,\n        -1.6728e-01,  1.3601e-01, -9.4265e-02, -3.5805e-02, -1.3126e-01,\n        -7.3716e-02,  4.6302e-02, -5.5326e-02,  8.6940e-02, -2.5061e-01,\n         7.0305e-02,  7.3881e-02, -1.4083e-02,  1.8914e-01,  3.1738e-02,\n        -2.8057e-02,  1.0428e-01, -4.5615e-02,  7.6921e-03, -1.8016e-01,\n         8.4051e-05, -1.8171e-01, -1.7909e-02, -1.4178e-02,  5.4015e-02,\n         2.5439e-01, -2.7733e-01,  2.3340e-01, -1.2361e-01, -2.2032e-01,\n        -8.9306e-02,  6.3847e-02, -7.4009e-02, -4.2775e-02, -1.4546e-01,\n         5.9691e-02, -2.3814e-01, -4.3989e-02, -1.7065e-01,  2.3011e-01,\n        -1.9524e-02, -8.2356e-02,  1.6535e-01,  1.1677e-01, -1.7481e-01,\n        -2.4286e-02, -5.4737e-02, -9.8581e-03,  5.8233e-02, -2.0740e-02,\n         3.7968e-02, -5.8098e-02,  3.2286e-02, -4.9859e-02,  1.3212e-02,\n        -1.4157e-01,  3.6066e-02, -7.8502e-03, -1.1150e-01,  1.7404e-01,\n         2.9603e-02, -6.5617e-02,  5.7098e-02,  4.4060e-02,  1.1591e-01,\n        -1.1777e-01, -2.4960e-01, -2.0873e-01, -5.8239e-02, -1.9206e-01,\n         5.1293e-02, -1.4662e-01, -1.0294e-01, -1.4170e-01,  1.0213e-01,\n        -1.0966e-01,  6.4190e-02, -1.2977e-02,  1.3461e-01,  9.0853e-02,\n         1.3335e-01, -1.0664e-01,  2.0796e-01, -1.0372e-01,  1.2097e-02,\n        -1.0545e-01,  4.7369e-02,  4.7679e-02], device='cuda:0',\n       grad_fn=&lt;ReshapeAliasBackward0&gt;), 'fc5.weight': tensor([[ 3.7005e-02,  1.7218e-02, -6.0654e-02,  1.9607e-01,  1.8402e-01,\n          7.0345e-02,  1.0801e-01,  2.0575e-02, -6.2460e-02, -2.6926e-02,\n          3.2469e-02, -5.7114e-02,  2.4663e-02, -8.2723e-02,  1.5335e-01,\n          2.5052e-01,  1.7047e-01,  3.2905e-02,  4.4214e-03,  1.0149e-01,\n          2.4933e-01,  1.0568e-01, -9.2186e-02,  1.0348e-01,  9.7047e-02,\n          2.4081e-02,  4.2537e-02, -1.3451e-01, -9.5488e-02,  2.5056e-02,\n          8.5907e-02, -3.5518e-01, -1.8330e-01, -2.0032e-01,  5.3350e-03,\n          8.3568e-02, -7.8669e-02, -4.0999e-02, -1.6546e-01,  7.6125e-02,\n          4.2919e-02, -1.1224e-01,  6.6079e-02,  7.5957e-02,  1.6435e-01,\n         -6.5331e-02, -1.4222e-01, -2.3253e-02, -2.4421e-02, -4.7380e-02,\n         -3.1845e-02,  1.4371e-01,  1.6564e-02, -9.1425e-02, -2.3566e-01,\n          2.4497e-02, -1.0897e-02, -6.5980e-02, -6.4120e-02, -4.8469e-02,\n          1.9976e-01,  7.1650e-02, -3.1568e-02, -7.8932e-02, -1.0762e-02,\n         -5.2840e-02,  1.5349e-01, -1.6643e-01, -1.1844e-01,  1.0926e-01,\n         -2.5987e-02, -2.4647e-01,  1.2376e-01,  1.1220e-02, -8.4029e-02,\n         -9.8139e-02,  5.3532e-02, -1.4702e-02,  1.4518e-01,  3.3054e-02,\n         -1.3509e-01, -1.8362e-01,  4.7140e-02,  7.5887e-03,  4.8157e-02,\n          1.6827e-01,  1.7298e-01, -1.3073e-01,  2.5464e-01,  8.3428e-02,\n         -1.0881e-01,  1.6493e-01,  2.8744e-03,  7.8934e-03,  8.0214e-02,\n         -4.2423e-02,  1.0871e-01,  6.9979e-02, -8.5941e-02,  1.5277e-01,\n          1.2681e-06, -1.6246e-01, -4.9869e-02, -7.0287e-02, -1.6674e-03,\n         -1.2628e-01,  7.2879e-02, -2.6658e-01, -4.3385e-02, -1.3041e-02,\n          1.3853e-01,  9.2652e-02,  1.3920e-01,  1.6977e-01, -8.1156e-02,\n          3.3611e-02,  1.9832e-02,  3.4340e-02, -1.0543e-02, -5.9265e-02,\n         -1.3928e-01, -1.9022e-02,  1.0770e-01,  6.6137e-02, -1.3821e-01,\n         -8.6274e-02, -1.2282e-01, -4.3384e-02],\n        [ 1.5446e-01, -1.2482e-01, -3.1733e-01, -5.0405e-02, -9.8563e-02,\n          1.4524e-02, -4.7723e-02, -2.1366e-01,  1.2520e-02,  9.3535e-02,\n         -1.7930e-02,  8.1626e-03, -7.2552e-02, -1.7400e-01, -4.8571e-03,\n          3.4715e-02, -1.6996e-01,  2.5612e-02,  1.7753e-02,  1.1491e-01,\n          3.2849e-02,  7.4169e-02,  2.3232e-02,  2.3961e-01, -1.4371e-01,\n          7.5394e-02, -5.6723e-03,  1.4591e-02, -1.0227e-01, -6.3025e-02,\n         -4.8187e-02,  3.7513e-02, -2.2861e-02, -2.5060e-02, -5.2416e-02,\n          3.6575e-02,  4.3657e-02,  3.9590e-02,  6.8889e-02,  7.8860e-02,\n         -6.1261e-02,  6.4804e-03,  6.4698e-02, -1.3405e-01,  4.9579e-02,\n          9.1552e-02,  2.5240e-01, -1.1347e-01,  4.2408e-02,  8.2981e-02,\n          1.1758e-01,  1.4038e-02, -5.8177e-02, -4.6107e-02,  9.8306e-02,\n          5.3295e-02, -1.2327e-02, -3.7325e-02,  2.1046e-01,  1.1949e-01,\n         -8.8206e-03,  9.5156e-02,  8.3061e-03,  8.0187e-02,  4.3940e-02,\n          1.4411e-01, -5.8862e-03,  1.5729e-02, -1.7983e-01,  5.2839e-02,\n          5.6460e-02, -1.3105e-01,  2.7025e-01,  4.4835e-02, -1.3793e-01,\n          4.7627e-02, -3.9222e-02, -6.4474e-02,  1.2132e-01, -5.4602e-02,\n         -4.1981e-02, -1.7641e-01,  1.1443e-01,  1.6024e-01, -4.2657e-02,\n          2.9715e-02, -6.2705e-03,  7.2440e-02, -3.1999e-01, -1.6657e-01,\n         -9.7144e-02,  7.7817e-02, -4.3598e-02,  1.5082e-03,  5.5090e-02,\n         -3.5226e-02,  7.4446e-02, -1.4492e-01,  1.9284e-01, -1.0586e-01,\n          9.7238e-02, -2.9331e-01,  1.6685e-01,  6.7393e-02, -3.8107e-02,\n         -5.1379e-03, -1.8144e-01, -1.0758e-01, -3.5332e-02, -9.5876e-02,\n         -3.3853e-02,  7.4512e-02,  7.1593e-02,  5.6300e-02, -2.2599e-01,\n         -2.6461e-02, -9.8212e-02, -1.3220e-01,  2.4089e-03, -2.6224e-01,\n         -4.1655e-02, -1.2883e-03, -1.4610e-01,  7.1842e-02,  1.6349e-01,\n         -3.0933e-01,  2.8165e-02,  4.9211e-02],\n        [ 1.6185e-02, -4.3435e-02,  2.9302e-01,  6.8762e-02, -1.0513e-01,\n          8.0041e-03,  3.1691e-02,  7.8240e-02, -3.0865e-01, -1.2082e-01,\n         -1.5014e-02, -6.4405e-02,  3.7052e-03,  1.5633e-01, -8.7142e-02,\n          2.7108e-01, -4.8148e-02,  1.9585e-01, -1.0857e-01, -1.6191e-02,\n          1.1271e-01, -4.2816e-02,  1.1897e-01,  2.1357e-01,  6.0726e-03,\n         -1.6058e-01,  1.0014e-01,  7.2932e-03, -3.3023e-02, -1.3890e-02,\n          2.3372e-01, -1.5602e-01, -1.3258e-01,  3.4258e-02,  4.1946e-03,\n         -4.5052e-02,  1.0765e-01,  8.8289e-02,  5.0133e-02,  1.0398e-01,\n          1.7352e-02, -1.1756e-01,  1.2870e-01,  9.9371e-02,  1.0367e-01,\n         -7.6240e-02, -6.7554e-02,  2.2970e-02,  1.8892e-01, -8.2191e-02,\n          1.6877e-01, -7.5892e-03,  4.1152e-02,  6.9047e-02,  1.0306e-01,\n          2.3320e-01,  1.2969e-01, -2.9587e-01,  2.2722e-01, -1.7229e-01,\n         -7.0078e-02,  6.9529e-02,  1.5175e-01,  4.4102e-02, -1.8311e-01,\n          4.8507e-03, -1.8906e-01,  1.3765e-01,  8.4732e-02, -8.0962e-02,\n         -6.6467e-02, -4.6670e-02, -1.5538e-02, -2.2635e-01,  2.1714e-01,\n          2.3420e-01, -1.2549e-01,  1.6654e-02,  7.8636e-02, -5.5591e-02,\n          1.5804e-01, -1.9898e-03, -1.6906e-01, -1.9567e-02,  3.1766e-02,\n          9.4322e-03, -7.3489e-02, -2.7695e-02,  1.2616e-01,  1.7542e-01,\n          1.9282e-01, -1.0990e-01, -2.4401e-01,  7.7423e-02, -1.5268e-01,\n         -3.8916e-03,  6.2967e-02,  1.1859e-01,  8.4724e-02, -1.8199e-01,\n         -7.2396e-02, -6.9568e-02, -7.0842e-02, -9.4516e-02,  2.1872e-01,\n          2.2665e-01, -1.6960e-01, -2.2812e-01,  1.2222e-01,  1.3715e-01,\n         -4.3735e-02, -7.3953e-02, -9.7945e-02, -2.6904e-02,  8.7878e-02,\n         -1.0421e-02,  1.6984e-02,  3.8456e-02,  1.7513e-01, -1.0124e-03,\n          3.8656e-02,  1.4228e-01,  9.2844e-03,  2.9087e-02,  5.2339e-02,\n         -1.9062e-01, -1.0616e-01,  1.0397e-01]], device='cuda:0',\n       grad_fn=&lt;ReshapeAliasBackward0&gt;), 'fc5.bias': tensor([-0.0278, -0.0784, -0.0022], device='cuda:0',\n       grad_fn=&lt;ReshapeAliasBackward0&gt;)}\n\n\n\nunravel_fn(weights_flattened)\n\n{'fc1.weight': tensor([[ 8.4120e-02,  3.9265e-02],\n         [-4.4006e-02, -5.9050e-03],\n         [-1.8446e-01,  1.7159e-01],\n         [ 1.2318e-02, -1.6082e-02],\n         [-1.2422e-01, -1.2124e-01],\n         [ 1.5859e-01, -6.5549e-02],\n         [-6.1395e-02,  1.4970e-01],\n         [-1.4709e-01,  3.1681e-02],\n         [ 3.6000e-02, -1.8523e-02],\n         [ 1.4831e-01, -1.5797e-01],\n         [ 1.8725e-01,  3.8700e-02],\n         [-1.8377e-01, -5.8808e-02],\n         [ 1.0143e-02, -1.4246e-01],\n         [-1.4042e-01, -5.2038e-05],\n         [ 2.3339e-01,  8.5529e-02],\n         [-1.0926e-01,  9.3398e-03],\n         [ 2.6608e-02,  4.7951e-02],\n         [-1.4424e-01, -2.1638e-01],\n         [-2.6549e-02,  2.2673e-01],\n         [ 6.6978e-02,  2.1663e-01],\n         [ 1.6812e-01, -1.4362e-01],\n         [ 1.8123e-01, -1.0973e-02],\n         [-9.9815e-03, -6.4446e-02],\n         [ 1.0362e-02, -1.1157e-01],\n         [ 1.9818e-02,  3.1765e-03],\n         [-4.1822e-02,  2.9222e-02],\n         [ 3.5510e-04, -9.5421e-02],\n         [ 4.0710e-02, -3.6539e-03],\n         [ 8.6873e-02,  2.0872e-01],\n         [-1.2687e-01,  1.7474e-01],\n         [ 1.4681e-01,  3.6409e-02],\n         [ 6.9398e-02,  4.4749e-02],\n         [-3.4701e-01, -3.8960e-02],\n         [ 2.0324e-01,  6.4354e-02],\n         [-2.0250e-01, -7.3486e-02],\n         [ 2.5662e-01, -6.7429e-02],\n         [ 9.0947e-03,  6.0853e-03],\n         [-1.3005e-01, -1.3669e-01],\n         [-1.4868e-01, -4.5554e-03],\n         [ 1.5069e-01,  8.6696e-02],\n         [ 4.6468e-02,  6.9869e-02],\n         [-1.2077e-01, -3.3872e-03],\n         [-5.2897e-03, -1.9553e-01],\n         [ 1.2461e-01, -4.0207e-03],\n         [-1.0743e-01,  1.4018e-01],\n         [ 2.1903e-01, -1.1876e-01],\n         [ 3.3978e-02, -5.6171e-03],\n         [-1.6130e-01, -7.7339e-02],\n         [ 1.7324e-01, -9.7195e-02],\n         [ 4.2971e-02,  1.0253e-01],\n         [-1.0369e-02, -4.1130e-02],\n         [-1.1557e-01, -1.0589e-01],\n         [ 1.8067e-01,  1.6982e-01],\n         [ 4.1407e-02, -2.6240e-02],\n         [ 6.1280e-02,  2.0380e-01],\n         [ 2.0553e-02, -1.8692e-01],\n         [ 5.4052e-02,  7.0528e-03],\n         [ 6.3718e-02,  1.0462e-01],\n         [ 6.9697e-02, -7.6473e-02],\n         [ 7.3403e-02,  7.8244e-02],\n         [-1.3281e-01, -7.3169e-04],\n         [ 1.2163e-01, -1.2142e-01],\n         [-1.1956e-01,  2.6188e-02],\n         [-2.0094e-01, -3.3400e-03],\n         [ 1.3937e-01,  9.1031e-02],\n         [ 1.1908e-01, -5.2545e-02],\n         [-9.3397e-02, -2.5996e-03],\n         [ 5.6113e-02,  8.7695e-02],\n         [ 9.4380e-02,  5.8306e-02],\n         [-5.8491e-02, -9.4788e-02],\n         [-1.0383e-01,  1.8825e-01],\n         [ 6.4387e-02, -6.8692e-02],\n         [-5.7511e-02, -1.0626e-01],\n         [ 1.8366e-01,  9.3314e-02],\n         [-1.5770e-01, -1.1771e-01],\n         [ 1.8512e-02, -1.0166e-01],\n         [ 1.1778e-01,  6.4702e-03],\n         [ 2.0301e-01, -2.4718e-02],\n         [ 1.1473e-02, -5.9337e-02],\n         [ 4.5290e-02, -2.7956e-04],\n         [ 7.4153e-02,  7.4455e-02],\n         [ 1.1620e-01,  9.1790e-02],\n         [-2.6550e-01,  4.0917e-02],\n         [-4.9924e-02,  3.5913e-01],\n         [ 2.3945e-01, -1.3163e-01],\n         [ 7.0677e-02,  4.3314e-02],\n         [ 5.4965e-02, -3.8266e-02],\n         [ 1.1958e-01,  1.9800e-02],\n         [-3.2567e-02, -7.0675e-02],\n         [ 7.4658e-02,  1.3528e-01],\n         [ 6.7624e-02,  1.3752e-01],\n         [ 1.4267e-01,  8.1409e-02],\n         [ 1.5277e-01, -9.4721e-02],\n         [ 9.5535e-03, -2.7910e-02],\n         [-6.1356e-02,  9.8481e-02],\n         [ 1.7242e-01, -2.3863e-02],\n         [-8.4674e-02, -5.6868e-03],\n         [ 1.4529e-01, -1.4019e-01],\n         [-2.7921e-02,  2.1329e-01],\n         [-4.0568e-02, -1.3940e-01],\n         [ 3.9279e-02,  3.4861e-02],\n         [ 1.6734e-01, -2.2712e-02],\n         [ 1.0885e-01,  1.1623e-01],\n         [ 1.6150e-01, -1.8630e-02],\n         [-7.1212e-02, -1.4402e-01],\n         [ 1.5908e-01, -2.2806e-01],\n         [-4.3496e-02,  1.4644e-01],\n         [-2.2250e-02, -5.0974e-02],\n         [-1.5041e-01, -1.0052e-01],\n         [-3.0135e-02,  7.5585e-02],\n         [-5.2927e-02, -1.2286e-01],\n         [ 1.0296e-02,  1.0396e-01],\n         [-1.3909e-01,  1.1614e-02],\n         [ 1.0974e-03,  2.1793e-01],\n         [-2.5740e-02, -9.2893e-02],\n         [ 4.9473e-02, -1.8746e-01],\n         [ 4.2226e-02,  1.7077e-01],\n         [ 1.5371e-01,  1.2752e-01],\n         [-7.0501e-02, -1.9778e-03],\n         [ 4.9510e-02,  2.5706e-02],\n         [-5.7692e-02,  2.4992e-01],\n         [-1.6351e-01,  6.7651e-02],\n         [-7.1454e-02, -1.0196e-01],\n         [ 6.8088e-02, -1.8465e-01],\n         [ 5.1298e-03,  1.0726e-02],\n         [ 6.1477e-03,  2.7650e-01],\n         [-1.9581e-02, -1.2533e-01],\n         [ 1.0733e-01, -3.6380e-02]], device='cuda:0',\n        grad_fn=&lt;ReshapeAliasBackward0&gt;),\n 'fc1.bias': tensor([ 0.0723,  0.1847, -0.0069,  0.0122, -0.1943,  0.1156,  0.0374,  0.0360,\n          0.0358,  0.0903,  0.0724,  0.0239,  0.0676, -0.1157, -0.1852, -0.0209,\n          0.2367, -0.0537,  0.1319, -0.0432, -0.1779, -0.3584,  0.1164,  0.1021,\n          0.0361, -0.1298, -0.0899,  0.3074,  0.0901, -0.1302,  0.2014, -0.1362,\n          0.1097,  0.1125,  0.0212,  0.0367, -0.0703,  0.1788, -0.0799, -0.0096,\n         -0.0012, -0.0678, -0.0614,  0.1798,  0.0877,  0.0767,  0.0112, -0.1888,\n         -0.0694,  0.1392,  0.1951,  0.1511,  0.1052,  0.0167, -0.0712, -0.0506,\n         -0.0346, -0.0385, -0.0656,  0.1426, -0.0630,  0.0468,  0.1634, -0.1161,\n          0.0537,  0.0200,  0.0444, -0.1248, -0.0696,  0.0193,  0.2154, -0.1409,\n         -0.0474, -0.1533, -0.0554, -0.0076,  0.1127, -0.0333,  0.2621, -0.0191,\n         -0.0227, -0.0550,  0.0126,  0.1161, -0.0172, -0.0711,  0.0292, -0.1158,\n         -0.0141, -0.0684,  0.0173, -0.1705, -0.0579,  0.1625, -0.2737, -0.1592,\n          0.1245, -0.0372,  0.0141, -0.2497, -0.1439,  0.1571,  0.1649,  0.0975,\n         -0.1605,  0.0610,  0.0287,  0.0312, -0.2894,  0.1572,  0.2392,  0.1032,\n         -0.0339, -0.3047,  0.0068,  0.0680,  0.3220, -0.0996,  0.0834,  0.0428,\n         -0.1010,  0.0792, -0.1126,  0.0308,  0.0697,  0.0592,  0.2190, -0.2508],\n        device='cuda:0', grad_fn=&lt;ReshapeAliasBackward0&gt;),\n 'fc2.weight': tensor([[ 0.1144, -0.0025, -0.1852,  ..., -0.1083, -0.1402, -0.0354],\n         [-0.1914, -0.0494, -0.0735,  ..., -0.0817,  0.0393,  0.0877],\n         [-0.2834,  0.0507,  0.0576,  ...,  0.0827, -0.0266,  0.1160],\n         ...,\n         [ 0.1178, -0.1181,  0.0901,  ...,  0.0402,  0.1107,  0.0868],\n         [ 0.1341,  0.0799,  0.1080,  ...,  0.0148,  0.0278,  0.1365],\n         [ 0.0267, -0.0451, -0.1338,  ...,  0.0759, -0.0383,  0.0161]],\n        device='cuda:0', grad_fn=&lt;ReshapeAliasBackward0&gt;),\n 'fc2.bias': tensor([ 0.1626,  0.1576,  0.0090,  0.2106, -0.2146,  0.0891,  0.0409, -0.0352,\n          0.1271,  0.1234, -0.2810,  0.0872,  0.0070,  0.0727,  0.1334, -0.0895,\n          0.0315, -0.0023, -0.0810, -0.1109, -0.2472, -0.1886, -0.1106, -0.0783,\n          0.0280, -0.0559, -0.1411, -0.1457, -0.1365, -0.1305,  0.0753,  0.2940,\n          0.0084,  0.0858, -0.0895, -0.1531, -0.2004, -0.0533, -0.0228, -0.0384,\n         -0.0674,  0.1116,  0.1309, -0.0194, -0.2964,  0.0495,  0.0392,  0.0084,\n         -0.3178, -0.0079, -0.0193, -0.0915,  0.0934,  0.0513,  0.1537, -0.1062,\n         -0.0009,  0.1223,  0.1528, -0.1958, -0.0226,  0.1976,  0.2536, -0.0666,\n         -0.0249,  0.0364,  0.1468, -0.1163, -0.1548, -0.2458, -0.0641, -0.2811,\n          0.1335, -0.0726, -0.0971,  0.0742,  0.0757,  0.1187, -0.0358,  0.1663,\n         -0.0367, -0.1590, -0.1435,  0.0068, -0.1081, -0.0447,  0.0111,  0.1300,\n          0.0330, -0.1142, -0.0675,  0.0358, -0.0160,  0.1685, -0.1855,  0.0587,\n         -0.0684, -0.0809,  0.2065, -0.0541, -0.0810, -0.2434, -0.0082,  0.1038,\n         -0.1389,  0.0092, -0.0100,  0.1909, -0.0241,  0.2466,  0.2520, -0.0404,\n         -0.0489, -0.0476,  0.0276, -0.1169,  0.0349,  0.1722,  0.1544,  0.1706,\n          0.1735, -0.1790, -0.1286, -0.0445,  0.0327,  0.0867, -0.0421, -0.0025],\n        device='cuda:0', grad_fn=&lt;ReshapeAliasBackward0&gt;),\n 'fc3.weight': tensor([[-0.2666, -0.1038, -0.2126,  ...,  0.0804, -0.1728,  0.1918],\n         [ 0.1665,  0.0134, -0.0402,  ..., -0.0687,  0.1080,  0.0771],\n         [ 0.0048, -0.0303,  0.0350,  ..., -0.0155, -0.0594, -0.0849],\n         ...,\n         [-0.0477,  0.2884,  0.0700,  ..., -0.0688, -0.2281,  0.1430],\n         [-0.1463,  0.1891, -0.1439,  ...,  0.0600, -0.0703, -0.1880],\n         [-0.1645,  0.0759,  0.0377,  ...,  0.1908, -0.0419, -0.0463]],\n        device='cuda:0', grad_fn=&lt;ReshapeAliasBackward0&gt;),\n 'fc3.bias': tensor([ 0.0396, -0.1677,  0.0536, -0.0781,  0.0929,  0.1182, -0.0727, -0.2348,\n         -0.0364,  0.0144, -0.0160, -0.1700, -0.1743,  0.1426,  0.0120,  0.0539,\n         -0.0199,  0.0310,  0.0724,  0.1968, -0.0554, -0.1410,  0.0416,  0.0742,\n          0.0453,  0.0107,  0.0289, -0.0609,  0.1151, -0.0019,  0.1392, -0.1715,\n         -0.1035, -0.1238, -0.2124,  0.1473, -0.1585,  0.0812, -0.0525, -0.1321,\n         -0.0063,  0.1354,  0.1208,  0.1932, -0.0749,  0.0256,  0.3203, -0.1646,\n         -0.1285, -0.0764,  0.0970, -0.0485, -0.0683,  0.1397,  0.0028,  0.0345,\n         -0.0697,  0.0072, -0.0970, -0.1684, -0.1472, -0.0457, -0.0492, -0.0625,\n         -0.0167, -0.1218,  0.1252,  0.0084,  0.0421,  0.0914,  0.0664,  0.1713,\n          0.1204,  0.1472, -0.1081,  0.0435, -0.0677, -0.0565,  0.0609,  0.0308,\n         -0.0063, -0.0176,  0.0553,  0.0903,  0.1117,  0.0311, -0.0477, -0.0769,\n          0.0113,  0.0810,  0.0353,  0.1450,  0.0064, -0.1846, -0.0507, -0.0900,\n         -0.1156,  0.0205,  0.2363, -0.0141, -0.2917, -0.1116, -0.0367,  0.0775,\n         -0.1252,  0.1363, -0.1238, -0.1238,  0.0410,  0.1691,  0.0815, -0.1077,\n          0.0502, -0.1703, -0.1145,  0.0335, -0.0765, -0.1086,  0.0892,  0.2044,\n          0.1037,  0.1099,  0.2079, -0.0267,  0.1770,  0.0329,  0.1299,  0.0670],\n        device='cuda:0', grad_fn=&lt;ReshapeAliasBackward0&gt;),\n 'fc4.weight': tensor([[ 0.2710,  0.3222,  0.0848,  ...,  0.2513, -0.0644, -0.2402],\n         [-0.0105,  0.0584,  0.0086,  ...,  0.0149,  0.0471,  0.1521],\n         [ 0.0950, -0.1262, -0.2527,  ..., -0.0147, -0.0260, -0.0571],\n         ...,\n         [ 0.0104, -0.1079,  0.0619,  ..., -0.0457, -0.1928,  0.1140],\n         [ 0.0292,  0.1185,  0.0900,  ...,  0.0677,  0.1507, -0.2381],\n         [-0.1662, -0.1149, -0.0110,  ..., -0.1055,  0.0474,  0.0477]],\n        device='cuda:0', grad_fn=&lt;ReshapeAliasBackward0&gt;),\n 'fc4.bias': tensor([ 0.2830, -0.1100, -0.1323,  0.0350,  0.0379,  0.1048,  0.0118, -0.0220,\n         -0.1217,  0.0384,  0.0729,  0.0511,  0.0749,  0.0740, -0.2084,  0.1948,\n          0.1429, -0.0998,  0.0815, -0.1628,  0.0771,  0.0368,  0.2225, -0.0697,\n         -0.0751,  0.0843, -0.1492,  0.0615, -0.0533,  0.0109, -0.1530,  0.0756,\n         -0.2430,  0.0396,  0.0362, -0.2492, -0.1522,  0.0886, -0.0118, -0.0884,\n          0.1067,  0.0107,  0.1849,  0.0924,  0.1142, -0.0098,  0.0068, -0.0713,\n         -0.0696,  0.0380, -0.1443, -0.1019, -0.0137, -0.1282,  0.0116,  0.2317,\n         -0.0611,  0.0513, -0.0315,  0.0154,  0.1059,  0.1193,  0.0808, -0.0066,\n         -0.2554, -0.0068,  0.1554, -0.1032, -0.0570, -0.0236,  0.0222,  0.1330,\n         -0.0821, -0.2539, -0.1511, -0.0738, -0.0669, -0.2200,  0.1576, -0.1148,\n         -0.0310, -0.0278,  0.0835, -0.0322, -0.0552, -0.0263,  0.1971,  0.3168,\n         -0.0859,  0.0482, -0.1642,  0.0771, -0.1242, -0.0020,  0.1226, -0.0466,\n          0.0177, -0.1199,  0.0924,  0.0138, -0.0817, -0.0337, -0.0027,  0.0997,\n         -0.0822,  0.0004,  0.0043, -0.0923, -0.0192, -0.0081, -0.1371, -0.0770,\n         -0.1474,  0.0268,  0.0800, -0.0389,  0.0587,  0.0421,  0.1361,  0.0970,\n          0.0347, -0.1867, -0.0402, -0.1091, -0.1063, -0.3271,  0.0813, -0.0061],\n        device='cuda:0', grad_fn=&lt;ReshapeAliasBackward0&gt;),\n 'fc5.weight': tensor([[ 1.9607e-01,  1.8402e-01,  7.0345e-02,  1.0801e-01,  2.0575e-02,\n          -6.2460e-02, -2.6926e-02,  3.2469e-02, -5.7114e-02,  2.4663e-02,\n          -8.2723e-02,  1.5335e-01,  2.5052e-01,  1.7047e-01,  3.2905e-02,\n           4.4214e-03,  1.0149e-01,  2.4933e-01,  1.0568e-01, -9.2186e-02,\n           1.0348e-01,  9.7047e-02,  2.4081e-02,  4.2537e-02, -1.3451e-01,\n          -9.5488e-02,  2.5056e-02,  8.5907e-02, -3.5518e-01, -1.8330e-01,\n          -2.0032e-01,  5.3350e-03,  8.3568e-02, -7.8669e-02, -4.0999e-02,\n          -1.6546e-01,  7.6125e-02,  4.2919e-02, -1.1224e-01,  6.6079e-02,\n           7.5957e-02,  1.6435e-01, -6.5331e-02, -1.4222e-01, -2.3253e-02,\n          -2.4421e-02, -4.7380e-02, -3.1845e-02,  1.4371e-01,  1.6564e-02,\n          -9.1425e-02, -2.3566e-01,  2.4497e-02, -1.0897e-02, -6.5980e-02,\n          -6.4120e-02, -4.8469e-02,  1.9976e-01,  7.1650e-02, -3.1568e-02,\n          -7.8932e-02, -1.0762e-02, -5.2840e-02,  1.5349e-01, -1.6643e-01,\n          -1.1844e-01,  1.0926e-01, -2.5987e-02, -2.4647e-01,  1.2376e-01,\n           1.1220e-02, -8.4029e-02, -9.8139e-02,  5.3532e-02, -1.4702e-02,\n           1.4518e-01,  3.3054e-02, -1.3509e-01, -1.8362e-01,  4.7140e-02,\n           7.5887e-03,  4.8157e-02,  1.6827e-01,  1.7298e-01, -1.3073e-01,\n           2.5464e-01,  8.3428e-02, -1.0881e-01,  1.6493e-01,  2.8744e-03,\n           7.8934e-03,  8.0214e-02, -4.2423e-02,  1.0871e-01,  6.9979e-02,\n          -8.5941e-02,  1.5277e-01,  1.2681e-06, -1.6246e-01, -4.9869e-02,\n          -7.0287e-02, -1.6674e-03, -1.2628e-01,  7.2879e-02, -2.6658e-01,\n          -4.3385e-02, -1.3041e-02,  1.3853e-01,  9.2652e-02,  1.3920e-01,\n           1.6977e-01, -8.1156e-02,  3.3611e-02,  1.9832e-02,  3.4340e-02,\n          -1.0543e-02, -5.9265e-02, -1.3928e-01, -1.9022e-02,  1.0770e-01,\n           6.6137e-02, -1.3821e-01, -8.6274e-02, -1.2282e-01, -4.3384e-02,\n           1.5446e-01, -1.2482e-01, -3.1733e-01],\n         [-5.0405e-02, -9.8563e-02,  1.4524e-02, -4.7723e-02, -2.1366e-01,\n           1.2520e-02,  9.3535e-02, -1.7930e-02,  8.1626e-03, -7.2552e-02,\n          -1.7400e-01, -4.8571e-03,  3.4715e-02, -1.6996e-01,  2.5612e-02,\n           1.7753e-02,  1.1491e-01,  3.2849e-02,  7.4169e-02,  2.3232e-02,\n           2.3961e-01, -1.4371e-01,  7.5394e-02, -5.6723e-03,  1.4591e-02,\n          -1.0227e-01, -6.3025e-02, -4.8187e-02,  3.7513e-02, -2.2861e-02,\n          -2.5060e-02, -5.2416e-02,  3.6575e-02,  4.3657e-02,  3.9590e-02,\n           6.8889e-02,  7.8860e-02, -6.1261e-02,  6.4804e-03,  6.4698e-02,\n          -1.3405e-01,  4.9579e-02,  9.1552e-02,  2.5240e-01, -1.1347e-01,\n           4.2408e-02,  8.2981e-02,  1.1758e-01,  1.4038e-02, -5.8177e-02,\n          -4.6107e-02,  9.8306e-02,  5.3295e-02, -1.2327e-02, -3.7325e-02,\n           2.1046e-01,  1.1949e-01, -8.8206e-03,  9.5156e-02,  8.3061e-03,\n           8.0187e-02,  4.3940e-02,  1.4411e-01, -5.8862e-03,  1.5729e-02,\n          -1.7983e-01,  5.2839e-02,  5.6460e-02, -1.3105e-01,  2.7025e-01,\n           4.4835e-02, -1.3793e-01,  4.7627e-02, -3.9222e-02, -6.4474e-02,\n           1.2132e-01, -5.4602e-02, -4.1981e-02, -1.7641e-01,  1.1443e-01,\n           1.6024e-01, -4.2657e-02,  2.9715e-02, -6.2705e-03,  7.2440e-02,\n          -3.1999e-01, -1.6657e-01, -9.7144e-02,  7.7817e-02, -4.3598e-02,\n           1.5082e-03,  5.5090e-02, -3.5226e-02,  7.4446e-02, -1.4492e-01,\n           1.9284e-01, -1.0586e-01,  9.7238e-02, -2.9331e-01,  1.6685e-01,\n           6.7393e-02, -3.8107e-02, -5.1379e-03, -1.8144e-01, -1.0758e-01,\n          -3.5332e-02, -9.5876e-02, -3.3853e-02,  7.4512e-02,  7.1593e-02,\n           5.6300e-02, -2.2599e-01, -2.6461e-02, -9.8212e-02, -1.3220e-01,\n           2.4089e-03, -2.6224e-01, -4.1655e-02, -1.2883e-03, -1.4610e-01,\n           7.1842e-02,  1.6349e-01, -3.0933e-01,  2.8165e-02,  4.9211e-02,\n           1.6185e-02, -4.3435e-02,  2.9302e-01],\n         [ 6.8762e-02, -1.0513e-01,  8.0041e-03,  3.1691e-02,  7.8240e-02,\n          -3.0865e-01, -1.2082e-01, -1.5014e-02, -6.4405e-02,  3.7052e-03,\n           1.5633e-01, -8.7142e-02,  2.7108e-01, -4.8148e-02,  1.9585e-01,\n          -1.0857e-01, -1.6191e-02,  1.1271e-01, -4.2816e-02,  1.1897e-01,\n           2.1357e-01,  6.0726e-03, -1.6058e-01,  1.0014e-01,  7.2932e-03,\n          -3.3023e-02, -1.3890e-02,  2.3372e-01, -1.5602e-01, -1.3258e-01,\n           3.4258e-02,  4.1946e-03, -4.5052e-02,  1.0765e-01,  8.8289e-02,\n           5.0133e-02,  1.0398e-01,  1.7352e-02, -1.1756e-01,  1.2870e-01,\n           9.9371e-02,  1.0367e-01, -7.6240e-02, -6.7554e-02,  2.2970e-02,\n           1.8892e-01, -8.2191e-02,  1.6877e-01, -7.5892e-03,  4.1152e-02,\n           6.9047e-02,  1.0306e-01,  2.3320e-01,  1.2969e-01, -2.9587e-01,\n           2.2722e-01, -1.7229e-01, -7.0078e-02,  6.9529e-02,  1.5175e-01,\n           4.4102e-02, -1.8311e-01,  4.8507e-03, -1.8906e-01,  1.3765e-01,\n           8.4732e-02, -8.0962e-02, -6.6467e-02, -4.6670e-02, -1.5538e-02,\n          -2.2635e-01,  2.1714e-01,  2.3420e-01, -1.2549e-01,  1.6654e-02,\n           7.8636e-02, -5.5591e-02,  1.5804e-01, -1.9898e-03, -1.6906e-01,\n          -1.9567e-02,  3.1766e-02,  9.4322e-03, -7.3489e-02, -2.7695e-02,\n           1.2616e-01,  1.7542e-01,  1.9282e-01, -1.0990e-01, -2.4401e-01,\n           7.7423e-02, -1.5268e-01, -3.8916e-03,  6.2967e-02,  1.1859e-01,\n           8.4724e-02, -1.8199e-01, -7.2396e-02, -6.9568e-02, -7.0842e-02,\n          -9.4516e-02,  2.1872e-01,  2.2665e-01, -1.6960e-01, -2.2812e-01,\n           1.2222e-01,  1.3715e-01, -4.3735e-02, -7.3953e-02, -9.7945e-02,\n          -2.6904e-02,  8.7878e-02, -1.0421e-02,  1.6984e-02,  3.8456e-02,\n           1.7513e-01, -1.0124e-03,  3.8656e-02,  1.4228e-01,  9.2844e-03,\n           2.9087e-02,  5.2339e-02, -1.9062e-01, -1.0616e-01,  1.0397e-01,\n          -2.7843e-02, -7.8383e-02, -2.1809e-03]], device='cuda:0',\n        grad_fn=&lt;ReshapeAliasBackward0&gt;),\n 'fc5.bias': tensor([ 0.0370,  0.0172, -0.0607], device='cuda:0',\n        grad_fn=&lt;ReshapeAliasBackward0&gt;)}"
  },
  {
    "objectID": "notebooks/mcmc-hamiltorch_zeel.html",
    "href": "notebooks/mcmc-hamiltorch_zeel.html",
    "title": "Probabilistic Machine Learning",
    "section": "",
    "text": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n\nimport torch\nimport torch.autograd.functional as F\nimport torch.distributions as dist\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\n\nimport pandas as pd\n%matplotlib inline\n\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom tueplots import bundles\n\nplt.rcParams.update(bundles.beamer_moml())\n# plt.rcParams.update(bundles.icml2022())\n\n\n# Also add despine to the bundle using rcParams\nplt.rcParams[\"axes.spines.right\"] = False\nplt.rcParams[\"axes.spines.top\"] = False\n\n# Increase font size to match Beamer template\nplt.rcParams[\"font.size\"] = 16\n# Make background transparent\nplt.rcParams[\"figure.facecolor\"] = \"none\"\n\n\nimport hamiltorch\n\n\nhamiltorch.set_random_seed(123)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndevice\n\ndevice(type='cuda')\n\n\n\ngt_distribution = torch.distributions.Normal(0, 1)\n\n\n# Samples from the ground truth distribution\ndef sample_gt(n):\n    return gt_distribution.sample((n,))\n\n\nsamples = sample_gt(1000)\n\n\nx_lin = torch.linspace(-3, 3, 1000)\ny_lin = torch.exp(gt_distribution.log_prob(x_lin))\n\nplt.plot(x_lin, y_lin, label=\"Ground truth\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fb356939c40&gt;\n\n\n\n\n\n\n# Logprob function to be passed to Hamiltorch sampler\ndef logprob(x):\n    return gt_distribution.log_prob(x)\n\n\nlogprob(torch.tensor([0.0]))\n\ntensor([-0.9189])\n\n\n\n# Initial state\nx0 = torch.tensor([0.0])\nnum_samples = 5000\nstep_size = 0.3\nnum_steps_per_sample = 5\nhamiltorch.set_random_seed(123)\n\n\nparams_hmc = hamiltorch.sample(\n    log_prob_func=logprob,\n    params_init=x0,\n    num_samples=num_samples,\n    step_size=step_size,\n    num_steps_per_sample=num_steps_per_sample,\n)\n\nSampling (Sampler.HMC; Integrator.IMPLICIT)\nTime spent  | Time remain.| Progress             | Samples   | Samples/sec\n0d:00:00:07 | 0d:00:00:00 | #################### | 5000/5000 | 698.22       \nAcceptance Rate 0.99\n\n\n\nparams_hmc = torch.tensor(params_hmc)\n\n\n# Trace plot\nplt.plot(params_hmc, label=\"Trace\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Parameter value\")\n\nText(0, 0.5, 'Parameter value')\n\n\n\n\n\n\n# view first 500 samples\nplt.plot(params_hmc[:500], label=\"Trace\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Parameter value\")\n\nText(0, 0.5, 'Parameter value')\n\n\n\n\n\n\n# KDE plot\nimport seaborn as sns\n\nplt.figure()\nsns.kdeplot(params_hmc.detach().numpy(), label=\"Samples\", shade=True, color=\"C1\")\nplt.plot(x_lin, y_lin, label=\"Ground truth\")\nplt.xlabel(\"Parameter value\")\nplt.ylabel(\"Density\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fb353810dc0&gt;\n\n\n\n\n\n\ndef plot_samples_gif(x_lin, y_lin, params_hmc, filename, frames=50):\n    fig, ax = plt.subplots()\n    plt.plot(x_lin, y_lin, label=\"Ground truth\")\n    scatter = ax.scatter([], [], color=\"C1\", marker=\"x\", s=100)\n\n    # Function to update the animation\n    def update(frame):\n        scatter.set_offsets(np.array([[params_hmc[frame], 0]]))\n        return (scatter,)\n\n    # Create the animation\n    anim = FuncAnimation(fig, update, frames=frames, blit=True)\n\n    # Save the animation as a GIF or video file (you can change the filename and format)\n    anim.save(filename, dpi=200)\n\n\nplot_samples_gif(\n    x_lin, y_lin, params_hmc, \"../figures/sampling/mcmc/hamiltorch-samples-normal.gif\"\n)\n\n\n\n\n\n\n# sample from Mixture of Gaussians\n\nmog = dist.MixtureSameFamily(\n    mixture_distribution=dist.Categorical(torch.tensor([0.3, 0.7])),\n    component_distribution=dist.Normal(\n        torch.tensor([-2.0, 2.0]), torch.tensor([1.0, 0.5])\n    ),\n)\n\nsamples = mog.sample((1000,))\nsns.kdeplot(samples.numpy(), label=\"Samples\", shade=True, color=\"C1\")\n\n&lt;AxesSubplot:ylabel='Density'&gt;\n\n\n\n\n\n\n# Logprob function to be passed to Hamiltorch sampler\ndef logprob(x):\n    return mog.log_prob(x)\n\n\nlogprob(torch.tensor([0.0]))\n\ntensor([-4.1114])\n\n\n\n# Initial state\nx0 = torch.tensor([0.0])\nnum_samples = 5000\nstep_size = 0.3\nnum_steps_per_sample = 5\nhamiltorch.set_random_seed(123)\n\nparams_hmc = hamiltorch.sample(\n    log_prob_func=logprob,\n    params_init=x0,\n    num_samples=num_samples,\n    step_size=step_size,\n    num_steps_per_sample=num_steps_per_sample,\n)\n\nSampling (Sampler.HMC; Integrator.IMPLICIT)\nTime spent  | Time remain.| Progress             | Samples   | Samples/sec\n0d:00:00:10 | 0d:00:00:00 | #################### | 5000/5000 | 462.18       \nAcceptance Rate 0.99\n\n\n\nparams_hmc = torch.tensor(params_hmc)\n\n\n# Trace plot\nplt.plot(params_hmc[:500], label=\"Trace\")\n\n\n\n\n\ny_lin = torch.exp(mog.log_prob(x_lin))\nplot_samples_gif(\n    x_lin,\n    y_lin,\n    params_hmc,\n    \"../figures/sampling/mcmc/hamiltorch-samples-mog.gif\",\n    frames=300,\n)\n\n\n\n\n\ndef p_tilde(x):\n    # normalising constant for standard normal distribution\n    Z = torch.sqrt(torch.tensor(2 * np.pi))\n    return dist.Normal(0, 1).log_prob(x).exp() * Z\n\n\ndef p_tilde_log_prob(x):\n    # normalising constant for standard normal distribution\n    Z = torch.sqrt(torch.tensor(2 * np.pi))\n    return dist.Normal(0, 1).log_prob(x) + torch.log(Z)\n\n\n# Plot unnormalized distribution\nx_lin = torch.linspace(-3, 3, 1000)\ny_lin = p_tilde(x_lin)\nplt.plot(x_lin, y_lin, label=\"Unnormalized distribution\")\n# Plot normalized distribution\nplt.plot(\n    x_lin, dist.Normal(0, 1).log_prob(x_lin).exp(), label=\"Normalized distribution\"\n)\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fb3378375e0&gt;\n\n\n\n\n\n\n# HMC over unnormalized distribution\n\n\n# Logprob function to be passed to Hamiltorch sampler\ndef logprob(x):\n    return p_tilde_log_prob(x)\n\n\n# HMC\nx0 = torch.tensor([0.0])\nnum_samples = 5000\nstep_size = 0.3\nnum_steps_per_sample = 5\nhamiltorch.set_random_seed(123)\n\nparams_hmc = hamiltorch.sample(\n    log_prob_func=logprob,\n    params_init=x0,\n    num_samples=num_samples,\n    step_size=step_size,\n    num_steps_per_sample=num_steps_per_sample,\n)\n\nparams_hmc = torch.tensor(params_hmc)\n\nSampling (Sampler.HMC; Integrator.IMPLICIT)\nTime spent  | Time remain.| Progress             | Samples   | Samples/sec\n0d:00:00:10 | 0d:00:00:00 | #################### | 5000/5000 | 488.07       \nAcceptance Rate 0.99\n\n\n\n# Trace plot\nplt.plot(params_hmc[:500], label=\"Trace\")\n\n\n\n\n\n# KDE plot\nsns.kdeplot(params_hmc.detach().numpy(), label=\"Samples\", shade=True, color=\"C1\")\nplt.plot(x_lin, y_lin, label=\"Unnormalized distribution\")\nplt.plot(\n    x_lin, dist.Normal(0, 1).log_prob(x_lin).exp(), label=\"Normalized distribution\"\n)\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fb3377450d0&gt;\n\n\n\n\n\n\n# Coin toss\n\nprior = dist.Beta(1, 1)\ndata = torch.tensor([1.0, 1.0, 1.0, 0.0, 0.0])\nn = len(data)\n\n\ndef log_prior(theta):\n    return prior.log_prob(theta)\n\n\ndef log_likelihood(theta):\n    return dist.Bernoulli(theta).log_prob(data).sum()\n\n\ndef negative_log_joint(theta):\n    return log_prior(theta) + log_likelihood(theta)\n\n\ndef run_hmc(logprob, x0, num_samples, step_size, num_steps_per_sample):\n    torch.manual_seed(12)\n    params_hmc = hamiltorch.sample(\n        log_prob_func=logprob,\n        params_init=x0,\n        num_samples=num_samples,\n        step_size=step_size,\n        num_steps_per_sample=num_steps_per_sample,\n    )\n    return torch.stack(params_hmc)\n\n\ntry:\n    params_hmc_theta = run_hmc(negative_log_joint, torch.tensor([0.5]), 5000, 0.3, 5)\nexcept Exception as e:\n    print(e)\n\nSampling (Sampler.HMC; Integrator.IMPLICIT)\nTime spent  | Time remain.| Progress             | Samples   | Samples/sec\nExpected value argument (Tensor of shape (1,)) to be within the support (Interval(lower_bound=0.0, upper_bound=1.0)) of the distribution Beta(), but found invalid values:\ntensor([-0.1017], requires_grad=True)\n\n\n\n# Let us work instead with logits\ndef log_prior(logits):\n    return prior.log_prob(torch.sigmoid(logits)).sum()\n\n\ndef log_likelihood(logits):\n    return dist.Bernoulli(logits=logits).log_prob(data).sum()\n\n\ndef log_joint(logits):\n    return log_prior(logits) + log_likelihood(logits)\n\n\nparams_hmc_logits = run_hmc(log_joint, torch.tensor([0.0]), 1000, 0.3, 5)\n\nSampling (Sampler.HMC; Integrator.IMPLICIT)\nTime spent  | Time remain.| Progress             | Samples   | Samples/sec\n0d:00:00:03 | 0d:00:00:00 | #################### | 1000/1000 | 278.54       \nAcceptance Rate 0.99\n\n\n\nfig, ax = plt.subplots(nrows=2, sharex=True)\nax[0].plot(params_hmc_logits[:500], label=\"Trace\")\nax[1].plot(torch.sigmoid(params_hmc_logits[:500]), label=\"Trace\")\n\n\n\n\n\nparams_hmc_logits[:, 0]\n\ntensor([ 0.0000e+00,  5.2157e-01,  1.3223e+00,  1.0867e+00, -5.2568e-01,\n        -2.0241e-01,  2.1629e+00,  1.4570e+00,  6.9446e-01,  1.0426e+00,\n        -9.3411e-01,  1.3008e+00, -4.8513e-02,  6.5970e-02, -4.6523e-01,\n        -5.8596e-01, -4.0162e-01,  1.1607e-01, -6.7595e-01,  6.2900e-01,\n        -5.1382e-02, -1.5011e-02,  8.3155e-01,  5.7020e-01,  1.1027e+00,\n         1.7802e+00,  6.8140e-01, -1.4304e+00,  8.8081e-01, -7.4787e-01,\n         1.1560e+00,  9.3348e-02,  9.5877e-01,  2.4764e-02,  2.5223e-01,\n         7.0275e-01,  2.4133e+00,  5.0036e-01, -3.1154e-01,  2.6437e+00,\n         1.0347e+00, -3.0697e+00, -9.9357e-02,  7.0691e-03,  6.6288e-01,\n         4.2240e-01,  2.6325e-01,  5.3176e-01, -5.1853e-01, -2.3356e-01,\n        -1.3524e-01,  9.2447e-01, -6.1977e-01,  5.7892e-01,  9.1914e-01,\n         1.2103e+00, -3.4587e-01, -8.8387e-01,  4.2828e-01,  9.6681e-01,\n         4.3113e-01, -6.5857e-01, -5.3167e-02, -1.2148e+00, -1.8391e+00,\n         2.7865e-01, -1.1244e+00,  1.6879e+00,  8.4666e-01,  1.7127e-01,\n         1.4485e+00, -5.0676e-01,  9.4826e-01,  1.2424e-01, -4.2752e-01,\n         3.0383e-01, -1.5217e-01,  6.3599e-02,  1.3415e+00,  1.1742e+00,\n         8.9259e-01,  9.5579e-01,  8.8160e-01,  3.3091e-01, -1.1668e+00,\n         1.0816e+00,  5.1343e-02, -1.4267e-01,  5.5287e-01,  1.1972e+00,\n        -3.1179e-01,  3.0234e-01,  1.9230e+00, -5.4003e-01,  1.9161e+00,\n         2.5963e-01,  5.3916e-01, -2.3909e-01, -4.7502e-01,  5.5535e-01,\n         9.9362e-01, -5.1886e-01, -1.0049e-01, -3.5520e-01,  4.1397e-02,\n        -2.3988e-01,  7.3340e-01, -6.0357e-01, -1.3952e+00,  1.2863e+00,\n         6.9414e-01, -3.6833e-01, -9.2557e-01,  7.9619e-01,  5.7926e-01,\n         5.2438e-01, -4.5046e-01,  9.7921e-01, -1.7372e+00,  7.9464e-01,\n        -3.8177e-01,  8.8204e-01,  4.0353e-01,  1.4525e+00,  8.4391e-01,\n         1.8884e+00,  4.2759e-01,  7.8184e-01,  1.4167e+00,  2.3686e+00,\n        -4.3994e-01,  2.5539e+00, -2.1615e-01,  3.3379e-01,  4.2178e-01,\n         5.1721e-03,  3.8601e-01,  1.7428e-02,  4.2170e-01,  7.6588e-01,\n        -1.8764e-01, -1.2382e-01,  9.7327e-01,  2.6694e+00,  1.3677e+00,\n         3.7424e-01,  6.3888e-01, -8.4959e-01, -8.8009e-01,  2.3543e-02,\n         1.6492e+00,  2.7357e-01, -1.3845e+00,  1.9475e+00, -9.1431e-01,\n        -1.3962e+00,  1.4945e+00, -1.6934e-01,  6.8331e-01,  2.7269e+00,\n         6.1146e-01,  7.2529e-01, -7.0244e-01,  1.5423e+00,  5.0477e-01,\n         7.5417e-01,  1.5440e+00,  7.5432e-01, -1.0567e+00,  6.8278e-03,\n        -9.1455e-01,  1.1774e+00, -4.7259e-02, -7.6331e-01,  1.5087e+00,\n         1.8454e+00, -2.9682e-01,  2.4198e-01,  1.4402e+00,  1.5362e+00,\n         3.3266e+00,  7.4973e-01,  1.4066e+00,  3.8852e-01, -4.4323e-01,\n         2.5634e-01, -5.2011e-01, -8.8203e-01,  1.8299e+00,  6.3801e-01,\n         1.4932e+00,  1.7013e+00,  2.8509e-01,  5.3203e-01,  3.7003e-01,\n         3.6926e-01,  2.1920e-01, -1.1465e-01,  1.6096e-01,  1.6096e-01,\n         1.6096e-01,  1.6096e-01,  7.3189e-02, -1.2568e+00, -1.0996e+00,\n        -6.6443e-01,  4.9931e-01, -1.1474e+00,  8.1839e-01, -4.1737e-01,\n        -7.6412e-02,  6.2048e-01,  4.2288e-01, -1.1374e-01,  1.0812e+00,\n        -3.7642e-01,  3.3120e-01,  6.6237e-02, -5.7989e-01,  8.3937e-01,\n        -1.4361e+00,  6.1602e-01,  8.5366e-01, -3.5081e-02, -1.2576e+00,\n         9.2308e-01,  1.2499e+00,  7.9868e-01, -4.0161e-01,  1.8000e+00,\n         2.1205e+00,  2.2769e+00,  1.1112e+00,  1.6897e-01,  3.6750e-01,\n        -4.5992e-01, -6.0003e-01,  9.8896e-01,  9.2506e-01, -1.2205e+00,\n         6.2656e-01,  2.4740e+00,  9.4190e-02,  1.1711e+00, -4.7227e-01,\n         1.7815e+00,  4.5940e-01,  6.8583e-01, -2.6535e-01,  3.0298e-01,\n        -5.7829e-01,  1.0377e+00,  1.3456e+00,  2.0190e-01,  7.1908e-01,\n         9.7466e-01,  6.5028e-01, -5.2753e-01,  5.5185e-01, -6.3477e-01,\n         1.1783e+00,  4.5608e-01,  1.2227e+00,  1.8145e-01, -6.6724e-01,\n         1.1463e+00,  1.3225e+00,  1.1086e+00, -1.1422e+00, -1.5377e-01,\n         9.5494e-01,  1.0698e-01,  5.1788e-01, -5.0524e-01, -9.8988e-01,\n         7.0583e-01, -4.0222e-01,  7.1202e-01,  4.4194e-01,  4.2165e-01,\n         3.9502e-01,  7.1842e-01,  1.2794e+00,  1.0915e+00,  2.5821e+00,\n         2.6053e+00,  9.5238e-01,  9.0762e-02,  2.6056e+00, -1.5100e+00,\n        -1.8069e-01, -2.9206e-01, -6.4654e-01,  1.0541e+00,  8.5845e-01,\n        -2.9253e-01,  9.3141e-01,  9.9560e-01,  7.7755e-01,  6.4045e-01,\n         1.3102e-01,  8.4557e-01,  8.8637e-01,  1.8827e+00,  1.8122e+00,\n         1.7478e-01,  3.1308e-02,  4.8694e-01,  4.5788e-01,  7.9718e-02,\n         9.6236e-01,  1.1176e+00,  2.1386e+00,  1.5038e+00, -9.0693e-01,\n         1.6350e+00,  6.3332e-01, -1.7795e-01,  3.8162e-01,  2.7007e+00,\n         1.2218e+00,  3.7495e-01,  3.5609e-01, -3.4414e-01,  1.8059e+00,\n        -2.0574e+00, -1.6110e+00,  2.0561e-01, -2.0184e+00,  1.4640e+00,\n         2.9137e-01,  2.2045e+00,  4.0625e+00,  8.4217e-01, -1.9306e+00,\n         1.3940e+00, -8.0387e-01, -9.4434e-01,  1.9487e+00,  1.9015e-01,\n        -1.9470e-01, -3.5154e-01, -4.7942e-01,  2.4984e+00,  1.5140e+00,\n        -5.1064e-01, -3.2031e+00,  4.6453e-02, -3.7326e-01, -7.8675e-02,\n         7.8834e-01, -7.0566e-01,  2.0437e+00, -7.3646e-01, -5.6822e-01,\n         1.1436e+00,  2.5813e+00,  2.1137e+00,  3.4318e-01,  6.2561e-01,\n         2.4020e+00, -6.3089e-01,  6.2534e-01,  4.0191e-01,  1.5595e+00,\n         3.4065e-01,  7.1987e-01,  1.3318e+00,  4.2015e-01,  1.0865e+00,\n         3.3707e-01,  4.4238e-01,  1.0057e+00,  4.0243e-01, -6.3879e-01,\n        -7.3347e-02, -3.9503e-01,  2.3499e-02,  1.2202e+00,  1.0019e+00,\n         9.6899e-01,  2.8081e-01,  1.4312e+00,  1.7951e+00,  1.7800e-01,\n         5.4844e-01, -4.9231e-01,  3.4481e-01, -6.0484e-01,  8.6655e-01,\n         1.5038e+00, -8.8249e-01, -2.3613e-01,  1.4711e+00,  1.9242e+00,\n        -1.2292e+00, -7.6492e-01, -1.3766e+00,  8.1369e-01,  1.3737e+00,\n         3.7870e-02,  7.5596e-01, -8.3360e-01,  5.8478e-01, -4.8147e-02,\n         1.6379e+00,  4.9499e-01,  1.1217e-01,  4.7315e-01, -3.8294e-01,\n         4.4057e-01, -6.5282e-01, -6.9137e-02,  2.0160e+00, -1.4416e+00,\n         8.7326e-02, -5.0134e-01,  5.9610e-01,  1.1678e+00, -7.0852e-01,\n         1.1161e+00,  5.1063e-01,  1.2536e+00,  9.7398e-01, -5.3866e-01,\n        -1.5744e-01,  1.3044e-01,  5.9685e-01,  1.3283e+00, -2.1259e-01,\n         5.9374e-01,  2.9467e-01,  5.7726e-01, -8.1029e-01,  2.4936e-01,\n         3.2499e-01,  1.4028e+00,  9.2111e-01,  1.6626e-01,  3.2936e-01,\n        -6.4536e-01,  4.0314e-01,  1.1408e+00,  1.1855e+00,  2.3198e+00,\n        -5.8188e-01,  1.1133e+00,  4.7731e-01,  1.7239e-01, -2.2253e-01,\n         5.7479e-01, -3.2582e-01,  3.6510e-01,  7.3250e-01,  1.2381e+00,\n         7.9945e-01, -2.4175e-01,  1.4102e+00, -2.3131e+00,  5.3915e-01,\n         1.9302e+00,  7.3718e-01, -1.4272e+00,  1.3649e+00, -3.0076e-02,\n         9.5023e-01, -3.3479e-01, -1.2208e+00,  2.0888e+00,  4.3582e-01,\n         3.7189e+00,  2.6196e-01, -9.2166e-01,  1.0887e+00,  3.5772e-01,\n         1.0647e+00, -1.1317e+00, -1.6681e-01, -1.6681e-01, -1.3717e-01,\n        -3.4850e-01,  1.5862e+00,  3.1538e+00,  1.4352e+00,  9.1958e-01,\n         1.2272e+00, -9.4940e-02,  1.4208e+00,  1.4999e+00,  8.8756e-01,\n         1.4787e+00, -1.6487e-01, -8.3603e-01, -1.1821e+00,  7.5473e-01,\n         2.0233e+00, -1.4535e+00,  1.6937e+00, -7.1143e-01, -2.5059e-01,\n         2.3465e-01,  4.7424e-01,  4.7742e-01,  3.3519e-01,  3.3519e-01,\n         8.8493e-01,  1.1561e+00, -3.1853e-01, -3.9363e-02,  6.7079e-01,\n         5.6049e-01, -5.4034e-01, -2.2138e-01,  2.1434e-01,  1.5668e+00,\n        -5.0666e-01,  1.2336e+00,  6.0810e-01,  1.0208e+00, -1.0918e-01,\n         6.1068e-01,  3.4561e-01,  3.1687e-01,  1.9952e-01, -4.2316e-01,\n        -6.8547e-01,  1.2960e+00,  4.3325e-01, -7.4669e-01,  3.4048e-01,\n         5.0448e-01,  1.9143e+00,  3.3597e+00,  7.2327e-01,  7.2327e-01,\n         1.9814e+00,  1.7529e+00,  1.2328e+00,  2.3874e+00,  8.9311e-01,\n         1.5371e+00,  6.3213e-01, -5.3584e-01,  1.6869e-01,  7.2835e-01,\n         5.0431e-01,  1.2030e+00,  1.8420e+00,  3.5781e+00,  2.5207e+00,\n         9.1733e-01,  1.2418e+00,  1.0947e+00, -3.2848e-01,  1.4335e+00,\n         1.5814e+00,  1.5399e+00,  1.1806e+00,  2.0780e+00,  2.3361e+00,\n         7.9356e-02,  1.9177e+00, -2.2122e-01,  2.0555e+00,  1.2023e+00,\n         5.6297e-01,  1.7306e+00, -4.2852e-01,  9.2878e-01, -5.6072e-01,\n         6.1711e-01,  1.4516e+00,  9.8128e-01,  1.4595e+00,  6.1320e-01,\n        -1.5955e+00,  5.4722e-01,  1.9468e+00,  4.5901e-02,  8.8389e-01,\n        -3.5987e-01,  1.2097e+00,  6.5788e-01, -6.0313e-01, -3.5547e-01,\n         7.2529e-01, -6.0797e-03,  1.8000e+00,  6.2690e-01,  8.4397e-01,\n        -5.3849e-01,  1.6233e+00,  1.9809e+00, -3.7226e-01,  1.0213e-01,\n         3.9237e-01,  5.6708e-02,  5.8428e-01,  9.4924e-01,  2.1116e+00,\n         1.3148e+00, -1.0424e-01, -2.1544e-02,  1.3272e+00,  3.0616e+00,\n         9.9475e-01, -1.5997e-01, -5.3483e-01, -5.4581e-01,  1.4050e+00,\n         1.8539e+00,  1.6646e+00, -3.4664e-01,  2.1784e-01, -7.7635e-01,\n         2.7252e+00, -6.2144e-01, -8.7422e-01,  4.6023e-01, -6.9424e-01,\n        -9.1952e-01, -3.1647e-02,  4.7649e-01,  1.8191e+00, -3.3237e-01,\n         1.6256e+00,  3.2647e-01,  1.3099e+00,  3.6260e-01,  1.0741e+00,\n        -1.5744e-01, -2.6766e-01,  8.3143e-01,  8.5419e-01,  1.2322e+00,\n        -1.1485e+00,  3.7815e-01, -2.8660e-01,  1.5768e+00, -3.4889e-01,\n         9.6623e-01,  7.0870e-01,  5.1634e-01,  1.2491e+00,  8.7313e-01,\n        -4.7971e-01,  3.6604e-01, -4.4042e-01,  6.1318e-02, -1.5561e-01,\n        -1.3538e-01, -2.0977e+00, -1.6288e+00,  5.0716e-01, -5.4127e-01,\n        -5.1384e-01,  1.0936e+00,  1.3244e+00,  3.1679e-01,  1.6746e+00,\n         1.9762e+00,  1.9939e-01,  1.5693e+00,  1.2094e+00,  3.3478e+00,\n         1.2666e+00,  2.0992e+00,  2.8287e-02, -4.1169e-01, -6.8056e-01,\n         1.0113e+00, -6.9923e-01,  1.6349e+00, -7.5508e-01,  1.5240e+00,\n        -4.1570e-01, -4.9153e-02,  3.1265e-01,  2.4256e+00,  9.4421e-01,\n         1.2525e-01,  2.6237e-01, -1.7258e-01,  4.4751e-01, -4.6017e-01,\n         1.7248e-02,  1.3471e+00, -9.5393e-01,  3.8606e-02,  6.6836e-01,\n         7.4823e-01,  7.3605e-01,  2.7526e-02,  1.6556e+00,  1.6328e+00,\n         1.8542e+00,  2.4735e+00,  1.0023e+00, -4.9651e-01, -9.9532e-01,\n         2.1426e-03, -3.1631e-02, -1.9769e+00, -1.2023e+00,  3.9273e-01,\n         3.5130e-01,  1.9974e-02,  1.8765e+00, -1.3989e+00,  1.0675e+00,\n         2.0070e-01,  7.1379e-01, -6.7666e-01,  8.5469e-01,  1.8344e-01,\n        -3.2166e-01, -3.2166e-01,  1.3211e+00,  4.1828e-01,  1.0699e+00,\n         1.5376e+00, -1.7779e-01,  2.8796e-01,  8.1860e-01,  5.8796e-01,\n         1.3816e+00,  1.3530e+00,  2.3099e-01,  2.4880e-01,  5.6564e-01,\n         1.2632e+00, -1.5422e+00, -1.2768e+00, -5.1313e-01,  7.9212e-01,\n         8.6664e-01,  1.8058e-01,  2.4497e-01,  3.2818e-01,  3.7874e-01,\n         5.5695e-01, -9.5376e-01,  5.0202e-01, -6.2435e-01,  1.0083e+00,\n         2.4775e-01,  1.9974e-01,  1.0435e+00,  1.8181e+00,  1.8181e+00,\n         1.5419e+00, -6.8479e-01,  1.0234e+00,  3.4072e-01, -6.8201e-01,\n        -6.0094e-03,  5.4767e-01,  1.2085e+00,  1.1278e+00, -2.5554e-01,\n        -9.6161e-01, -3.8192e-01,  2.3630e-02,  1.0346e+00, -1.9409e-01,\n        -1.7149e-01,  1.2116e+00,  1.0531e+00,  1.7023e-01, -4.3127e-01,\n         6.6011e-01,  7.7393e-01, -3.0603e-01,  1.0112e+00,  2.9591e-01,\n         1.2036e-01, -4.1267e-01, -4.4962e-01,  7.4402e-01, -6.3659e-01,\n         4.6125e-01, -1.3424e+00,  2.4050e-01, -2.0018e-01,  6.3407e-01,\n        -2.5910e-01,  9.2403e-02, -3.3480e-01, -3.3480e-01, -6.5366e-01,\n         2.5464e-01,  1.3055e+00, -1.5981e+00,  7.8798e-01, -4.1375e-01,\n         7.5003e-01,  6.3855e-02, -3.7143e-01,  1.5067e+00,  1.6203e+00,\n         5.0651e-01,  1.1662e+00,  6.5912e-01,  6.5912e-01,  9.6239e-01,\n        -2.9655e-02, -1.8119e+00,  1.4097e+00,  7.1906e-01,  6.5160e-01,\n         6.4548e-01,  8.0345e-01,  1.8066e+00, -1.8827e+00, -1.9057e+00,\n         1.7102e-01, -5.2693e-01,  1.8661e-01,  2.8774e-01,  9.7600e-01,\n         1.5767e+00,  1.0017e+00,  1.4579e+00, -3.2165e-03,  1.1204e+00,\n         1.5134e+00,  2.0321e+00,  1.6208e-01, -7.9307e-01,  1.5747e+00,\n         7.5000e-01, -3.5063e-03,  2.1710e+00,  9.3272e-01, -3.8879e-04,\n         1.1818e+00, -1.7795e-02, -2.4369e-01, -1.9889e+00, -5.1913e-01,\n        -7.0746e-01,  1.6388e+00,  1.4992e+00,  1.7651e+00,  3.5630e+00,\n         4.6029e-02, -1.4069e+00,  5.9076e-02,  2.2806e+00, -8.9770e-01,\n        -6.2536e-01,  1.6230e+00,  1.5135e+00,  7.6925e-01, -3.8131e-01,\n         1.7301e+00,  1.3660e+00,  6.2360e-01,  5.6905e-01, -3.4309e-01,\n        -1.1920e+00,  1.8200e+00, -5.0699e-01,  4.0612e-02,  1.2228e+00,\n         9.6594e-01, -7.0298e-01, -6.2520e-01,  1.6632e+00,  8.8678e-01,\n         5.2583e-01,  9.1454e-01,  1.4418e+00,  1.4364e+00,  5.6974e-01,\n        -1.1927e+00,  4.6645e-01,  5.5446e-01,  7.2413e-01,  1.9243e+00,\n         7.7880e-01, -7.4584e-01,  1.2390e+00, -1.1413e-01,  1.2299e+00,\n         1.0727e+00, -2.9502e-01,  3.1766e-01,  3.7765e-01,  9.4944e-01,\n        -2.1452e-01, -4.7935e-01,  1.0967e+00, -2.7905e-01, -8.6457e-01,\n         1.1860e+00, -1.1904e+00,  5.9031e-01,  6.6663e-01,  1.2639e+00,\n         6.7896e-02, -1.0120e+00,  2.5375e+00,  1.8990e+00, -1.0375e+00,\n         4.4640e-01,  1.4028e+00,  1.1473e+00,  6.1459e-01,  3.9804e-01,\n         7.8219e-01,  1.6068e+00,  5.6333e-01,  1.6903e-01, -8.0579e-01,\n        -7.3818e-01,  1.0135e+00,  1.1724e+00, -3.5863e-01, -3.4891e-01,\n        -2.9014e-01,  1.9020e+00,  1.0430e+00,  2.6433e+00,  3.1035e+00,\n         2.0789e+00, -4.8755e-01, -1.5892e+00, -1.9662e+00,  1.1934e+00,\n         1.1145e+00, -7.9593e-02,  1.2906e+00,  3.1053e-01, -5.8267e-01,\n         1.4457e+00,  7.9235e-01,  9.0271e-01,  4.6118e-01,  1.0147e+00,\n         2.2204e+00,  1.0090e+00,  2.4175e-01, -6.5125e-01,  5.9952e-01,\n         2.0044e+00,  8.9869e-01, -2.2489e-01, -1.0497e+00,  2.0135e+00,\n        -4.7356e-01,  4.7832e-01, -1.2162e-01,  1.1530e+00, -1.1612e-02,\n         1.4690e+00,  1.0307e+00,  2.2840e+00,  3.3513e+00,  8.8326e-01,\n         1.2590e+00,  1.7236e+00,  4.5989e-01,  6.4716e-01,  3.0418e-01,\n         3.9410e-01,  3.4654e-01,  1.4171e+00,  6.6960e-01,  6.7712e-01,\n        -2.0732e-01, -1.0382e-01,  9.9374e-01,  1.6327e+00,  3.1100e-01,\n         4.4393e-01, -2.9965e-01,  1.9955e-01, -9.0940e-03,  6.6451e-02,\n         1.8670e+00,  1.3433e+00, -1.4861e+00, -1.5094e+00,  1.1104e+00,\n        -4.7540e-02,  8.7289e-01, -8.7741e-01,  1.6113e+00, -2.0599e-01,\n         2.9714e-01,  2.4836e+00,  8.8014e-01,  2.2057e+00,  1.5712e+00,\n         6.9619e-02,  2.8588e-02, -3.6439e-01,  7.4889e-01,  9.1645e-01])\n\n\n\n# Plot posterior KDE using seaborn but clip to [0, 1]\nsns.kdeplot(\n    torch.sigmoid(params_hmc_logits[:, 0]).detach().numpy(),\n    label=\"Samples\",\n    shade=True,\n    color=\"C1\",\n    clip=(0, 1),\n)\n# True posterior\nx_lin = torch.linspace(0, 1, 1000)\ny_lin = dist.Beta(1 + 3, 1 + 2).log_prob(x_lin).exp()\nplt.plot(x_lin, y_lin, label=\"True posterior\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fb3376f4ac0&gt;\n\n\n\n\n\n\n# Linear regression for 1 dimensional input using HMC\n\ntorch.manual_seed(123)\nx_lin = torch.linspace(-3, 3, 90)\ntheta_0_true = torch.tensor([2.0])\ntheta_1_true = torch.tensor([3.0])\nf = lambda x: theta_0_true + theta_1_true * x\neps = torch.randn_like(x_lin) * 1.0\ny_lin = f(x_lin) + eps\n\nplt.scatter(x_lin, y_lin, label=\"Data\", color=\"C0\")\nplt.plot(x_lin, f(x_lin), label=\"Ground truth\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\n\nText(0, 0.5, 'y')\n\n\n\n\n\n\n# Esimate theta_0, theta_1 using HMC assuming noise variance is known to be 1\ndef logprob(theta):\n    y_pred = theta[0] + x_lin * theta[1]\n    # print(y_pred.shape, y_lin.shape)\n    # print(y_pred.shape, y_lin.shape, y_pred)\n    return dist.Normal(y_pred, 1).log_prob(y_lin).sum()\n\n\ndef log_prior(theta):\n    return dist.Normal(0, 1).log_prob(theta).sum()\n\n\ndef log_posterior(theta):\n    log_prior_val = log_prior(theta)\n    log_likelihood = logprob(theta)\n    log_joint = log_prior_val + log_likelihood\n    # print(log_joint, log_prior_val, log_likelihood)\n    return log_joint\n\n\nparams = torch.tensor([0.1, 0.2])\nprint(params.dtype)\nparams_hmc_lin_reg = run_hmc(log_posterior, params, 1000, 0.1, 5)\n\ntorch.float32\nSampling (Sampler.HMC; Integrator.IMPLICIT)\nTime spent  | Time remain.| Progress             | Samples   | Samples/sec\n0d:00:00:03 | 0d:00:00:00 | #################### | 1000/1000 | 265.17       \nAcceptance Rate 0.83\n\n\nSampling (Sampler.HMC; Integrator.IMPLICIT) Time spent | Time remain.| Progress | Samples | Samples/sec tensor(-1433.3967, grad_fn=) tensor(-1.8629, grad_fn=) tensor(-1431.5338, grad_fn=) tensor(-1433.3967, grad_fn=) tensor(-1.8629, grad_fn=) tensor(-1431.5338, grad_fn=) tensor(-355.0762, grad_fn=) tensor(-10.8475, grad_fn=) tensor(-344.2287, grad_fn=) tensor(-729.5259, grad_fn=) tensor(-18.6343, grad_fn=) tensor(-710.8916, grad_fn=) tensor(-1269.0886, grad_fn=) tensor(-9.7863, grad_fn=) tensor(-1259.3024, grad_fn=) tensor(-218.4335, grad_fn=) tensor(-12.0944, grad_fn=) tensor(-206.3391, grad_fn=) tensor(-1101.4713, grad_fn=) tensor(-19.0104, grad_fn=) tensor(-1082.4608, grad_fn=) tensor(-1101.4713, grad_fn=) tensor(-19.0104, grad_fn=) tensor(-1082.4608, grad_fn=) 0d:00:00:00 | 0d:00:00:00 | #################### | 1/1 | 140.03\nAcceptance Rate 1.00\n\nparams_hmc_lin_reg\n\ntensor([[0.1000, 0.2000],\n        [1.7248, 0.6831],\n        [2.1176, 5.2166],\n        ...,\n        [1.9162, 3.1325],\n        [2.1559, 2.9292],\n        [1.8733, 3.0645]])\n\n\n\n# Plot the traces corresponding to the two parameters\nfig, axes = plt.subplots(2, 1, sharex=True)\n\nfor i, param_vals in enumerate(params_hmc_lin_reg.T):\n    axes[i].plot(param_vals, label=\"Trace\")\n    axes[i].set_xlabel(\"Iteration\")\n    axes[i].set_ylabel(rf\"$\\theta_{i}$\")\n\n# Plot the true values as well\nfor i, param_vals in enumerate([theta_0_true, theta_1_true]):\n    axes[i].axhline(param_vals.numpy(), color=\"C1\", label=\"Ground truth\")\n    axes[i].legend()\n\nfindfont: Font family ['cursive'] not found. Falling back to DejaVu Sans.\nfindfont: Generic family 'cursive' not found because none of the following families were found: Apple Chancery, Textile, Zapf Chancery, Sand, Script MT, Felipa, Comic Neue, Comic Sans MS, cursive\n\n\n\n\n\n\n# Plot KDE of the samples for the two parameters\nfig, axes = plt.subplots(2, 1, sharex=True)\n\nfor i, param_vals in enumerate(params_hmc_lin_reg.T):\n    sns.kdeplot(\n        param_vals.detach().numpy(), label=\"Samples\", shade=True, color=\"C1\", ax=axes[i]\n    )\n    axes[i].set_ylabel(rf\"$\\theta_{i}$\")\n\n# Plot the true values as well\nfor i, param_vals in enumerate([theta_0_true, theta_1_true]):\n    axes[i].axvline(param_vals.numpy(), color=\"C0\", label=\"Ground truth\")\n    axes[i].legend()\n\n\n\n\n\n# Plot the posterior predictive distribution\nplt.figure()\nplt.scatter(x_lin, y_lin, label=\"Data\", color=\"C0\")\nplt.plot(x_lin, f(x_lin), label=\"Ground truth\", color=\"C1\", linestyle=\"--\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\n\n# Get posterior samples. Thin first 100 samples to remove burn-in\nposterior_samples = params_hmc_lin_reg[100:].detach()\ny_hat = posterior_samples[:, 0].unsqueeze(1) + x_lin * posterior_samples[\n    :, 1\n].unsqueeze(1)\n\n# Plot mean and 95% confidence interval\n\nplt.plot(x_lin, y_hat.mean(axis=0), label=\"Mean\", color=\"C2\")\nplt.fill_between(\n    x_lin,\n    y_hat.mean(axis=0) - 2 * y_hat.std(axis=0),\n    y_hat.mean(axis=0) + 2 * y_hat.std(axis=0),\n    alpha=0.5,\n    label=\"95% CI\",\n    color=\"C2\",\n)\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fb336ba0d30&gt;\n\n\n\n\n\n\n# Using a neural network with HMC\n\n\nclass Net(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(1, 1)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        return x\n\n\nnet = Net()\nnet\n\nNet(\n  (fc1): Linear(in_features=1, out_features=1, bias=True)\n)\n\n\n\nhamiltorch.util.flatten(net).shape\n\ntorch.Size([2])\n\n\n\ntheta_params = hamiltorch.util.flatten(net)\ntheta_params.shape\n\ntorch.Size([2])\n\n\n\nparams_list = hamiltorch.util.unflatten(net, theta_params)\nparams_list[0].shape, params_list[1].shape\n\n(torch.Size([1, 1]), torch.Size([1]))\n\n\n\nparams_init = theta_params.clone().detach()\nparams_init.dtype\n\ntorch.float32\n\n\n\nt = torch.tensor([0.1, 0.2])\n# reverse t\nt.flip(0)\n\ntensor([0.2000, 0.1000])\n\n\n\n# hamiltorch.util.update_model_params_in_place??\n\n\ntheta = torch.tensor([0.1, 0.2])\nprint(theta.shape)\nparams_list = hamiltorch.util.unflatten(net, theta)\nprint(params_list, params_list[0].shape, params_list[1].shape)\nhamiltorch.util.update_model_params_in_place(net, params_list)\nnet.state_dict()\nnet(torch.tensor([[1.0]]))\n\ntorch.Size([2])\n[tensor([[0.1000]]), tensor([0.2000])] torch.Size([1, 1]) torch.Size([1])\n\n\ntensor([[0.3000]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\ndef log_prior(theta):\n    return dist.Normal(0, 1).log_prob(theta).sum()\n\n\ndef log_likelihood(theta):\n    theta = theta.flip(0)\n    params_list = hamiltorch.util.unflatten(net, theta)\n\n    ### Inplace call\n    # hamiltorch.util.update_model_params_in_place(net, params_list)\n    # y_pred = net(x_lin.unsqueeze(1)).squeeze()\n    # print(y_pred[0:4], \"first\")\n\n    ## Functional call\n    params = net.state_dict()\n    for i, (name, _) in enumerate(params.items()):\n        params[name] = params_list[i]\n    y_pred = torch.func.functional_call(net, params, x_lin.unsqueeze(1)).squeeze()\n    # print(y_pred[0:4], \"second\")\n\n    # print(y_pred.shape, y_lin.shape, y_pred)\n    return dist.Normal(y_pred, 1).log_prob(y_lin).sum()\n\n\ndef log_joint(theta):\n    log_prior_val = log_prior(theta)\n    log_likelihood_val = log_likelihood(theta)\n    log_joint = log_prior_val + log_likelihood_val\n    # print(log_joint, log_prior_val, log_likelihood_val)\n    return log_joint\n\n\nparams_hmc = run_hmc(log_joint, torch.tensor([0.1, 0.2]), 1000, 0.1, 5)\nparams_hmc\n\nSampling (Sampler.HMC; Integrator.IMPLICIT)\nTime spent  | Time remain.| Progress             | Samples   | Samples/sec\n0d:00:00:04 | 0d:00:00:00 | #################### | 1000/1000 | 201.06       \nAcceptance Rate 0.81\n\n\ntensor([[0.1000, 0.2000],\n        [1.6531, 0.5802],\n        [2.0851, 5.3383],\n        ...,\n        [1.9668, 3.0019],\n        [2.0264, 3.0395],\n        [2.0264, 3.0395]])\n\n\n\n# Plot the traces corresponding to the two parameters\nfig, axes = plt.subplots(2, 1, sharex=True)\n\nfor i, param_vals in enumerate(params_hmc.T):\n    axes[i].plot(param_vals, label=\"Trace\")\n    axes[i].set_xlabel(\"Iteration\")\n    axes[i].set_ylabel(rf\"$\\theta_{i}$\")\n\n\n\n\n\n# Plot KDE of the samples for the two parameters\nfig, axes = plt.subplots(2, 1, sharex=True)\n\nfor i, param_vals in enumerate(params_hmc.T):\n    sns.kdeplot(\n        param_vals.detach().numpy(), label=\"Samples\", shade=True, color=\"C1\", ax=axes[i]\n    )\n    axes[i].set_ylabel(rf\"$\\theta_{i}$\")\n\nNameError: name 'sns' is not defined\n\n\n\n\n\n\n# Plot predictions\nplt.figure()\nplt.scatter(x_lin, y_lin, label=\"Data\", color=\"C0\")\nplt.plot(x_lin, f(x_lin), label=\"Ground truth\", color=\"C1\", linestyle=\"--\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\n\n# Get posterior samples. Thin first 100 samples to remove burn-in\nposterior_samples = params_hmc[100:].detach()\nwith torch.no_grad():\n    y_hat = net(x_lin.unsqueeze(1))\n\n# Plot mean and 95% confidence interval\n\nplt.plot(x_lin, y_hat.ravel(), label=\"Mean\", color=\"C2\")\n\n\n\n\n\ny_hat.ravel()\n\ntensor([-1.4220, -1.3707, -1.3194, -1.2681, -1.2168, -1.1656, -1.1143, -1.0630,\n        -1.0117, -0.9604, -0.9091, -0.8579, -0.8066, -0.7553, -0.7040, -0.6527,\n        -0.6014, -0.5501, -0.4989, -0.4476, -0.3963, -0.3450, -0.2937, -0.2424,\n        -0.1912, -0.1399, -0.0886, -0.0373,  0.0140,  0.0653,  0.1165,  0.1678,\n         0.2191,  0.2704,  0.3217,  0.3730,  0.4242,  0.4755,  0.5268,  0.5781,\n         0.6294,  0.6807,  0.7320,  0.7832,  0.8345,  0.8858,  0.9371,  0.9884,\n         1.0397,  1.0909,  1.1422,  1.1935,  1.2448,  1.2961,  1.3474,  1.3986,\n         1.4499,  1.5012,  1.5525,  1.6038,  1.6551,  1.7064,  1.7576,  1.8089,\n         1.8602,  1.9115,  1.9628,  2.0141,  2.0653,  2.1166,  2.1679,  2.2192,\n         2.2705,  2.3218,  2.3730,  2.4243,  2.4756,  2.5269,  2.5782,  2.6295,\n         2.6808,  2.7320,  2.7833,  2.8346,  2.8859,  2.9372,  2.9885,  3.0397,\n         3.0910,  3.1423])\n\n\n\n# Now, solve the above using Hamiltorch's MCMC sample_model function\n\n\n### Bayesian Logistic Regression\n\nfrom sklearn.datasets import make_moons\n\n# Generate data\nx, y = make_moons(n_samples=1000, noise=0.1, random_state=0)\n\nplt.scatter(x[:, 0], x[:, 1], c=y)\n\nx = torch.tensor(x).float()\ny = torch.tensor(y).float()"
  },
  {
    "objectID": "notebooks/quantile-regression.html",
    "href": "notebooks/quantile-regression.html",
    "title": "Probabilistic Machine Learning",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n%matplotlib inline\n# Retina display\n%config InlineBackend.figure_format = 'retina'\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\n\n# Generating data with heteroscedastic noise\nx_lin = torch.linspace(-5, 5, 200)\nf_true = lambda x: torch.sin(x) + 0.5 * x\neps = torch.randn_like(x_lin)*0.1*(x_lin+5) + 0.1*torch.cos(x_lin)*0.1*(x_lin-5)\ny_lin = f_true(x_lin) + eps\n\nplt.plot(x_lin, f_true(x_lin), 'k--')\nplt.plot(x_lin, y_lin, 'o', alpha=0.5)\nplt.xlabel('x')\nplt.ylabel('y')\n\nText(0, 0.5, 'y')\n\n\n\n\n\n\n# Define a simple neural network with three outputs corresponding to the three quantiles\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(1, 20)\n        self.fc2 = nn.Linear(20, 3)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\ndef loss(y_pred, y, tau):\n    e = -1*(y_pred - y) #check for sign\n    return torch.mean(torch.max(tau*e, (tau-1)*e))\n\nnet = Net()\noptimizer = optim.Adam(net.parameters(), lr=0.01)\n\n\nnet(x_lin[0].reshape(-1, 1))\n\ntensor([[-1.3076, -1.6280, -0.6429]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\ntaus = [0.1, 0.5, 0.9]\n\n# Plot the predictions of the network before training\ndef plot_quantiles(net):\n    y_pred = net(x_lin.reshape(-1, 1))\n    plt.plot(x_lin, f_true(x_lin), 'k--')\n    plt.plot(x_lin, y_lin, 'o', alpha=0.5)\n    for i, tau in enumerate(taus):\n        plt.plot(x_lin, y_pred[:, i].detach(), label=tau)\n    plt.legend()\nplot_quantiles(net)\n\n\n\n\n\nfor epoch in range(200):\n    optimizer.zero_grad()\n    y_pred = net(x_lin.unsqueeze(1))\n    loss_val = sum([loss(y_pred[:, i], y_lin, tau) for i, tau in enumerate(taus)])\n    loss_val.backward()\n    optimizer.step()\n    if epoch % 100 == 0:\n        print('Epoch {}: loss {}'.format(epoch, loss_val.item()))\n\nEpoch 0: loss 1.4152570962905884\nEpoch 100: loss 0.4124695360660553\n\n\n\nplot_quantiles(net)\n\n\n\n\n\nfor i, tau in enumerate(taus):\n    print(f'Fraction of points lesser than {tau}th quantile: {(y_lin &lt; y_pred[:, i]).float().mean():0.2f}')\n\nFraction of points lesser than 0.1th quantile: 0.08\nFraction of points lesser than 0.5th quantile: 0.49\nFraction of points lesser than 0.9th quantile: 0.90"
  },
  {
    "objectID": "notebooks/hierarchical_lr_numpyro.html",
    "href": "notebooks/hierarchical_lr_numpyro.html",
    "title": "Vanilla Linear Regression",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n%matplotlib inline\n# Retina display\n%config InlineBackend.figure_format = 'retina'\nfrom jax import random\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams['figure.constrained_layout.use'] = True\n\n\nimport seaborn as sns\nsns.set_context(\"notebook\")\n\n\nURL = \"https://gist.githubusercontent.com/ucals/\" + \"2cf9d101992cb1b78c2cdd6e3bac6a4b/raw/\"+ \"43034c39052dcf97d4b894d2ec1bc3f90f3623d9/\"+ \"osic_pulmonary_fibrosis.csv\"\n\n\ntrain = pd.read_csv(URL)\ntrain.head()\n\n\n  \n    \n\n\n\n\n\n\nPatient\nWeeks\nFVC\nPercent\nAge\nSex\nSmokingStatus\n\n\n\n\n0\nID00007637202177411956430\n-4\n2315\n58.253649\n79\nMale\nEx-smoker\n\n\n1\nID00007637202177411956430\n5\n2214\n55.712129\n79\nMale\nEx-smoker\n\n\n2\nID00007637202177411956430\n7\n2061\n51.862104\n79\nMale\nEx-smoker\n\n\n3\nID00007637202177411956430\n9\n2144\n53.950679\n79\nMale\nEx-smoker\n\n\n4\nID00007637202177411956430\n11\n2069\n52.063412\n79\nMale\nEx-smoker\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ntrain.describe()\n\n\n  \n    \n\n\n\n\n\n\nWeeks\nFVC\nPercent\nAge\n\n\n\n\ncount\n1549.000000\n1549.000000\n1549.000000\n1549.000000\n\n\nmean\n31.861846\n2690.479019\n77.672654\n67.188509\n\n\nstd\n23.247550\n832.770959\n19.823261\n7.057395\n\n\nmin\n-5.000000\n827.000000\n28.877577\n49.000000\n\n\n25%\n12.000000\n2109.000000\n62.832700\n63.000000\n\n\n50%\n28.000000\n2641.000000\n75.676937\n68.000000\n\n\n75%\n47.000000\n3171.000000\n88.621065\n72.000000\n\n\nmax\n133.000000\n6399.000000\n153.145378\n88.000000\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n# Number of unique patients\ntrain['Patient'].nunique()\n\n176\n\n\n\n#Number of records per patient\ntrain['Patient'].value_counts().reset_index()['Patient'].value_counts().sort_index().plot(kind='bar')\nplt.xlabel(\"Number of weeks\")\nplt.ylabel(\"Number of Patients\")\n\nText(0, 0.5, 'Number of Patients')\n\n\n\n\n\n\ndef chart_patient(patient_id, ax):\n    data = train[train[\"Patient\"] == patient_id]\n    x = data[\"Weeks\"]\n    y = data[\"FVC\"]\n    ax.set_title(patient_id, fontsize=8)\n    sns.regplot(x=x, y=y, ax=ax, ci=None, line_kws={\"color\": \"black\", \"lw\":2})\n\n\nf, axes = plt.subplots(1, 3, sharey=True)\nchart_patient(\"ID00007637202177411956430\", axes[0])\nchart_patient(\"ID00009637202177434476278\", axes[1])\nchart_patient(\"ID00010637202177584971671\", axes[2])\nf.tight_layout()\n\n\n\n\n\ntry:\n    import numpyro\nexcept ImportError:\n    %pip install numpyro\n    import numpyro\n\n\nimport numpyro.distributions as dist\n\n\nsample_weeks = train[\"Weeks\"].values\nsample_fvc = train[\"FVC\"].values\n\n\n### Linear regression from scikit-learn\nfrom sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\nlr.fit(sample_weeks.reshape(-1, 1), sample_fvc)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nall_weeks = np.arange(-12, 134, 1)\n\n\n# Plot the data and the regression line\nplt.scatter(sample_weeks, sample_fvc, alpha=0.3)\nplt.plot(all_weeks, lr.predict(all_weeks.reshape(-1, 1)), color=\"black\", lw=2)\nplt.xlabel(\"Weeks\")\nplt.ylabel(\"FVC\")\n\nText(0, 0.5, 'FVC')\n\n\n\n\n\n\nlr.coef_, lr.intercept_\n\n(array([-1.48471319]), 2737.784722381955)\n\n\n\n# Finding the mean absolute error\n\nfrom sklearn.metrics import mean_absolute_error\n\nmaes = {}\nmaes[\"LinearRegression\"] = mean_absolute_error(sample_fvc, lr.predict(sample_weeks.reshape(-1, 1)))\nmaes\n\n{'LinearRegression': 654.8103093180237}\n\n\n\nPooled model\n\\(\\alpha \\sim \\text{Normal}(0, 500)\\)\n\\(\\beta \\sim \\text{Normal}(0, 500)\\)\n\\(\\sigma \\sim \\text{HalfNormal}(100)\\)\nfor i in range(N_Weeks):\n\\(FVC_i \\sim \\text{Normal}(\\alpha + \\beta \\cdot Week_i, \\sigma)\\)\n\ndef pooled_model(sample_weeks, sample_fvc=None):\n    α = numpyro.sample(\"α\", dist.Normal(0., 500.))\n    β = numpyro.sample(\"β\", dist.Normal(0., 500.))\n    σ = numpyro.sample(\"σ\", dist.HalfNormal(50.))\n    with numpyro.plate(\"samples\", len(sample_weeks)):\n        fvc = numpyro.sample(\"fvc\", dist.Normal(α + β * sample_weeks, σ), obs=sample_fvc)\n    return fvc\n\n\nsample_weeks.shape\n\n(1549,)\n\n\n\n# Render the model graph\nnumpyro.render_model(pooled_model, model_kwargs={\"sample_weeks\": sample_weeks, \"sample_fvc\": sample_fvc},\n                render_distributions=True,\n                render_params=True,\n                )\n\n\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\npatient_encoder = LabelEncoder()\ntrain[\"patient_code\"] = patient_encoder.fit_transform(train[\"Patient\"].values)\n\n\nsample_patient_code = train[\"patient_code\"].values\n\n\nsample_patient_code\n\narray([  0,   0,   0, ..., 175, 175, 175])\n\n\n\nfrom numpyro.infer import MCMC, NUTS, Predictive\n\n\n\nnuts_kernel = NUTS(pooled_model)\n\nmcmc = MCMC(nuts_kernel, num_samples=4000, num_warmup=2000)\nrng_key = random.PRNGKey(0)\n\nWARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\n\nmcmc.run(rng_key, sample_weeks=sample_weeks, sample_fvc=sample_fvc)\nposterior_samples = mcmc.get_samples()\n\nsample: 100%|██████████| 6000/6000 [00:05&lt;00:00, 1137.39it/s, 7 steps of size 5.24e-01. acc. prob=0.88]\n\n\n\nimport arviz as az\n\nidata = az.from_numpyro(mcmc)\naz.plot_trace(idata, compact=True);\n\n\n\n\n\n# Summary statistics\naz.summary(idata, round_to=2)\n\nShape validation failed: input_shape: (1, 4000), minimum_shape: (chains=2, draws=4)\n\n\n\n  \n    \n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nα\n2724.98\n33.46\n2663.52\n2786.90\n0.74\n0.52\n2052.67\n1952.52\nNaN\n\n\nβ\n-1.22\n0.85\n-2.80\n0.43\n0.02\n0.01\n1955.95\n2019.17\nNaN\n\n\nσ\n775.05\n12.03\n752.85\n797.95\n0.25\n0.17\n2411.55\n2720.36\nNaN\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n# Predictive distribution\npredictive = Predictive(pooled_model, mcmc.get_samples())\n\n\npredictive\n\n&lt;numpyro.infer.util.Predictive at 0x7d2774b0b0a0&gt;\n\n\n\npredictions = predictive(rng_key, all_weeks, None)\n\n\npd.DataFrame(predictions[\"fvc\"]).mean().plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nplt.plot(all_weeks, predictions[\"fvc\"].mean(axis=0))\nplt.scatter(sample_weeks, sample_fvc, alpha=0.1)\n\n&lt;matplotlib.collections.PathCollection at 0x7d2774851d50&gt;\n\n\n\n\n\n\n# Get the mean and standard deviation of the predictions\nmu = predictions[\"fvc\"].mean(axis=0)\nsigma = predictions[\"fvc\"].std(axis=0)\n\n# Plot the predictions\nplt.plot(all_weeks, mu)\nplt.fill_between(all_weeks, mu - 1.96*sigma, mu + 1.96*sigma, alpha=0.2)\nplt.scatter(sample_weeks, sample_fvc, alpha=0.2)\nplt.xlabel(\"Weeks\")\nplt.ylabel(\"FVC\")\n\nText(0, 0.5, 'FVC')\n\n\n\n\n\n\npreds_pooled  = predictive(rng_key, sample_weeks, None)['fvc']\npredictions_train_pooled = preds_pooled.mean(axis=0)\nstd_train_pooled = preds_pooled.std(axis=0)\n\n\n### Computing Mean Absolute Error and Coverage at 95% confidence interval\n\nmaes[\"PooledModel\"] = mean_absolute_error(sample_fvc, predictions_train_pooled)\nmaes\n\n{'LinearRegression': 654.8103093180237, 'PooledModel': 654.6283571306085}\n\n\n\n### Computing the coverage at 95% confidence interval\n\ndef coverage(y_true, y_pred, sigma):\n    lower = y_pred - 1.96 * sigma\n    upper = y_pred + 1.96 * sigma\n    return np.mean((y_true &gt;= lower) & (y_true &lt;= upper))\ncoverages = {}\ncoverages[\"pooled\"] = coverage(sample_fvc, predictions_train_pooled, std_train_pooled).item()\ncoverages\n\n{'pooled': 0.9399612545967102}\n\n\nThe above numbers are comparable, which is a good sign. Let’s see if we can do better with a personalised model.\n\n\nUnpooled Model\n\\(\\sigma \\sim \\text{HalfNormal}(20)\\)\n\nfor p in range(N_patients):\n\\(\\alpha_p \\sim \\text{Normal}(0, 500)\\)\n\\(\\beta_p \\sim \\text{Normal}(0, 500)\\)\n\nfor PAT in range(N_patients):\n    for i in range(N_Weeks):\n\\(FVC_i \\sim \\text{Normal}(\\alpha_{p}[PAT] + \\beta_{p}[PAT] \\cdot Week_i, \\sigma)\\)\n\n# Unpooled model\ndef unpool_model(sample_weeks, sample_patient_code, sample_fvc=None):\n    σ = numpyro.sample(\"σ\", dist.HalfNormal(20.))\n    with numpyro.plate(\"patients\", sample_patient_code.max() + 1):\n        α_p = numpyro.sample(\"α_p\", dist.Normal(0, 500.))\n        β_p = numpyro.sample(\"β_p\", dist.Normal(0, 500.))\n    with numpyro.plate(\"samples\", len(sample_weeks)):\n        fvc = numpyro.sample(\"fvc\", numpyro.distributions.Normal(α_p[sample_patient_code] +\n                                                                 β_p[sample_patient_code] * sample_weeks, σ),\n                             obs=sample_fvc)\n    return fvc\n\n\n# Render the model graph\n\nmodel_kwargs = {\"sample_weeks\": sample_weeks,\n                \"sample_patient_code\": sample_patient_code,\n                \"sample_fvc\": sample_fvc}\n\nnumpyro.render_model(unpool_model, model_kwargs=model_kwargs,\n                render_distributions=True,\n                render_params=True,\n                )\n\n\n\n\n\nnuts_kernel_unpooled = NUTS(unpool_model)\n\nmcmc_unpooled = MCMC(nuts_kernel_unpooled, num_samples=200, num_warmup=500)\nrng_key = random.PRNGKey(0)\n\n\nmodel_kwargs\n\n{'sample_weeks': array([-4,  5,  7, ..., 31, 43, 59]),\n 'sample_patient_code': array([  0,   0,   0, ..., 175, 175, 175]),\n 'sample_fvc': array([2315, 2214, 2061, ..., 2908, 2975, 2774])}\n\n\n\nmcmc_unpooled.run(rng_key, **model_kwargs)\n\nposterior_samples = mcmc_unpooled.get_samples()\n\nsample: 100%|██████████| 700/700 [00:07&lt;00:00, 90.53it/s, 63 steps of size 9.00e-02. acc. prob=0.91] \n\n\n\naz.plot_trace(az.from_numpyro(mcmc_unpooled), compact=True);\n\n\n\n\n\n# Predictive distribution for unpooled model\n\npredictive_unpooled = Predictive(unpool_model, mcmc_unpooled.get_samples())\n\n\n# Predictive distribution for unpooled model for all weeks for a given patient\n\nall_weeks = np.arange(-12, 134, 1)\ndef predict_unpooled(patient_code):\n    predictions = predictive_unpooled(rng_key, all_weeks, patient_code)\n    mu = predictions[\"fvc\"].mean(axis=0)\n    sigma = predictions[\"fvc\"].std(axis=0)\n    return mu, sigma\n\n# Plot the predictions for a given patient\ndef plot_patient(patient_code):\n    mu, sigma = predict_unpooled(patient_code)\n    plt.plot(all_weeks, mu)\n    plt.fill_between(all_weeks, mu - 1.96*sigma, mu + 1.96*sigma, alpha=0.6)\n    id_to_patient = patient_encoder.inverse_transform(patient_code)[0]\n    patient_weeks = train[train[\"Patient\"] == id_to_patient][\"Weeks\"]\n    patient_fvc = train[train[\"Patient\"] == id_to_patient][\"FVC\"]\n    plt.scatter(patient_weeks, patient_fvc, alpha=0.5)\n    plt.xlabel(\"Weeks\")\n    plt.ylabel(\"FVC\")\n    plt.title(id_to_patient)\n\n\nplot_pooled = False\n\n\ndef plot_total(patient_id = 0, plot_pooled = False):\n  plot_patient(np.array([patient_id]))\n  if plot_pooled:\n      plt.plot(all_weeks, mu, color='g')\n      plt.fill_between(all_weeks, mu - 1.96*sigma, mu + 1.96*sigma, alpha=0.05, color='g')\n\nplot_total(0, True)\n\n\n\n\n\nplot_total(0, True)\n\n\n\n\n\nplot_total(1, True)\n\n\n\n\n\nplot_total(3)\n\n\n\n\nComputing metrics on the training set for unpool_model\n\n\npredictions_train_unpooled = predictive_unpooled(rng_key,\n                                                 sample_weeks,\n                                                 train[\"patient_code\"].values)['fvc']\npredictions_train_unpooled.shape\n\n(200, 1549)\n\n\n\nmu_predictions_train_unpooled = predictions_train_unpooled.mean(axis=0)\nstd_predictions_train_unpooled = predictions_train_unpooled.std(axis=0)\n\nmaes[\"UnpooledModel\"] = mean_absolute_error(sample_fvc, mu_predictions_train_unpooled)\nmaes\n\n{'LinearRegression': 654.8103093180237,\n 'PooledModel': 654.6283571306085,\n 'UnpooledModel': 98.18583518632232}\n\n\n\ncoverages[\"unpooled\"] = coverage(sample_fvc, mu_predictions_train_unpooled, std_predictions_train_unpooled).item()\ncoverages\n\n{'pooled': 0.9399612545967102, 'unpooled': 0.974822461605072}\n\n\n\n\nExercise: Another variation of unpooled model\n\n# Unpooled model\ndef unpool_model_var(sample_weeks, sample_patient_code, sample_fvc=None):\n    with numpyro.plate(\"patients\", sample_patient_code.max() + 1):\n        α_p = numpyro.sample(\"α_p\", dist.Normal(0, 500.))\n        β_p = numpyro.sample(\"β_p\", dist.Normal(0, 500.))\n        σ_p = numpyro.sample(\"σ_p\", dist.HalfNormal(20.))\n    with numpyro.plate(\"samples\", len(sample_weeks)):\n        fvc = numpyro.sample(\"fvc\", numpyro.distributions.Normal(α_p[sample_patient_code] +\n                                                                 β_p[sample_patient_code] * sample_weeks,\n                                                                 σ_p[sample_patient_code]),\n                             obs=sample_fvc)\n    return fvc\n\n# Plot graphical model\nnumpyro.render_model(unpool_model_var, model_kwargs=model_kwargs,\n                render_distributions=True,\n                render_params=True,\n                )\n\n\n\n\n\nnuts_kernel_unpooled_2 = NUTS(unpool_model_var)\n\nmcmc_unpooled_2 = MCMC(nuts_kernel_unpooled_2, num_samples=400, num_warmup=500)\nrng_key = random.PRNGKey(0)\n\nmcmc_unpooled_2.run(rng_key, **model_kwargs)\n\nposterior_samples_2 = mcmc_unpooled_2.get_samples()\n\nsample: 100%|██████████| 900/900 [00:08&lt;00:00, 102.48it/s, 63 steps of size 8.17e-02. acc. prob=0.88]\n\n\n\naz.plot_trace(az.from_numpyro(mcmc_unpooled_2), compact=True);\n\n\n\n\n\n# Predictive distribution for unpooled model variation 2\n\npredictive_unpooled_2 = Predictive(unpool_model_var, mcmc_unpooled_2.get_samples())\n\n\npredictions_train_unpooled_2 = predictive_unpooled_2(rng_key,\n                                                        sample_weeks,\n                                                        train[\"patient_code\"].values)['fvc']\n\nmu_predictions_train_unpooled_2 = predictions_train_unpooled_2.mean(axis=0)\nstd_predictions_train_unpooled_2 = predictions_train_unpooled_2.std(axis=0)\n\nmaes[\"UnpooledModel2\"] = mean_absolute_error(sample_fvc, mu_predictions_train_unpooled_2)\n\ncoverages[\"unpooled2\"] = coverage(sample_fvc, mu_predictions_train_unpooled_2, std_predictions_train_unpooled_2).item()\n\nprint(maes)\nprint(coverages)\n\n{'LinearRegression': 654.8103093180237, 'PooledModel': 654.6283571306085, 'UnpooledModel': 98.18583518632232, 'UnpooledModel2': 81.93005607511398}\n{'pooled': 0.9399612545967102, 'unpooled': 0.974822461605072, 'unpooled2': 0.886378288269043}\n\n\n\n\nHierarchical model\n\\(\\sigma \\sim \\text{HalfNormal}(100)\\)\n\n\\(\\mu_{\\alpha} \\sim \\text{Normal}(0, 500)\\)\n\\(\\sigma_{\\alpha} \\sim \\text{HalfNormal}(100)\\)\n\\(\\mu_{\\beta} \\sim \\text{Normal}(0, 500)\\)\n\\(\\sigma_{\\beta} \\sim \\text{HalfNormal}(100)\\)\n\nfor p in range(N_patients):\n\\(\\alpha_p \\sim \\text{Normal}(\\mu_{\\alpha}, \\sigma_{\\alpha})\\)\n\\(\\beta_p \\sim \\text{Normal}(\\mu_{\\beta}, \\sigma_{\\beta})\\)\n\nfor i in range(N_Weeks):\n\\(FVC_i \\sim \\text{Normal}(\\alpha_{p[i]} + \\beta_{p[i]} \\cdot Week_i, \\sigma)\\)\n\n### Hierarchical model\n\ndef final_model(sample_weeks, sample_patient_code, sample_fvc=None):\n    μ_α = numpyro.sample(\"μ_α\", dist.Normal(0.0, 500.0))\n    σ_α = numpyro.sample(\"σ_α\", dist.HalfNormal(100.0))\n    μ_β = numpyro.sample(\"μ_β\", dist.Normal(0.0, 3.0))\n    σ_β = numpyro.sample(\"σ_β\", dist.HalfNormal(3.0))\n\n    n_patients = len(np.unique(sample_patient_code))\n\n    with numpyro.plate(\"Participants\", n_patients):\n        α = numpyro.sample(\"α\", dist.Normal(μ_α, σ_α))\n        β = numpyro.sample(\"β\", dist.Normal(μ_β, σ_β))\n\n    σ = numpyro.sample(\"σ\", dist.HalfNormal(100.0))\n    FVC_est = α[sample_patient_code] + β[sample_patient_code] * sample_weeks\n\n    with numpyro.plate(\"data\", len(sample_patient_code)):\n        numpyro.sample(\"fvc\", dist.Normal(FVC_est, σ), obs=sample_fvc)\n\n\n# Render the model graph\n\n\nnumpyro.render_model(final_model, model_kwargs=model_kwargs,\n                render_distributions=True,\n                render_params=True,\n                )\n\n\n\n\n\nnuts_final = NUTS(final_model)\n\nmcmc_final = MCMC(nuts_final, num_samples=4000, num_warmup=2000)\nrng_key = random.PRNGKey(0)\n\n\nmcmc_final.run(rng_key, **model_kwargs)\n\nsample: 100%|██████████| 6000/6000 [00:50&lt;00:00, 117.83it/s, 63 steps of size 1.04e-02. acc. prob=0.84]\n\n\n\npredictive_final = Predictive(final_model, mcmc_final.get_samples())\n\n\naz.plot_trace(az.from_numpyro(mcmc_final), compact=True);\n\n\n\n\n\npredictive_hierarchical = Predictive(final_model, mcmc_final.get_samples())\n\n\npredictions_train_hierarchical = predictive_hierarchical(rng_key,\n                                                         sample_weeks = model_kwargs[\"sample_weeks\"],\n                                                         sample_patient_code = model_kwargs[\"sample_patient_code\"])['fvc']\n\nmu_predictions_train_h = predictions_train_hierarchical.mean(axis=0)\nstd_predictions_train_h = predictions_train_hierarchical.std(axis=0)\n\nmaes[\"Hierarchical\"] = mean_absolute_error(sample_fvc, mu_predictions_train_h)\n\ncoverages[\"Hierarchical\"] = coverage(sample_fvc, mu_predictions_train_h, std_predictions_train_h).item()\n\nprint(maes)\nprint(coverages)\n\n{'LinearRegression': 654.8103093180237, 'PooledModel': 654.6283571306085, 'UnpooledModel': 98.18583518632232, 'UnpooledModel2': 81.93005607511398, 'Hierarchical': 83.05726212759492}\n{'pooled': 0.9399612545967102, 'unpooled': 0.974822461605072, 'unpooled2': 0.886378288269043, 'Hierarchical': 0.9754680395126343}\n\n\n\npd.Series(maes)\n\nLinearRegression    654.810309\nPooledModel         654.628357\nUnpooledModel        98.185835\nUnpooledModel2       81.930056\nHierarchical         83.057262\ndtype: float64\n\n\n\n# Predict for a given patient\n\n\ndef predict_final(patient_code):\n    predictions = predictive_final(rng_key, all_weeks, patient_code)\n    mu = predictions[\"fvc\"].mean(axis=0)\n    sigma = predictions[\"fvc\"].std(axis=0)\n    return mu, sigma\n\n# Plot the predictions for a given patient\ndef plot_patient_final(patient_code):\n    mu, sigma = predict_final(patient_code)\n    plt.plot(all_weeks, mu)\n    plt.fill_between(all_weeks, mu - sigma, mu + sigma, alpha=0.1)\n    id_to_patient = patient_encoder.inverse_transform([patient_code])[0]\n    #print(id_to_patient[0], patient_code)\n    #print(patient_code, id_to_patient)\n    patient_weeks = train[train[\"Patient\"] == id_to_patient][\"Weeks\"]\n    patient_fvc = train[train[\"Patient\"] == id_to_patient][\"FVC\"]\n    plt.scatter(patient_weeks, patient_fvc, alpha=0.5)\n    #plt.scatter(sample_weeks[train[\"patient_code\"] == patient_code.item()], fvc[train[\"patient_code\"] == patient_code.item()], alpha=0.5)\n    plt.xlabel(\"Weeks\")\n    plt.ylabel(\"FVC\")\n    plt.title(patient_encoder.inverse_transform([patient_code])[0])\n\n\n# plot for a given patient\nplot_patient_final(np.array([0]))\n\n\n\n\n\nplot_total(0)\n\n\n\n\n\n\nQuestions to ponder\n\nIncorporating extra information\nHow to get sigma for sklearn model\nHow to predict for a new partipant in the hierarchical model\n\nAssuming they have some observations\n\nAdd the data from this participant at the training time\n“Fine-tune” on this participant data\n\nAssuming they do not have any observations"
  },
  {
    "objectID": "notebooks/baseball.html",
    "href": "notebooks/baseball.html",
    "title": "Baseball",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 12\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\n\nfrom ISLP import load_data\nHitters = load_data('Hitters')\n\n\nHitters\n\n\n\n\n\n\n\n\nAtBat\nHits\nHmRun\nRuns\nRBI\nWalks\nYears\nCAtBat\nCHits\nCHmRun\nCRuns\nCRBI\nCWalks\nLeague\nDivision\nPutOuts\nAssists\nErrors\nSalary\nNewLeague\n\n\n\n\n0\n293\n66\n1\n30\n29\n14\n1\n293\n66\n1\n30\n29\n14\nA\nE\n446\n33\n20\nNaN\nA\n\n\n1\n315\n81\n7\n24\n38\n39\n14\n3449\n835\n69\n321\n414\n375\nN\nW\n632\n43\n10\n475.0\nN\n\n\n2\n479\n130\n18\n66\n72\n76\n3\n1624\n457\n63\n224\n266\n263\nA\nW\n880\n82\n14\n480.0\nA\n\n\n3\n496\n141\n20\n65\n78\n37\n11\n5628\n1575\n225\n828\n838\n354\nN\nE\n200\n11\n3\n500.0\nN\n\n\n4\n321\n87\n10\n39\n42\n30\n2\n396\n101\n12\n48\n46\n33\nN\nE\n805\n40\n4\n91.5\nN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n317\n497\n127\n7\n65\n48\n37\n5\n2703\n806\n32\n379\n311\n138\nN\nE\n325\n9\n3\n700.0\nN\n\n\n318\n492\n136\n5\n76\n50\n94\n12\n5511\n1511\n39\n897\n451\n875\nA\nE\n313\n381\n20\n875.0\nA\n\n\n319\n475\n126\n3\n61\n43\n52\n6\n1700\n433\n7\n217\n93\n146\nA\nW\n37\n113\n7\n385.0\nA\n\n\n320\n573\n144\n9\n85\n60\n78\n8\n3198\n857\n97\n470\n420\n332\nA\nE\n1314\n131\n12\n960.0\nA\n\n\n321\n631\n170\n9\n77\n44\n31\n11\n4908\n1457\n30\n775\n357\n249\nA\nW\n408\n4\n3\n1000.0\nA\n\n\n\n\n322 rows × 20 columns\n\n\n\n\n(Hitters.Hits/Hitters.AtBat).hist(bins=50)\nplt.grid(False)\nplt.xlabel(\"Batting Average\")\nplt.savefig(\"baseball-a.pdf\")\n\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\n\n\n\n\n\n\nimport torch.distributions as dist\nimport torch\n\n\nparams = {\"A\": (3, 10),\n         \"B\": (250, 400),\n         \"C\": (153, 450),\n         }\n\ncolors = [\"g\", \"b\", \"k\"]\nlines = [\"--\", \"-.\", \":\", \"-\"]\n(Hitters.Hits/Hitters.AtBat).hist(bins=50, alpha=0.2)\n\nx_lin = torch.linspace(0.0, 0.5, 100)\nfor i, (name, d) in enumerate(params.items()):\n    plt.plot(x_lin, dist.Beta(*d).log_prob(x_lin).exp(), \n             label=f\"{name}: Beta{d}\", lw=4, ls=lines[i], c=colors[i])\nplt.legend(title=\"Prior\")\nplt.grid(False)\nplt.xlabel(\"Batting Average\")\nplt.savefig(\"baseball-b.pdf\")\n\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\n\n\n\n\n\n\nimport numpy as np\nfrom scipy import stats\n\n# Your data\ndata = (Hitters.Hits / Hitters.AtBat).values\n\n# Define the distribution you want to fit (for example, a beta distribution)\ndistribution = stats.beta\n\n\n\n# Specify bounds for the parameters\nbounds = ((1, 300), (1, 300))\n\n# Fit the distribution with bounds\nres = stats.fit(distribution, data, bounds=bounds)\n\n\nres\n\n  params: FitParams(a=33.704412916832084, b=96.02829213089092, loc=0.0, scale=1.0)\n success: True\n message: 'Optimization terminated successfully.'"
  },
  {
    "objectID": "notebooks/error-code-dilbert.html",
    "href": "notebooks/error-code-dilbert.html",
    "title": "Error Code Dilbert",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n%matplotlib inline\n\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n\n\n!wget https://upload.wikimedia.org/wikipedia/en/f/f3/Dilbert-20050910.png\n\n--2023-06-08 09:33:06--  https://upload.wikimedia.org/wikipedia/en/f/f3/Dilbert-20050910.png\nResolving upload.wikimedia.org (upload.wikimedia.org)... 103.102.166.240, 2001:df2:e500:ed1a::2:b\nConnecting to upload.wikimedia.org (upload.wikimedia.org)|103.102.166.240|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 10713 (10K) [image/png]\nSaving to: ‘Dilbert-20050910.png’\n\nDilbert-20050910.pn 100%[===================&gt;]  10.46K  --.-KB/s    in 0s      \n\n2023-06-08 09:33:06 (212 MB/s) - ‘Dilbert-20050910.png’ saved [10713/10713]\n\n\n\n\n# Read Dilbert image as black and white\ndilbert = plt.imread('Dilbert-20050910.png')[:,:,0]\n\n\n# Plot Dilbert without axis\nplt.imshow(dilbert, cmap='gray')\nplt.axis('off')\n\n(-0.5, 196.5, 184.5, -0.5)\n\n\n\n\n\n\n# Binarize Dilbert\ndilbert = (dilbert &gt; 0.5).astype(int)\n\n# Plot Dilbert without axis\nplt.imshow(dilbert, cmap='gray')\nplt.axis('off')\n\n(-0.5, 196.5, 184.5, -0.5)\n\n\n\n\n\n\n# Function to flip a bit from 0 to 1 or 1 to 0 with a probability f\ndef flip_bit(x, f):\n    if np.random.rand() &lt; f:\n        return 1 - x\n    else:\n        return x\n    \n# Function to generate a noisy image\ndef noisy_image(image, f):\n    return np.array([[flip_bit(x, f) for x in row] for row in image])\n\n\nnoisy_image1 = noisy_image(dilbert, 0.1)\nnoisy_image2 = noisy_image(dilbert, 0.2)\n\n# Plot noisy images\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 3, 1)\nplt.imshow(dilbert, cmap='gray')\nplt.axis('off')\nplt.title('Original')\nplt.subplot(1, 3, 2)\nplt.imshow(noisy_image1, cmap='gray')\nplt.axis('off')\nplt.title('Noisy 1')\nplt.subplot(1, 3, 3)\nplt.imshow(noisy_image2, cmap='gray')\nplt.axis('off')\nplt.title('Noisy 2')\n\nText(0.5, 1.0, 'Noisy 2')\n\n\n\n\n\n\n# Function to generate N noisy images\n\ndef generate_noisy_images(image, f, N):\n    return np.array([noisy_image(image, f) for i in range(N)])\n\n# Function to plot original and N noisy images\ndef plot_noisy_images(image, noisy_images):\n    N = len(noisy_images)\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, N+1, 1)\n    plt.imshow(image, cmap='gray')\n    plt.axis('off')\n    plt.title('Original')\n    for i in range(N):\n        plt.subplot(1, N+1, i+2)\n        plt.imshow(noisy_images[i], cmap='gray')\n        plt.axis('off')\n        plt.title('Noisy {}'.format(i+1))\n\n\nnoisy_images = generate_noisy_images(dilbert, 0.1, 3)\nplot_noisy_images(dilbert, noisy_images)\n\n\n\n\n\n# Build the decoder which does a majority vote across the N noisy images\n# Each image contains only 0s and 1s, so the majority vote is argmax\n\n# Average across the N noisy images\ndef average_noisy_images(noisy_images):\n    return np.mean(noisy_images, axis=0)\n\n# plot the average noisy image\naverage_noisy_image = average_noisy_images(noisy_images)\nplot_noisy_images(dilbert, [average_noisy_image])\n\n\n\n\n\n\ndilbert\n\narray([[1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       ...,\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 1, 1, 1]])\n\n\n\ndecoded_image = (average_noisy_image &gt; 0.5).astype(int)\ndecoded_image\n\narray([[1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       ...,\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 1, 1, 1]])\n\n\n\n# Plot the decoded image\nplt.imshow(decoded_image, cmap='gray')\nplt.axis('off')\n\n(-0.5, 196.5, 184.5, -0.5)\n\n\n\n\n\n\n# Error between original and decoded image\ndef error(image, decoded_image):\n    return np.sum(image != decoded_image) / image.size\n\n# Error between original and noisy images\ndef error_noisy_images(image, noisy_images):\n    return np.array([error(image, noisy_image) for noisy_image in noisy_images])\n\n# Error between each N noisy images and the original \ndef error_noisy_images(image, noisy_images):\n    return np.array([error(image, noisy_image) for noisy_image in noisy_images])\n\n# Plot the error between each noisy image and the original\nerror_noisy_images(dilbert, noisy_images)\n\narray([0.10155028, 0.10166004, 0.100782  ])\n\n\n\n# Error between decoded image and original\nerror(dilbert, decoded_image)\n\n0.028700782000274386\n\n\n\n# Function to generate noisy images and decode them and compute the error for a given f and N\n# Plot the noisy images and the decoded image.\n# Return the error between the original and decoded image\n# Title the plot with the error\ndef error_f_N(f, N):\n    noisy_images = generate_noisy_images(dilbert, f, N)\n    decoded_image = ( average_noisy_images(noisy_images) &gt; 0.5 ).astype(int)\n    err  = error(dilbert, decoded_image)\n    # Plot all noisy images in first figure and the second figure just shows original and the decoded image\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, N+1, 1)\n    plt.imshow(dilbert, cmap='gray')\n    plt.axis('off')\n    plt.title('Original')\n    for i in range(N):\n        plt.subplot(1, N+1, i+2)\n        plt.imshow(noisy_images[i], cmap='gray')\n        plt.axis('off')\n        # If N &gt; 8 then the title is too long and the plot is not readable so we do not show the title\n        if N &lt;= 8:\n            plt.title('Noisy {}'.format(i+1))\n\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    plt.imshow(dilbert, cmap='gray')\n    plt.axis('off')\n    plt.title('Original')\n    plt.subplot(1, 2, 2)\n    plt.imshow(decoded_image, cmap='gray')\n    plt.title('Decoded (Error: {:.4f})'.format(err))\n    plt.axis('off')\n    \n    return err\n\n\nerror_f_N(0.1, 3)\n\n0.026807518178076552\n\n\n\n\n\n\n\n\n\nerror_f_N(0.1, 5)\n\n0.008698038139662505\n\n\n\n\n\n\n\n\n\nerror_f_N(0.1, 10)\n\n0.0012621758814652215\n\n\n\n\n\n\n\n\n\nerror_f_N(0.4, 10)\n\n0.3344491699821649\n\n\n\n\n\n\n\n\n\nerror_f_N(0.4, 12)\n\n0.3031691590067225\n\n\n\n\n\n\n\n\n\nerror_f_N(0.4, 100)\n\n0.025792289751680613"
  },
  {
    "objectID": "notebooks/mle_multivariate.html",
    "href": "notebooks/mle_multivariate.html",
    "title": "Probabilistic Machine Learning",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Retina display\n%config InlineBackend.figure_format = 'retina'\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\n\nimport torch.distributions as dist\nfrom torch.distributions import MultivariateNormal\n\n\nmean = torch.tensor([2.0, 3.0])\ncovariance_matrix = torch.tensor([[1.0, 0.5],\n                                 [0.5, 2.0]])\nmvn_distribution = MultivariateNormal(\n    loc=mean, covariance_matrix=covariance_matrix)\n\nSampled Data Shape: torch.Size([1000, 2])\nSampled Data Mean: tensor([1.9390, 2.9668])\nSampled Data Covariance Matrix: tensor([[0.9759, 0.4904],\n        [0.4904, 1.9176]])\n\n\n\nnum_samples = 1000\nsamples = mvn_distribution.sample((num_samples,))\n\nprint(\"Sampled Data Mean:\", torch.mean(samples, dim=0))\nprint(\"Sampled Data Covariance Matrix:\", torch.matmul(\n    (samples - mean).T, samples - mean) / num_samples)\nprint(\"Actual Mean:\", mean)\nprint(\"Actual Covariance Matrix:\", covariance_matrix)\n\nSampled Data Mean: tensor([1.9626, 2.9814])\nSampled Data Covariance Matrix: tensor([[1.0030, 0.4721],\n        [0.4721, 1.8350]])\nActual Mean: tensor([2., 3.])\nActual Covariance Matrix: tensor([[1.0000, 0.5000],\n        [0.5000, 2.0000]])\n\n\n\nplt.rcParams.update({'figure.figsize': (6, 4)})\n\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom scipy.stats import multivariate_normal\n\n\nmean = np.array([2.0, 3.0])\ncovariance_matrix = np.array([[1.0, 0.5],\n                              [0.5, 2.0]])\n\nx_range = np.linspace(mean[0] - 3, mean[0] + 3, 100)\ny_range = np.linspace(mean[1] - 3, mean[1] + 3, 100)\nX, Y = np.meshgrid(x_range, y_range)\ngrid = np.column_stack((X.flatten(), Y.flatten()))\n\npdf_values = multivariate_normal.pdf(grid, mean, covariance_matrix)\npdf_values = pdf_values.reshape(X.shape)\n\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nsurf = ax.plot_surface(X, Y, pdf_values, cmap='viridis')\ncset = ax.contourf(X, Y, pdf_values, zdir='z', offset=-0.15, cmap='viridis')\nax.set_zlim(-0.15, 0.1)\nax.set_zticks(np.linspace(0, 0.1, 5))\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Probability Density')\nax.set_title(f'Covariance Matrix:\\n{covariance_matrix}')\nplt.savefig('figures/mle/bivariate_normal.pdf')\nplt.show()\n\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed"
  },
  {
    "objectID": "notebooks/mle_bernoulli.html",
    "href": "notebooks/mle_bernoulli.html",
    "title": "Probabilistic Machine Learning",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Retina display\n%config InlineBackend.figure_format = 'retina'\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\n\nimport torch.distributions as dist\n\n\ntheta = 0.9\n\n\ndef prob(theta, data):\n    dist2 = dist.Bernoulli(probs=theta)\n    prob = np.exp(1)\n    for i in range(len(data)):\n        prob *= dist2.log_prob(data[i]).exp()\n    return prob\n\n\nprobs = []\nber = dist.Bernoulli(probs=theta)\ndata = ber.sample((10,))\n\n\nthetas = np.linspace(0.01, 0.99, 100)\nfor i in range(100):\n    ber = dist.Bernoulli(torch.tensor([theta]))\n    probs.append(prob(theta=thetas[i], data=data))\n\n\nprobs = torch.tensor(probs)\n\n\nplt.plot(thetas, probs)\nplt.xlabel(r'$\\theta$')\nplt.ylabel(r'$p(x|\\theta)$')\nplt.title(r'Likelihood function for Bernoulli distribution')\n# plt.savefig('D:/IITGN/Sem-7/pml/pml-teaching/figures/mle/bernoulli_likelihood_2.pdf')\n\nText(0.5, 1.0, 'Likelihood function for Bernoulli distribution')\n\n\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\nfindfont: Generic family 'sans-serif' not found because none of the following families were found: Roboto Condensed\n\n\n\n\n\n\nfrom ipywidgets import interact\n\n\n@interact(seed=(1, 10))\ndef plot_likelihood(seed):\n    ber = dist.Bernoulli(probs=0.3)\n    data = ber.sample(sample_shape=(10,))\n    thetas = np.linspace(0.01, 0.99, 100)\n    probs = []\n    for i in range(100):\n        ber = dist.Bernoulli(torch.tensor([theta]))\n        probs.append(prob(theta=thetas[i], data=data))\n    probs = torch.tensor(probs)\n    plt.plot(thetas, probs)\n    plt.xticks(np.linspace(0, 1, 11))\n    plt.xlabel(r\"Probability of head: $\\theta$\")\n    plt.ylabel(r\"likelihood: $p(D|\\theta)$\")\n    plt.title(f\"Data ={data}\")\n    plt.grid(True)"
  },
  {
    "objectID": "notebooks/heteroskedastic-n.html",
    "href": "notebooks/heteroskedastic-n.html",
    "title": "Heteroskedastic N",
    "section": "",
    "text": "Empty notebook"
  },
  {
    "objectID": "notebooks/random-uniform.html",
    "href": "notebooks/random-uniform.html",
    "title": "Probabilistic Machine Learning",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n# Retina display\n%config InlineBackend.figure_format = 'retina'\nimport warnings\nwarnings.filterwarnings('ignore')\nimport arviz as az\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\n\nclass SimplePRNG:\n    def __init__(self, seed=0):\n        self.seed = seed\n        self.a = 1664525\n        self.c = 1013904223\n        self.m = 2**32\n\n    def random(self):\n        self.seed = (self.a * self.seed + self.c) % self.m\n        return self.seed / self.m\n\n    def generate_N_random_numbers(self, N):\n        random_numbers = []\n        for _ in range(N):\n            random_numbers.append(self.random())\n        return random_numbers\n\n# Usage\nprng = SimplePRNG(seed=42)  # You can change the seed value\nN = 10000  # Change N to the number of random numbers you want to generate\nrandom_numbers = prng.generate_N_random_numbers(N)\n\n\n\n_ = plt.hist(random_numbers, bins=10)\n\n\n\n\n\naz.plot_kde(np.array(random_numbers), rug=False)\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\n_ = plt.hist(np.random.rand(10000), bins=10)\n\n\n\n\n\naz.plot_kde(np.random.rand(10000), rug=False)\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\n### Uniform (a, b)\n\na = -2\nb = 2\n\nrandom_numbers_a_b = a + (b - a) * np.array(random_numbers)\n\n\nplt.hist(random_numbers_a_b, bins=10)\n\n(array([1006., 1012.,  963.,  964., 1008.,  939., 1039., 1012., 1031.,\n        1026.]),\n array([-1.99864224e+00, -1.59883546e+00, -1.19902869e+00, -7.99221917e-01,\n        -3.99415144e-01,  3.91628593e-04,  4.00198402e-01,  8.00005174e-01,\n         1.19981195e+00,  1.59961872e+00,  1.99942549e+00]),\n &lt;BarContainer object of 10 artists&gt;)"
  },
  {
    "objectID": "notebooks/blr.html",
    "href": "notebooks/blr.html",
    "title": "Bayesian Linear Regression",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate synthetic dataset\ntorch.manual_seed(42)\nnum_samples = 20\nx = torch.linspace(0, 1, num_samples).unsqueeze(1)\ny = 3 * x + 2 * x**2 + 0.2 * torch.randn(num_samples, 1)\n\n# Define polynomial basis features\ndegree = 2\nX_poly = torch.cat([x ** i for i in range(degree + 1)], dim=1)\n\n# Closed-form solutions\nX_T = X_poly.t()\nX_T_X = torch.mm(X_T, X_poly)\nX_T_y = torch.mm(X_T, y)\n\n# MLE solution\ntheta_mle = torch.linalg.solve(X_T_X, X_T_y)\n\n# MAP solution (assuming Gaussian priors)\nlambda_identity = torch.eye(degree + 1)  # Identity matrix as the prior covariance\nlambda_prior = 0.1\ntheta_map = torch.linalg.solve(X_T_X + lambda_prior * lambda_identity, X_T_y + lambda_prior * theta_mle)\n\n# Full posterior (assuming Gaussian posterior)\nposterior_covariance = torch.linalg.inv(X_T_X + lambda_prior * lambda_identity)\nposterior_mean = torch.mm(posterior_covariance, X_T_y)\n\n# Predictions\nx_test = torch.linspace(0, 1, 100).unsqueeze(1)\nX_test_poly = torch.cat([x_test ** i for i in range(degree + 1)], dim=1)\n\npred_mean_mle = torch.mm(X_test_poly, theta_mle)\npred_mean_map = torch.mm(X_test_poly, theta_map)\npred_mean_posterior = torch.mm(X_test_poly, posterior_mean)\n\n# Uncertainties (aleatoric and epistemic)\npred_var_mle = torch.sum((torch.mm(X_test_poly, posterior_covariance) * X_test_poly), dim=1, keepdim=True).squeeze()\npred_std_mle = torch.sqrt(pred_var_mle).squeeze()\n\nplt.figure(figsize=(10, 6))\nplt.scatter(x.numpy(), y.numpy(), label='True Data')\nplt.plot(x_test.numpy(), pred_mean_mle.numpy(), label='MLE Prediction', color='orange')\n\n# Extract 1-dimensional values for x_test\nx_test_1d = x_test.squeeze().numpy()\n\nplt.fill_between(x_test_1d, (pred_mean_mle - 2 * pred_std_mle).numpy(), (pred_mean_mle + 2 * pred_std_mle).numpy(), color='orange', alpha=0.3, label='Epistemic Uncertainty')\nplt.plot(x_test.numpy(), pred_mean_map.numpy(), label='MAP Prediction', color='green')\nplt.plot(x_test.numpy(), pred_mean_posterior.numpy(), label='Full Posterior Prediction', color='blue')\n\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.title('Bayesian Linear Regression')\nplt.legend()\nplt.show()\n\nValueError: 'y1' is not 1-dimensional"
  },
  {
    "objectID": "notebooks/hypernet-np.html",
    "href": "notebooks/hypernet-np.html",
    "title": "Train on all tasks",
    "section": "",
    "text": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n%matplotlib inline\n# Retina display\n%config InlineBackend.figure_format = 'retina'\nimport torch\nimport torch.nn as nn\n\ntorch.set_default_device(\"cuda\")\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\nimport torch.distributions as dist\nμ_α = dist.Normal(0.0, 2.0)\nσ_α =dist.HalfNormal(1.0)\nμ_β = dist.Normal(0.0, 3.0)\nσ_β = dist.HalfNormal(1.0) \n\nn_tasks = 11\ntorch.manual_seed(0)\nμ_α_samples = μ_α.sample((n_tasks,))\nσ_α_samples = σ_α.sample((n_tasks,))\nμ_β_samples = μ_β.sample((n_tasks,))\nσ_β_samples = σ_β.sample((n_tasks,))\ntorch.manual_seed(0)\nα = dist.Normal(μ_α_samples, σ_α_samples).sample()\nβ = dist.Normal(μ_β_samples, σ_β_samples).sample()\n\nσ = dist.HalfNormal(5.0).sample((n_tasks,))\nx_lin = torch.linspace(-1, 1, 100)\n\ntrue_fs = []\nys = []\n\nfor i in range(n_tasks):\n    true_fs.append(α[i] + β[i] * x_lin)\n\n# Add noise\nfor i in range(n_tasks):\n    ys.append(dist.Normal(true_fs[i], σ[i]).sample())\n\n# Normalize both x and y for all tasks\ntrue_fs_norm = []\nys_norm = []\nx_means_task = []\nx_stds_task = []\ny_means_task = []\ny_stds_task = []\n\n\nfor i in range(n_tasks):\n    x_means_task.append(x_lin.mean())\n    x_stds_task.append(x_lin.std())\n    y_means_task.append(ys[i].mean())\n    y_stds_task.append(ys[i].std())\n    ys_norm.append((ys[i] - ys[i].mean()) / ys[i].std())\n    true_fs_norm.append((true_fs[i] - true_fs[i].mean()) / true_fs[i].std())\n# Plot the `n_tasks` functions with noise and the true functions in grid \n# of 2 x 5\n\nfig, axes = plt.subplots(2, 5, figsize=(8, 4), sharex=True, sharey=True)\n\nfor i, ax in enumerate(axes.flatten()):\n    ax.plot(x_lin.cpu(), true_fs[i].cpu(), label='True function')\n    ax.scatter(x_lin.cpu(), ys[i].cpu(), label='Data', s=4, alpha=0.2)\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    # Print the parameters in the title\n    ax.set_title(f'Task {i}\\n α={α[i]:.2f}, β={β[i]:.2f}, σ={σ[i]:.2f}')\n    ax.legend()\nplt.suptitle(r'True functions $(f(x) = \\alpha + \\beta x)$ and data $(y \\sim \\mathcal{N}(f(x), \\sigma))$')\nplt.savefig(\"../diagrams/metalearning/true.pdf\", bbox_inches=\"tight\")\n\nfindfont: Font family ['cursive'] not found. Falling back to DejaVu Sans.\nfindfont: Generic family 'cursive' not found because none of the following families were found: Apple Chancery, Textile, Zapf Chancery, Sand, Script MT, Felipa, Comic Neue, Comic Sans MS, cursive\n# Plot the normalized functions and data in grid of 2 x 5\nfig, axes = plt.subplots(2, 5, figsize=(8, 4), sharex=True, sharey=True)\n\nfor i, ax in enumerate(axes.flatten()):\n    ax.plot(x_lin.cpu(), true_fs_norm[i].cpu(), label='True function')\n    ax.scatter(x_lin.cpu(), ys_norm[i].cpu(), label='Data', s=4, alpha=0.2)\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title(f'Task {i}\\n α={α[i]:.2f}, β={β[i]:.2f}, σ={σ[i]:.2f}')\n    ax.legend()\nplt.suptitle(r'True functions $(f(x) = \\alpha + \\beta x)$ and data $(y \\sim \\mathcal{N}(f(x), \\sigma))$')\n\nText(0.5, 0.98, 'True functions $(f(x) = \\\\alpha + \\\\beta x)$ and data $(y \\\\sim \\\\mathcal{N}(f(x), \\\\sigma))$')\ntorch.manual_seed(0)\n# Plot the last task with few data points (context)\ncontext_size = 5\ncontext_idx = torch.randperm(100)[:context_size]\ncontext_x = x_lin[context_idx]\ncontext_y = ys[-1][context_idx]\n\nplt.scatter(context_x.cpu(), context_y.cpu(), label='Context', s=20, color='k')\nplt.plot(x_lin.cpu(), true_fs[-1].cpu(), label='True function (to estimate)')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.title(\"New Task\")\nplt.savefig(\"../diagrams/metalearning/context.pdf\", bbox_inches=\"tight\")\ntorch.manual_seed(0)\n# Split data across each task into train and test sets\n\nx_train = []\ny_train = []\n\nx_test = []\ny_test = []\n\nfor i in range(n_tasks):\n    # For each task, divide the data into 50% train and 50% context randomly\n    r_perm = torch.randperm(100)\n    train_idx = r_perm[:50]\n    test_idx = r_perm[50:]\n    x_train.append(x_lin[train_idx])\n    y_train.append(ys_norm[i][train_idx])\n    print(x_train[i].shape, y_train[i].shape)\n    x_test.append(x_lin[test_idx])\n    y_test.append(ys_norm[i][test_idx])\n    \n    \n# Plot the train and test sets for each task\n\nfig, axes = plt.subplots(2, 5, figsize=(8, 4), sharex=True, sharey=True)\n\nfor i, ax in enumerate(axes.flatten()):\n    ax.scatter(x_train[i].cpu(), y_train[i].cpu(), label='Train', s=4)\n    ax.scatter(x_test[i].cpu(), y_test[i].cpu(), label='Context', s=4)\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title(f'Task {i}')\n    ax.legend()\n    \n\ntorch.Size([50]) torch.Size([50])\ntorch.Size([50]) torch.Size([50])\ntorch.Size([50]) torch.Size([50])\ntorch.Size([50]) torch.Size([50])\ntorch.Size([50]) torch.Size([50])\ntorch.Size([50]) torch.Size([50])\ntorch.Size([50]) torch.Size([50])\ntorch.Size([50]) torch.Size([50])\ntorch.Size([50]) torch.Size([50])\ntorch.Size([50]) torch.Size([50])\ntorch.Size([50]) torch.Size([50])\ntorch.manual_seed(0)\n# Define the hyper-net and target-net\nhyper_net = torch.nn.Sequential(torch.nn.Linear(2, 64), torch.nn.SELU(), torch.nn.Linear(64, 2))\ntarget_net = torch.nn.Linear(1, 1)\n# Let us pick Task 0\n\ntask = 0\nlen_train = len(x_train[task])\nx_train_task = x_train[task]\ny_train_task = y_train[task]\n\n# Context is 50% of the training data, last 50% is training data\nx_c = x_train_task[:len_train // 2]\ny_c = y_train_task[:len_train // 2]\n\nx_t = x_train_task[len_train // 2:]\ny_t = y_train_task[len_train // 2:]\n\n# Concatenate x_c and y_c to form the context\ncontext = torch.cat([x_c.view(-1, 1), y_c.view(-1, 1)], dim=1)\n\nprint(context.shape, x_t.shape, y_t.shape)\n\ntorch.Size([25, 2]) torch.Size([25]) torch.Size([25])\n# Pass the context to the hyper-net to get output\nhyper_out = hyper_net(context)\nhyper_out.shape\n\ntorch.Size([25, 2])\n# Average the output of the hyper-net to get the weights of the target-net\nweights = hyper_out.mean(dim=0)\nweights.shape\n\ntorch.Size([2])\ntorch.manual_seed(0)\n# Create a new target-net with the weight and bias from the hyper-net\ntarget_net_new = torch.nn.Linear(1, 1)\nprint(target_net_new.weight.data, target_net_new.bias.data)\nprint(target_net_new.state_dict())\n\n# Set the weights and bias of the new target-net from the hyper-net\ntarget_net_new.weight.data = weights[:1].view(1, 1)\ntarget_net_new.bias.data = weights[1:]\n\nprint(target_net_new.weight.data, target_net_new.bias.data)\nprint(target_net_new.state_dict())\n\n# Create torch.func.functional_call to call the target-net with the \n# weights and bias from the hyper-net\n# and pass the test data to get the predictions\n\ntarget_net_new = torch.nn.Linear(1, 1)\nnew_dict = target_net_new.state_dict()\nnew_dict.update({'weight': weights[:1].view(1, 1), 'bias': weights[1:]})\ntarget_net_new.load_state_dict(new_dict)\n\nprint(target_net_new.weight.data, target_net_new.bias.data)\nprint(target_net_new.state_dict())\n\ntensor([[-0.2019]], device='cuda:0') tensor([0.9445], device='cuda:0')\nOrderedDict([('weight', tensor([[-0.2019]], device='cuda:0')), ('bias', tensor([0.9445], device='cuda:0'))])\ntensor([[-0.1925]], device='cuda:0') tensor([-0.2296], device='cuda:0')\nOrderedDict([('weight', tensor([[-0.1925]], device='cuda:0')), ('bias', tensor([-0.2296], device='cuda:0'))])\ntensor([[-0.1925]], device='cuda:0') tensor([-0.2296], device='cuda:0')\nOrderedDict([('weight', tensor([[-0.1925]], device='cuda:0')), ('bias', tensor([-0.2296], device='cuda:0'))])\n# Predict on the train set with the new target-net\ny_pred = target_net_new(x_t.view(-1, 1)).ravel()\n\ncriterion = torch.nn.MSELoss()\n\nl = criterion(y_pred, y_t)\nprint(l)\n\ntensor(0.7359, device='cuda:0', grad_fn=&lt;MseLossBackward0&gt;)\ntorch.manual_seed(0)\nplt.scatter(x_t.cpu(), y_t.cpu(), label='Train')\nplt.scatter(x_c.cpu(), y_c.cpu(), label='Context')\nfor i in range(10):\n    # Define hyper_net and target_net architectures\n    hyper_net = torch.nn.Sequential(torch.nn.Linear(2, 64), torch.nn.SELU(), torch.nn.Linear(64, 2))\n    target_net = torch.nn.Linear(1, 1)  # Create a target_net\n   \n    # Learnt function \n    with torch.no_grad():\n        plt.plot(x_lin.cpu(), target_net(x_lin.view(-1, 1)).cpu().ravel(), label=f'Learnt function {i}')\n    # Put legend outside the plot\n    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\ntorch.manual_seed(0)\n# Define hyper_net and target_net architectures\nhyper_net = torch.nn.Sequential(torch.nn.Linear(2, 64), torch.nn.SELU(), torch.nn.Linear(64, 2))\ntarget_net = torch.nn.Linear(1, 1)  # Create a target_net\n\noptimizer = torch.optim.Adam(hyper_net.parameters(), lr=0.01)\ncriterion = nn.MSELoss()\nnew_dict = target_net.state_dict()\n\n\nfor epoch in range(1000):\n    optimizer.zero_grad()\n\n    hyper_out = hyper_net(context)\n    weights = hyper_out.mean(dim=0)\n    new_dict.update({'weight': weights[:1].view(1, 1), 'bias': weights[1:]})\n    #target_net.load_state_dict(new_dict)\n\n    y_pred = torch.func.functional_call(target_net, new_dict, x_t.view(-1, 1)).ravel()\n\n    l = criterion(y_pred, y_t)\n    l.backward()\n    optimizer.step()\n\n    if epoch % 30 == 0:\n        print(f'Epoch {epoch} loss {l:.2f}')\n\nEpoch 0 loss 0.74\nEpoch 30 loss 0.37\nEpoch 60 loss 0.37\nEpoch 90 loss 0.37\nEpoch 120 loss 0.37\nEpoch 150 loss 0.37\nEpoch 180 loss 0.37\nEpoch 210 loss 0.37\nEpoch 240 loss 0.37\nEpoch 270 loss 0.37\nEpoch 300 loss 0.37\nEpoch 330 loss 0.37\nEpoch 360 loss 0.37\nEpoch 390 loss 0.37\nEpoch 420 loss 0.37\nEpoch 450 loss 0.37\nEpoch 480 loss 0.37\nEpoch 510 loss 0.37\nEpoch 540 loss 0.37\nEpoch 570 loss 0.37\nEpoch 600 loss 0.37\nEpoch 630 loss 0.37\nEpoch 660 loss 0.37\nEpoch 690 loss 0.37\nEpoch 720 loss 0.37\nEpoch 750 loss 0.37\nEpoch 780 loss 0.37\nEpoch 810 loss 0.37\nEpoch 840 loss 0.37\nEpoch 870 loss 0.37\nEpoch 900 loss 0.37\nEpoch 930 loss 0.37\nEpoch 960 loss 0.37\nEpoch 990 loss 0.37\nplt.scatter(x_t.cpu(), y_t.cpu(), label='Train')\nplt.scatter(x_c.cpu(), y_c.cpu(), label='Context')\n# Learnt function \nwith torch.no_grad():\n    plt.plot(x_lin.cpu(), target_net(x_lin.view(-1, 1)).ravel().cpu(), label='Learnt function')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f8a3efa7bb0&gt;\n# Define hyper_net and target_net architectures\ntorch.manual_seed(0)\nhyper_net = torch.nn.Sequential(torch.nn.Linear(2, 64), torch.nn.SELU(), torch.nn.Linear(64, 2))\ntarget_net = torch.nn.Linear(1, 1)  # Create a target_net\n\noptimizer = torch.optim.Adam(hyper_net.parameters(), lr=0.01)\ncriterion = nn.MSELoss()\nnew_dict = target_net.state_dict()\n\n\nfor epoch in range(200):\n    for task in range(len(x_train)):\n        len_train = len(x_train[task])\n        x_train_task = x_train[task]\n        y_train_task = y_train[task]\n\n        # Context is 50% of the training data, last 50% is training data\n        x_c = x_train_task[:len_train // 2]\n        y_c = y_train_task[:len_train // 2]\n\n        x_t = x_train_task[len_train // 2:]\n        y_t = y_train_task[len_train // 2:]\n\n        # Concatenate x_c and y_c to form the context\n        context = torch.cat([x_c.view(-1, 1), y_c.view(-1, 1)], dim=1)\n        optimizer.zero_grad()\n\n        hyper_out = hyper_net(context)\n        weights = hyper_out.mean(dim=0)\n        new_dict.update({'weight': weights[:1].view(1, 1), 'bias': weights[1:]})\n        #target_net.load_state_dict(new_dict)\n\n        y_pred = torch.func.functional_call(target_net, new_dict, x_t.view(-1, 1)).ravel()\n\n        l = criterion(y_pred, y_t)\n        l.backward()\n        optimizer.step()\n\n    if epoch % 30 == 0:\n        print(f'Epoch {epoch} loss {l:.2f}')\n\nEpoch 0 loss 0.82\nEpoch 30 loss 0.77\nEpoch 60 loss 0.78\nEpoch 90 loss 0.77\nEpoch 120 loss 0.76\nEpoch 150 loss 0.75\nEpoch 180 loss 0.74"
  },
  {
    "objectID": "notebooks/hypernet-np.html#predict-on-all-tasks",
    "href": "notebooks/hypernet-np.html#predict-on-all-tasks",
    "title": "Train on all tasks",
    "section": "Predict on all tasks",
    "text": "Predict on all tasks\n\nfig, axes = plt.subplots(2, 5, figsize=(8, 4), sharex=True, sharey=True)\naxes = axes.flatten()\nhyper_net.eval()\ntarget_net.eval()\nfor task, ax in zip(range(len(x_train)), axes):\n    # Context is 50% of the training data, last 50% is training data\n    # x_c = x_train_task[:len_train // 2]\n    # y_c = y_train_task[:len_train // 2]\n\n    # x_t = x_train_task[len_train // 2:]\n    # y_t = y_train_task[len_train // 2:]\n\n    # Concatenate x_c and y_c to form the context\n    context = torch.cat([x_train[task].view(-1, 1), y_train[task].view(-1, 1)], dim=1)\n    # print(\"context\", context.shape)\n    optimizer.zero_grad()\n\n    hyper_out = hyper_net(context)\n    weights = hyper_out.mean(dim=0)\n    new_dict.update({'weight': weights[:1].view(1, 1), 'bias': weights[1:]})\n    #target_net.load_state_dict(new_dict)\n\n    with torch.no_grad():\n        y_pred = torch.func.functional_call(target_net, new_dict, x_test[task].view(-1, 1)).ravel()\n    # print(y_pred.shape)\n    \n    ax.scatter(x_train[task].cpu(), y_train[task].cpu(), label='Train', s=4)\n    ax.scatter(x_test[task].cpu(), y_test[task].cpu(), label='Context', s=4)\n    ax.plot(x_test[task].cpu(), y_pred.cpu(), label='Learnt function', color='k')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title(f'Task {task}')\n    ax.legend()"
  },
  {
    "objectID": "notebooks/hypernet-np.html#predict-on-new-task",
    "href": "notebooks/hypernet-np.html#predict-on-new-task",
    "title": "Train on all tasks",
    "section": "Predict on new task",
    "text": "Predict on new task\n\nfig, ax = plt.subplots()\n    # Concatenate x_c and y_c to form the context\ncontext = torch.cat([x_train[-1].view(-1, 1), y_train[-1].view(-1, 1)], dim=1)\n\n##########\n# Cut the context to have only 5 points\n#########\ncontext = context[:5]\n\nprint(\"context\", context.shape)\noptimizer.zero_grad()\n\nhyper_out = hyper_net(context)\nweights = hyper_out.mean(dim=0)\nnew_dict.update({'weight': weights[:1].view(1, 1), 'bias': weights[1:]})\n#target_net.load_state_dict(new_dict)\n\nwith torch.no_grad():\n    y_pred = torch.func.functional_call(target_net, new_dict, x_test[-1].view(-1, 1)).ravel()\n# print(y_pred.shape)\n\nax.scatter(context[:, 0].cpu(), context[:, 1].cpu(), label='Train', s=4)\nax.scatter(x_test[-1].cpu(), y_test[-1].cpu(), label='Context', s=4)\nax.plot(x_test[-1].cpu(), y_pred.cpu(), label='Learnt function', color='k')\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_title(f'-1 {i}')\nax.legend()\n\ncontext torch.Size([5, 2])\n\n\n&lt;matplotlib.legend.Legend at 0x7f8a3ef97f10&gt;"
  },
  {
    "objectID": "notebooks/hypernet-np.html#neural-processes",
    "href": "notebooks/hypernet-np.html#neural-processes",
    "title": "Train on all tasks",
    "section": "Neural Processes",
    "text": "Neural Processes\n\nencoder = torch.nn.Sequential(torch.nn.Linear(2, 128), torch.nn.ReLU(), torch.nn.Linear(128, 128))\ndecoder = torch.nn.Sequential(torch.nn.Linear(128+1, 128), torch.nn.ReLU(), torch.nn.Linear(128, 1))\n\n\nTrain on a single task\n\n# Let us pick Task 0\n\ntask = 4\nlen_train = len(x_train[task])\nx_train_task = x_train[task]\ny_train_task = y_train[task]\n\n# Context is 50% of the training data, last 50% is training data\nx_c = x_train_task[:len_train // 2]\ny_c = y_train_task[:len_train // 2]\n\nx_t = x_train_task[len_train // 2:]\ny_t = y_train_task[len_train // 2:]\n\n# Concatenate x_c and y_c to form the context\ncontext = torch.cat([x_c.view(-1, 1), y_c.view(-1, 1)], dim=1)\n\nprint(context.shape, x_t.shape, y_t.shape)\n\ntorch.Size([25, 2]) torch.Size([25]) torch.Size([25])\n\n\n\nrepresentation = encoder(context)\nrepresentation = representation.mean(dim=0, keepdim=True)\nprint(representation.shape)\n\ntorch.Size([1, 128])\n\n\n\ntarget_repr = representation.repeat(x_t.shape[0], 1)\nprint(target_repr.shape)\n\ntorch.Size([25, 128])\n\n\n\njoint_target_x = torch.cat([target_repr, x_t.view(-1, 1)], dim=1)\nprint(joint_target_x.shape)\n\ntorch.Size([25, 129])\n\n\n\npred = decoder(joint_target_x)\nprint(pred.shape)\n\ntorch.Size([25, 1])\n\n\n\ntorch.manual_seed(0)\n# Define hyper_net and target_net architectures\n# hyper_net = torch.nn.Sequential(torch.nn.Linear(2, 64), torch.nn.SELU(), torch.nn.Linear(64, 2))\n# target_net = torch.nn.Linear(1, 1)  # Create a target_net\n\noptimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-2)\ncriterion = nn.MSELoss()\n\nfor epoch in range(1000):\n    optimizer.zero_grad()\n\n    representation = encoder(context)\n    representation = representation.mean(dim=0, keepdim=True)\n    target_repr = representation.repeat(x_t.shape[0], 1)\n    joint_target_x = torch.cat([target_repr, x_t.view(-1, 1)], dim=1)\n    y_pred = decoder(joint_target_x)\n\n    l = criterion(y_pred, y_t)\n    l.backward()\n    optimizer.step()\n\n    if epoch % 30 == 0:\n        print(f'Epoch {epoch} loss {l:.2f}')\n\nEpoch 0 loss 1.09\nEpoch 30 loss 1.00\nEpoch 60 loss 1.00\nEpoch 90 loss 1.00\nEpoch 120 loss 1.00\nEpoch 150 loss 1.00\nEpoch 180 loss 1.00\nEpoch 210 loss 1.00\nEpoch 240 loss 1.00\nEpoch 270 loss 1.00\nEpoch 300 loss 1.00\nEpoch 330 loss 1.00\nEpoch 360 loss 1.00\nEpoch 390 loss 1.00\nEpoch 420 loss 1.00\nEpoch 450 loss 1.00\nEpoch 480 loss 1.00\nEpoch 510 loss 1.00\nEpoch 540 loss 1.00\nEpoch 570 loss 1.00\nEpoch 600 loss 1.00\nEpoch 630 loss 1.00\nEpoch 660 loss 1.00\nEpoch 690 loss 1.00\nEpoch 720 loss 1.00\nEpoch 750 loss 1.00\nEpoch 780 loss 1.00\nEpoch 810 loss 1.00\nEpoch 840 loss 1.00\nEpoch 870 loss 1.00\nEpoch 900 loss 1.00\nEpoch 930 loss 1.00\nEpoch 960 loss 1.00\nEpoch 990 loss 1.00\n\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/utils/_device.py:62: UserWarning: Using a target size (torch.Size([25])) that is different to the input size (torch.Size([25, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return func(*args, **kwargs)\n\n\n\nplt.scatter(x_t.cpu(), y_t.cpu(), label='Train')\nplt.scatter(x_c.cpu(), y_c.cpu(), label='Context')\n# Learnt function \nwith torch.no_grad():\n    representation = encoder(context)\n    representation = representation.mean(dim=0, keepdim=True)\n    target_repr = representation.repeat(x_lin.shape[0], 1)\n    print(target_repr.shape, x_lin.shape)\n    joint_target_x = torch.cat([target_repr, x_lin.view(-1, 1)], dim=1)\n    plt.plot(x_lin.cpu(), decoder(joint_target_x).ravel().cpu(), label='Learnt function')\nplt.legend();\n\ntorch.Size([100, 128]) torch.Size([100])\n\n\n\n\n\n\n\nTrain on all tasks\n\n# Define hyper_net and target_net architectures\ntorch.manual_seed(0)\nencoder = torch.nn.Sequential(torch.nn.Linear(2, 8), torch.nn.ReLU(), torch.nn.Linear(8, 8))\ndecoder = torch.nn.Sequential(torch.nn.Linear(8+1, 8), torch.nn.ReLU(), torch.nn.Linear(8, 1))\n\noptimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=0.001)\ncriterion = nn.MSELoss()\nnew_dict = target_net.state_dict()\n\n\nfor epoch in range(300):\n    for task in range(len(x_train)):\n        len_train = len(x_train[task])\n        x_train_task = x_train[task]\n        y_train_task = y_train[task]\n\n        # Context is 50% of the training data, last 50% is training data\n        x_c = x_train_task[:len_train // 2]\n        y_c = y_train_task[:len_train // 2]\n\n        x_t = x_train_task[len_train // 2:]\n        y_t = y_train_task[len_train // 2:]\n\n        # Concatenate x_c and y_c to form the context\n        context = torch.cat([x_c.view(-1, 1), y_c.view(-1, 1)], dim=1)\n        optimizer.zero_grad()\n\n        representation = encoder(context)\n        representation = representation.mean(dim=0, keepdim=True)\n        target_repr = representation.repeat(x_t.shape[0], 1)\n        joint_target_x = torch.cat([target_repr, x_t.view(-1, 1)], dim=1)\n        y_pred = decoder(joint_target_x)\n\n        l = criterion(y_pred, y_t)\n        l.backward()\n        optimizer.step()\n\n    if epoch % 30 == 0:\n        print(f'Epoch {epoch} loss {l:.2f}')\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/utils/_device.py:62: UserWarning: Using a target size (torch.Size([25])) that is different to the input size (torch.Size([25, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return func(*args, **kwargs)\n\n\nEpoch 0 loss 0.73\nEpoch 30 loss 0.72\nEpoch 60 loss 0.72\nEpoch 90 loss 0.72\nEpoch 120 loss 0.72\nEpoch 150 loss 0.72\nEpoch 180 loss 0.72\nEpoch 210 loss 0.72"
  },
  {
    "objectID": "notebooks/hypernet-np.html#predict-on-all-tasks-1",
    "href": "notebooks/hypernet-np.html#predict-on-all-tasks-1",
    "title": "Train on all tasks",
    "section": "Predict on all tasks",
    "text": "Predict on all tasks\n\nfig, axes = plt.subplots(2, 5, figsize=(8, 4), sharex=True, sharey=True)\naxes = axes.flatten()\nencoder.eval()\ndecoder.eval()\nfor task, ax in zip(range(len(x_train)), axes):\n    # Context is 50% of the training data, last 50% is training data\n    # x_c = x_train_task[:len_train // 2]\n    # y_c = y_train_task[:len_train // 2]\n\n    # x_t = x_train_task[len_train // 2:]\n    # y_t = y_train_task[len_train // 2:]\n\n    # Concatenate x_c and y_c to form the context\n    context = torch.cat([x_train[task].view(-1, 1), y_train[task].view(-1, 1)], dim=1)\n    # print(\"context\", context.shape)\n    optimizer.zero_grad()\n\n    representation = encoder(context)\n    representation = representation.mean(dim=0, keepdim=True)\n    # print(representation)\n    target_repr = representation.repeat(x_test[task].shape[0], 1)\n    joint_target_x = torch.cat([target_repr, x_test[task].view(-1, 1)], dim=1)\n\n    with torch.no_grad():\n        y_pred = decoder(joint_target_x).ravel()\n        # print(y_pred)\n    # print(y_pred.shape)\n    \n    ax.scatter(x_train[task].cpu(), y_train[task].cpu(), label='Train', s=4)\n    ax.scatter(x_test[task].cpu(), y_test[task].cpu(), label='Context', s=4)\n    ax.plot(x_test[task].cpu(), y_pred.cpu(), label='Learnt function', color='k')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title(f'Task {task}')\n    ax.legend()\n\n\n\n\n\npip install git+https://github.com/sustainability-lab/ASTRA\n\nCollecting git+https://github.com/sustainability-lab/ASTRA\n  Cloning https://github.com/sustainability-lab/ASTRA to /tmp/pip-req-build-uokk43yt\n  Running command git clone --filter=blob:none -q https://github.com/sustainability-lab/ASTRA /tmp/pip-req-build-uokk43yt\n  Resolved https://github.com/sustainability-lab/ASTRA to commit f0b6c0c0d39ae14b036d7f6a6a824e12cee7fa88\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nRequirement already satisfied: pandas in /home/nipun.batra/miniforge3/lib/python3.9/site-packages (from astra-lib==0.0.2.dev12+gf0b6c0c) (2.1.1)\nRequirement already satisfied: tqdm in /home/nipun.batra/miniforge3/lib/python3.9/site-packages (from astra-lib==0.0.2.dev12+gf0b6c0c) (4.66.1)\nRequirement already satisfied: numpy in /home/nipun.batra/miniforge3/lib/python3.9/site-packages (from astra-lib==0.0.2.dev12+gf0b6c0c) (1.26.0)\nRequirement already satisfied: matplotlib in /home/nipun.batra/miniforge3/lib/python3.9/site-packages (from astra-lib==0.0.2.dev12+gf0b6c0c) (3.5.1)\nRequirement already satisfied: xarray in /home/nipun.batra/miniforge3/lib/python3.9/site-packages (from astra-lib==0.0.2.dev12+gf0b6c0c) (2023.8.0)\nCollecting optree\n  Downloading optree-0.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n     |████████████████████████████████| 319 kB 5.1 MB/s            \nRequirement already satisfied: python-dateutil&gt;=2.7 in /home/nipun.batra/miniforge3/lib/python3.9/site-packages (from matplotlib-&gt;astra-lib==0.0.2.dev12+gf0b6c0c) (2.8.2)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /home/nipun.batra/miniforge3/lib/python3.9/site-packages (from matplotlib-&gt;astra-lib==0.0.2.dev12+gf0b6c0c) (4.43.1)\nRequirement already satisfied: pillow&gt;=6.2.0 in /home/nipun.batra/miniforge3/lib/python3.9/site-packages (from matplotlib-&gt;astra-lib==0.0.2.dev12+gf0b6c0c) (9.3.0)\nRequirement already satisfied: cycler&gt;=0.10 in /home/nipun.batra/miniforge3/lib/python3.9/site-packages (from matplotlib-&gt;astra-lib==0.0.2.dev12+gf0b6c0c) (0.12.1)\nRequirement already satisfied: packaging&gt;=20.0 in /home/nipun.batra/miniforge3/lib/python3.9/site-packages (from matplotlib-&gt;astra-lib==0.0.2.dev12+gf0b6c0c) (23.2)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /home/nipun.batra/miniforge3/lib/python3.9/site-packages (from matplotlib-&gt;astra-lib==0.0.2.dev12+gf0b6c0c) (1.4.5)\nRequirement already satisfied: pyparsing&gt;=2.2.1 in /home/nipun.batra/miniforge3/lib/python3.9/site-packages (from matplotlib-&gt;astra-lib==0.0.2.dev12+gf0b6c0c) (3.1.1)\nRequirement already satisfied: typing-extensions&gt;=4.0.0 in /home/nipun.batra/miniforge3/lib/python3.9/site-packages (from optree-&gt;astra-lib==0.0.2.dev12+gf0b6c0c) (4.8.0)\nRequirement already satisfied: tzdata&gt;=2022.1 in /home/nipun.batra/miniforge3/lib/python3.9/site-packages (from pandas-&gt;astra-lib==0.0.2.dev12+gf0b6c0c) (2023.3)\nRequirement already satisfied: pytz&gt;=2020.1 in /home/nipun.batra/miniforge3/lib/python3.9/site-packages (from pandas-&gt;astra-lib==0.0.2.dev12+gf0b6c0c) (2023.3.post1)\nRequirement already satisfied: six&gt;=1.5 in /home/nipun.batra/miniforge3/lib/python3.9/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib-&gt;astra-lib==0.0.2.dev12+gf0b6c0c) (1.16.0)\nBuilding wheels for collected packages: astra-lib\n  Building wheel for astra-lib (pyproject.toml) ... done\n  Created wheel for astra-lib: filename=astra_lib-0.0.2.dev12+gf0b6c0c-py3-none-any.whl size=18547 sha256=55f98c0691926cb2d4e054b09386e4058030819bbf863a5161caf91e52fcd5ac\n  Stored in directory: /tmp/pip-ephem-wheel-cache-f1p2f46_/wheels/cd/44/cd/bb5605bfb1009031a707a97cf28f2eba493e872ddcffc6fbad\nSuccessfully built astra-lib\nInstalling collected packages: optree, astra-lib\nSuccessfully installed astra-lib-0.0.2.dev12+gf0b6c0c optree-0.9.2\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n### Hyper-Net for image reconstruction\n\n\nfrom astra.torch.data import load_mnist\n\nds, ds_name = load_mnist()\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/astra/torch/data.py:11: UserWarning: TORCH_HOME not set, setting it to /home/nipun.batra/.cache/torch\n  warnings.warn(f\"TORCH_HOME not set, setting it to {os.environ['TORCH_HOME']}\")\n100%|██████████| 9912422/9912422 [00:01&lt;00:00, 7744753.54it/s] \n100%|██████████| 28881/28881 [00:00&lt;00:00, 37748736.00it/s]\n100%|██████████| 1648877/1648877 [00:00&lt;00:00, 3499620.93it/s]\n100%|██████████| 4542/4542 [00:00&lt;00:00, 9520504.13it/s]\n\n\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /home/nipun.batra/.cache/torch/data/MNIST/raw/train-images-idx3-ubyte.gz\nExtracting /home/nipun.batra/.cache/torch/data/MNIST/raw/train-images-idx3-ubyte.gz to /home/nipun.batra/.cache/torch/data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /home/nipun.batra/.cache/torch/data/MNIST/raw/train-labels-idx1-ubyte.gz\nExtracting /home/nipun.batra/.cache/torch/data/MNIST/raw/train-labels-idx1-ubyte.gz to /home/nipun.batra/.cache/torch/data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /home/nipun.batra/.cache/torch/data/MNIST/raw/t10k-images-idx3-ubyte.gz\nExtracting /home/nipun.batra/.cache/torch/data/MNIST/raw/t10k-images-idx3-ubyte.gz to /home/nipun.batra/.cache/torch/data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /home/nipun.batra/.cache/torch/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\nExtracting /home/nipun.batra/.cache/torch/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /home/nipun.batra/.cache/torch/data/MNIST/raw\n\n\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (sample: 70000, channel: 1, x: 28, y: 28)\nCoordinates:\n  * sample   (sample) int64 0 1 2 3 4 5 ... 69994 69995 69996 69997 69998 69999\n  * channel  (channel) int64 0\n  * x        (x) int64 27 26 25 24 23 22 21 20 19 18 17 ... 9 8 7 6 5 4 3 2 1 0\n  * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 ... 18 19 20 21 22 23 24 25 26 27\nData variables:\n    img      (sample, channel, x, y) float32 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0\n    label    (sample) float32 5.0 0.0 4.0 1.0 9.0 2.0 ... 2.0 3.0 4.0 5.0 6.0xarray.DatasetDimensions:sample: 70000channel: 1x: 28y: 28Coordinates: (4)sample(sample)int640 1 2 3 ... 69996 69997 69998 69999array([    0,     1,     2, ..., 69997, 69998, 69999])channel(channel)int640array([0])x(x)int6427 26 25 24 23 22 ... 5 4 3 2 1 0array([27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10,\n        9,  8,  7,  6,  5,  4,  3,  2,  1,  0])y(y)int640 1 2 3 4 5 6 ... 22 23 24 25 26 27array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27])Data variables: (2)img(sample, channel, x, y)float320.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0array([[[[0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.]]],\n\n\n       [[[0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.]]],\n\n\n       [[[0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n...\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.]]],\n\n\n       [[[0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.]]],\n\n\n       [[[0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32)label(sample)float325.0 0.0 4.0 1.0 ... 3.0 4.0 5.0 6.0array([5., 0., 4., ..., 4., 5., 6.], dtype=float32)Indexes: (4)samplePandasIndexPandasIndex(Index([    0,     1,     2,     3,     4,     5,     6,     7,     8,     9,\n       ...\n       69990, 69991, 69992, 69993, 69994, 69995, 69996, 69997, 69998, 69999],\n      dtype='int64', name='sample', length=70000))channelPandasIndexPandasIndex(Index([0], dtype='int64', name='channel'))xPandasIndexPandasIndex(Index([27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10,\n        9,  8,  7,  6,  5,  4,  3,  2,  1,  0],\n      dtype='int64', name='x'))yPandasIndexPandasIndex(Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27],\n      dtype='int64', name='y'))Attributes: (0)\n\n\n\n# Read 1000 images from ds xarray dataset into PyTorch tensors\nprint(ds['img'])\n\nimgs = ds['img'].values[:1000]\n\n&lt;xarray.DataArray 'img' (sample: 70000, channel: 1, x: 28, y: 28)&gt;\narray([[[[0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.]]],\n\n\n       [[[0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.]]],\n\n\n       [[[0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n...\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.]]],\n\n\n       [[[0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.]]],\n\n\n       [[[0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32)\nCoordinates:\n  * sample   (sample) int64 0 1 2 3 4 5 ... 69994 69995 69996 69997 69998 69999\n  * channel  (channel) int64 0\n  * x        (x) int64 27 26 25 24 23 22 21 20 19 18 17 ... 9 8 7 6 5 4 3 2 1 0\n  * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 ... 18 19 20 21 22 23 24 25 26 27\n\n\n\n# Make the imgs as PyTorch tensors\nimgs = torch.from_numpy(imgs)\nimgs.shape\n\ntorch.Size([1000, 1, 28, 28])\n\n\n\n# Plot the first 10 images\nfig, axes = plt.subplots(2, 5, figsize=(8, 4), sharex=True, sharey=True)\naxes = axes.flatten()\nfor i, ax in enumerate(axes):\n    ax.imshow(imgs[i].view(28, 28).cpu().numpy(), cmap='gray')\n    ax.set_title(f'Image {i}')\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n\n\n\n\n### Coodinate MLP for image reconstruction\n\nclass CoordMLP(nn.Module):\n    def __init__(self, in_dim=2, out_dim=1, hidden_dim=64):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.fc3 = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = torch.sin(self.fc1(x))\n        x = torch.sin(self.fc2(x))\n        return self.fc3(x)\n\n\ncoord = CoordMLP()\n\n# Fit the model on the first image\noptimizer = torch.optim.Adam(coord.parameters(), lr=0.01)\ncriterion = nn.MSELoss()\n\n# Create input to the model\nx = torch.linspace(-1, 1, 28)\ny = torch.linspace(-1, 1, 28)\n\n# Create a grid of x and y\nx_grid, y_grid = torch.meshgrid(x, y)\n\n# Flatten the grid\nx_flat = x_grid.flatten()\ny_flat = y_grid.flatten()\n\n# Concatenate x and y to form the input\ninp = torch.cat([x_flat.view(-1, 1), y_flat.view(-1, 1)], dim=1)\n\n# Create the target\ntarget = imgs[0].flatten()\n\nfor epoch in range(1000):\n    optimizer.zero_grad()\n\n    pred = coord(inp)\n    l = criterion(pred, target)\n    l.backward()\n    optimizer.step()\n\n    if epoch % 100 == 0:\n        print(f'Epoch {epoch} loss {l:.2f}')\n\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/utils/_device.py:62: UserWarning: Using a target size (torch.Size([784])) that is different to the input size (torch.Size([784, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return func(*args, **kwargs)\n\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n\n\n\n# Plot the first 10 images\nfig, axes = plt.subplots(2, 5, figsize=(8, 4), sharex=True, sharey=True)\naxes = axes.flatten()\nfor i, ax in enumerate(axes):\n    ax.imshow(imgs[i], cmap='gray')\n    ax.set_title(f'Label: {ds[\"label\"].values[i]}')\n    ax.axis('off')\n\nTypeError: Invalid shape (1, 28, 28) for image data"
  }
]